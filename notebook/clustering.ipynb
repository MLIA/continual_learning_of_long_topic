{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0f93716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fefe952e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ir_datasets.load('msmarco-passage/train/judged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b0ba305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading qrels\n",
      "Loading documents\n",
      "Loading queries\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading qrels\")\n",
    "qrels_collection =\\\n",
    "    pd.DataFrame.from_records(dataset.qrels_iter(),\n",
    "                                columns=['query_id', 'doc_id', 'relevance','iteration'],\n",
    "                                index='query_id')\n",
    "print(\"Loading documents\")\n",
    "documents_collection =\\\n",
    "    pd.DataFrame.from_records(dataset.docs_iter(),\n",
    "                                columns=['doc_id', 'text'],\n",
    "                                index='doc_id')\n",
    "print(\"Loading queries\")\n",
    "queries_collection =\\\n",
    "    pd.DataFrame.from_records(dataset.queries_iter(),\n",
    "                                columns=['query_id', 'text'],\n",
    "                                index='query_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9488127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/people/gerald/Documents/repositories/continual_learning_of_long_topic\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dcd62e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lire.dataset import generate_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ff124724",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample = text_queries[: 10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "248453aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Clustering query to build tasks.\n",
    "The following script create a train/val/test set with(n, 20, 40) queries respectivelly\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "import tqdm\n",
    "import random\n",
    "import inspect\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "\n",
    "\n",
    "class ContinualGenerator():\n",
    "    ''' Continual generator that split corpus based on topics.\n",
    "\n",
    "        Parameters:\n",
    "\n",
    "            query_set : [(query_id, query_txt)]\n",
    "                The set of queries information (query_id and the text \n",
    "                associated to the query)\n",
    "            pre_computed_embedding_path : str(path) Optional\n",
    "                The path to the queries embedding if not given\n",
    "                embeddings will be computed instead (can take time).\n",
    "                If no file is available at the current path but the \n",
    "                parameter is given the embedding will be saved at the \n",
    "                location\n",
    "\n",
    "    '''\n",
    "    def __init__(self, query_set, pre_computed_embedding_path=None):\n",
    "        self.query_set = query_set\n",
    "        self.pre_computed_embedding_path = pre_computed_embedding_path\n",
    "\n",
    "\n",
    "    def _get_embeddings(self, queries_content): \n",
    "        if self.pre_computed_embedding_path is None:\n",
    "            model = SentenceTransformer('stsb-roberta-large')\n",
    "            corpus_embeddings = model.encode(queries_content, show_progress_bar=True, convert_to_numpy=True)\n",
    "\n",
    "        elif not os.path.exists(self.pre_computed_embedding_path):\n",
    "            os.makedirs(os.path.dirname(self.pre_computed_embedding_path), exist_ok=True)\n",
    "            model = SentenceTransformer('stsb-roberta-large')\n",
    "            corpus_embeddings = model.encode(queries_content, show_progress_bar=True, convert_to_numpy=True)\n",
    "\n",
    "            with open(self.pre_computed_embedding_path, \"wb\") as fOut:\n",
    "                pickle.dump({'sentences': queries_content , 'embeddings': corpus_embeddings}, fOut)\n",
    "\n",
    "        else:\n",
    "            with open(self.pre_computed_embedding_path, \"rb\") as fOut:\n",
    "                corpus_embeddings = pickle.load(fOut)['embeddings']\n",
    "\n",
    "        return corpus_embeddings\n",
    "    \n",
    "    @staticmethod\n",
    "    def community_filtering(cos_scores, alpha):\n",
    "        return torch.arange(len(cos_scores))[cos_scores >= alpha]\n",
    "\n",
    "    @staticmethod\n",
    "    def community_detection_clustering(embeddings, ss_embeddings_estimation=50000,\n",
    "                                       alpha=0.75, beta=0.55, mcs=2000):\n",
    "        rp = torch.randperm(len(embeddings))\n",
    "        sse = rp[:ss_embeddings_estimation]\n",
    "        ss_embeddings = embeddings[sse]\n",
    "        emcs = round((ss_embeddings_estimation/len(embeddings)) * mcs)\n",
    "        print('Computing the cos score on ', len(ss_embeddings), ' embeddings')\n",
    "        cos_scores = util.pytorch_cos_sim(ss_embeddings, ss_embeddings)\n",
    "\n",
    "        print('Retrieve the ', emcs, ' closest neigbhors for each embeddings (max similarity)')\n",
    "        top_k_values, top_k_indexes = cos_scores.topk(k=emcs, largest=True)\n",
    "\n",
    "        print('Create the communities')\n",
    "        communities = [(rp[top_k_indexes[i][0]], rp[ContinualGenerator.community_filtering(cos_scores[i], alpha)]) \n",
    "                    for i in range(len(top_k_values))  \n",
    "                    if(top_k_values[i][-1] >= alpha)]\n",
    "        print('Filtering communities to avoid overlapping (at the time there is ', len(communities),' communities)')\n",
    "        sorted_communities = sorted(communities, key=lambda x: len(x[1]), reverse=True)\n",
    "\n",
    "        unique_communities = []\n",
    "        extracted_ids = set()\n",
    "\n",
    "        for centroid, community in sorted_communities:\n",
    "            add_cluster = True\n",
    "            for idx in community:\n",
    "                if idx in extracted_ids:\n",
    "                    add_cluster = False\n",
    "                    break\n",
    "\n",
    "            if add_cluster:\n",
    "                unique_communities.append((centroid, community))\n",
    "                for idx in community:\n",
    "                    extracted_ids.add(idx)\n",
    "        centroids = [torch.Tensor(embeddings[centroid]) for centroid, _ in unique_communities]\n",
    "\n",
    "        print('creating real clusters according to number of examples by clusters')\n",
    "        print(len(embeddings))\n",
    "        print(len(centroids))\n",
    "        cluster_center = torch.stack(centroids)\n",
    "        complete_communities = [[] for i in range(len(cluster_center))]\n",
    "        for i, d in enumerate(embeddings):\n",
    "            cos_sim = util.pytorch_cos_sim(cluster_center, d).flatten()\n",
    "            max_value, max_index = cos_sim.max(-1)\n",
    "            if max_value >= beta:\n",
    "                complete_communities[max_index].append(i)\n",
    "        \n",
    "    \n",
    "        return centroids, complete_communities\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def generate(self, t1=0.75, t2=0.5, mcs=2000, estimation_set_size=50000):\n",
    "        ''' Generate groups of queries.\n",
    "            \n",
    "            Parameters:\n",
    "                t1: float\n",
    "                    the first threshold\n",
    "                t2: float\n",
    "                    the second threshold (all dataset)\n",
    "                msc: int\n",
    "                    minimum number of groups representent\n",
    "                estimation_set_size: int \n",
    "                    the number of elements to perform clustering\n",
    "                    according to t1\n",
    "        '''\n",
    "        queries_ids = [k for (k, v) in self.query_set]\n",
    "        queries_content = [v for (k, v) in self.query_set]\n",
    "        embeddings = self._get_embeddings(queries_content)\n",
    "        centroids, communities =\\\n",
    "            ContinualGenerator.community_detection_clustering(embeddings, estimation_set_size, t1, t2, mcs)\n",
    "        fcom = [[queries_ids[query_index] for j, query_index in enumerate(community)]\n",
    "                 for i, community in enumerate(communities)]\n",
    "        return fcom, centroids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "556bf89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = ContinualGenerator(subsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "213b3c48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d3ea97eed64707961a20602326edfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cos score on  500  embeddings\n",
      "Retrieve the  5  closest neigbhors for each embeddings (max similarity)\n",
      "Create the communities\n",
      "Filtering communities to avoid overlapping (at the time there is  59  communities)\n",
      "creating real clusters according to number of examples by clusters\n",
      "1000\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "clusters, embeddings = generator.generate(t1=0.5, t2=0.5, mcs=10, estimation_set_size=500)\n",
    "\n",
    "\n",
    "# three generation with parameters\n",
    "# small 0.7 0.5 (mcs must be higher)\n",
    "# medium 0.75 0.5\n",
    "# large 0.75 0.55 (mcs must be lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6933d860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "56a781ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[text    what is an example of paracrine signaling\n",
       " Name: 714345, dtype: object,\n",
       " text    can depression cause insomnia\n",
       " Name: 66378, dtype: object,\n",
       " text    what is substance p\n",
       " Name: 800666, dtype: object,\n",
       " text    how many paracetamol is an overdose\n",
       " Name: 291598, dtype: object]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[queries_collection.loc[i] for  i in clusters[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcce71c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
