{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import inspect\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe()))),\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from lire.data_tools.dataset import MSMarco\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/gerald/libraries/conda/envs/lire/lib/python3.9/site-packages/numpy/lib/arraysetops.py:583: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the index\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset\n",
    "# config\n",
    "data_folder = '/local/gerald/CPD/data'\n",
    "split = 'train'\n",
    "\n",
    "# loading the corpus using positive\n",
    "dataset = MSMarco.MSMarcoPassageRankingTopicSmall(data_folder,\n",
    "                                                  download=False,\n",
    "                                                  split=split,\n",
    "                                                  getter='triplet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class struct(dict):\n",
    "    def __getattr__(self, name):\n",
    "        if name in self:\n",
    "            return self[name]\n",
    "        else:\n",
    "            raise AttributeError(\"No such attribute: \" + name)\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        self[name] = value\n",
    "\n",
    "    def __delattr__(self, name):\n",
    "        if name in self:\n",
    "            del self[name]\n",
    "        else:\n",
    "            raise AttributeError(\"No such attribute: \" + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-23 17:03:19,579 Initialise tokenizer\n",
      "2021-02-23 17:03:19,912 Loading pretrained model\n",
      "2021-02-23 17:03:24,076 Use default optimizer Adam\n",
      "2021-02-23 17:03:24,079 Initialise meta info\n"
     ]
    }
   ],
   "source": [
    "from experiments import t5ranker\n",
    "options = t5ranker.default_options\n",
    "\n",
    "my_opt = struct(options)\n",
    "my_experiment = t5ranker.T5TaskRanker( my_opt, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-23 17:03:24,106 Number of examples for the current task : 13053\n",
      "2021-02-23 17:03:34,945 Loss for iteration 160 is 0.9887910485267639\n",
      "2021-02-23 17:04:13,922 Loss for iteration 1760 is 0.19354314800049807\n",
      "2021-02-23 17:04:52,649 Loss for iteration 3360 is 0.11989713679133998\n",
      "2021-02-23 17:05:30,682 Loss for iteration 4960 is 0.09224656328762555\n",
      "2021-02-23 17:06:09,482 Loss for iteration 6560 is 0.07717552811922528\n",
      "2021-02-23 17:06:47,605 Loss for iteration 8160 is 0.0669092447414936\n",
      "2021-02-23 17:07:26,212 Loss for iteration 9760 is 0.05929534410701153\n",
      "2021-02-23 17:08:05,506 Loss for iteration 11360 is 0.053393529509633005\n",
      "2021-02-23 17:08:44,798 Loss for iteration 12960 is 0.04851997008463357\n",
      "2021-02-23 17:08:46,170 Number of examples for the current task : 9555\n",
      "2021-02-23 17:08:56,958 Loss for iteration 160 is 0.04182290946218101\n",
      "2021-02-23 17:09:38,257 Loss for iteration 1760 is 0.026057718590170413\n",
      "2021-02-23 17:10:18,734 Loss for iteration 3360 is 0.01984333676634736\n",
      "2021-02-23 17:10:59,677 Loss for iteration 4960 is 0.016055941818299977\n",
      "2021-02-23 17:11:40,035 Loss for iteration 6560 is 0.014121452773866807\n",
      "2021-02-23 17:12:21,257 Loss for iteration 8160 is 0.012338397149559303\n",
      "2021-02-23 17:12:49,414 Number of examples for the current task : 14610\n",
      "2021-02-23 17:13:00,105 Loss for iteration 160 is 0.05130666477436369\n",
      "2021-02-23 17:13:37,465 Loss for iteration 1760 is 0.029011467530450842\n",
      "2021-02-23 17:14:14,690 Loss for iteration 3360 is 0.021257338944990278\n",
      "2021-02-23 17:14:51,932 Loss for iteration 4960 is 0.017115253410856103\n",
      "2021-02-23 17:15:29,401 Loss for iteration 6560 is 0.01447938312719737\n",
      "2021-02-23 17:16:06,907 Loss for iteration 8160 is 0.01266974368178898\n",
      "2021-02-23 17:16:44,480 Loss for iteration 9760 is 0.011288157203198531\n",
      "2021-02-23 17:17:22,583 Loss for iteration 11360 is 0.010169902967050693\n",
      "2021-02-23 17:18:00,895 Loss for iteration 12960 is 0.009210483426538654\n",
      "2021-02-23 17:18:38,355 Loss for iteration 14560 is 0.0086014953808\n",
      "2021-02-23 17:18:39,018 Number of examples for the current task : 10060\n",
      "2021-02-23 17:18:49,722 Loss for iteration 160 is 0.059857833622531456\n",
      "2021-02-23 17:19:28,858 Loss for iteration 1760 is 0.025200356628645112\n",
      "2021-02-23 17:20:07,865 Loss for iteration 3360 is 0.017208382245914578\n",
      "2021-02-23 17:20:46,955 Loss for iteration 4960 is 0.013047521232052368\n",
      "2021-02-23 17:21:25,731 Loss for iteration 6560 is 0.010883281975129161\n",
      "2021-02-23 17:22:04,967 Loss for iteration 8160 is 0.009294789980054941\n",
      "2021-02-23 17:22:44,625 Loss for iteration 9760 is 0.008186886105946502\n",
      "2021-02-23 17:22:49,999 Number of examples for the current task : 8647\n",
      "2021-02-23 17:23:00,841 Loss for iteration 160 is 0.0507893157614903\n",
      "2021-02-23 17:23:38,998 Loss for iteration 1760 is 0.017482492294548464\n",
      "2021-02-23 17:24:17,044 Loss for iteration 3360 is 0.011639230755230597\n",
      "2021-02-23 17:24:54,567 Loss for iteration 4960 is 0.010107802814830969\n",
      "2021-02-23 17:25:31,571 Loss for iteration 6560 is 0.008971190023892548\n",
      "2021-02-23 17:26:08,754 Loss for iteration 8160 is 0.008037996961980777\n",
      "2021-02-23 17:26:17,445 Number of examples for the current task : 10022\n",
      "2021-02-23 17:26:28,508 Loss for iteration 160 is 0.04981341399252415\n",
      "2021-02-23 17:27:08,208 Loss for iteration 1760 is 0.018914572154129814\n",
      "2021-02-23 17:27:48,023 Loss for iteration 3360 is 0.01325778833588241\n",
      "2021-02-23 17:28:27,907 Loss for iteration 4960 is 0.010360064890297052\n",
      "2021-02-23 17:29:06,544 Loss for iteration 6560 is 0.008759508194648854\n",
      "2021-02-23 17:29:45,773 Loss for iteration 8160 is 0.007555579565438795\n",
      "2021-02-23 17:30:25,640 Loss for iteration 9760 is 0.006703948456418293\n",
      "2021-02-23 17:30:30,421 Number of examples for the current task : 9300\n",
      "2021-02-23 17:30:41,583 Loss for iteration 160 is 0.04443454903296449\n",
      "2021-02-23 17:31:18,017 Loss for iteration 1760 is 0.019786785905215855\n",
      "2021-02-23 17:31:54,433 Loss for iteration 3360 is 0.013951734959984296\n",
      "2021-02-23 17:32:30,374 Loss for iteration 4960 is 0.011795744028876736\n",
      "2021-02-23 17:33:06,848 Loss for iteration 6560 is 0.009747709795564115\n",
      "2021-02-23 17:33:43,528 Loss for iteration 8160 is 0.008376486440783903\n",
      "2021-02-23 17:34:04,193 Number of examples for the current task : 9864\n",
      "2021-02-23 17:34:15,532 Loss for iteration 160 is 0.06573290428654714\n",
      "2021-02-23 17:34:57,539 Loss for iteration 1760 is 0.025051605738783338\n",
      "2021-02-23 17:35:39,563 Loss for iteration 3360 is 0.017863163460262333\n",
      "2021-02-23 17:36:21,015 Loss for iteration 4960 is 0.013428589355326303\n",
      "2021-02-23 17:37:02,522 Loss for iteration 6560 is 0.011119950914723174\n",
      "2021-02-23 17:37:44,701 Loss for iteration 8160 is 0.009503860024476415\n",
      "2021-02-23 17:38:26,551 Loss for iteration 9760 is 0.008419253034423332\n",
      "2021-02-23 17:38:28,253 Number of examples for the current task : 11123\n",
      "2021-02-23 17:38:39,766 Loss for iteration 160 is 0.04919638738713481\n",
      "2021-02-23 17:39:19,004 Loss for iteration 1760 is 0.02658161795949815\n",
      "2021-02-23 17:39:58,286 Loss for iteration 3360 is 0.019202462414619446\n",
      "2021-02-23 17:40:37,850 Loss for iteration 4960 is 0.015772465106258557\n",
      "2021-02-23 17:41:17,268 Loss for iteration 6560 is 0.013049447138843618\n",
      "2021-02-23 17:41:56,854 Loss for iteration 8160 is 0.011520804803885214\n",
      "2021-02-23 17:42:36,748 Loss for iteration 9760 is 0.01047340512521054\n",
      "2021-02-23 17:43:03,487 Number of examples for the current task : 8937\n",
      "2021-02-23 17:43:14,668 Loss for iteration 160 is 0.053686017339879814\n",
      "2021-02-23 17:43:52,432 Loss for iteration 1760 is 0.01790524541970913\n",
      "2021-02-23 17:44:30,258 Loss for iteration 3360 is 0.01145571047358515\n",
      "2021-02-23 17:45:07,807 Loss for iteration 4960 is 0.008699002856230723\n",
      "2021-02-23 17:45:45,607 Loss for iteration 6560 is 0.0069474744425279\n",
      "2021-02-23 17:46:23,699 Loss for iteration 8160 is 0.005965131538815422\n",
      "2021-02-23 17:46:38,293 Number of examples for the current task : 8088\n",
      "2021-02-23 17:46:49,474 Loss for iteration 160 is 0.03187696275893937\n",
      "2021-02-23 17:47:27,393 Loss for iteration 1760 is 0.012930448100952481\n",
      "2021-02-23 17:48:05,402 Loss for iteration 3360 is 0.008565262317413605\n",
      "2021-02-23 17:48:43,620 Loss for iteration 4960 is 0.006798283118494282\n",
      "2021-02-23 17:49:21,530 Loss for iteration 6560 is 0.0056182659428904895\n",
      "2021-02-23 17:49:49,848 Number of examples for the current task : 15451\n",
      "2021-02-23 17:50:00,767 Loss for iteration 160 is 0.048610598356886345\n",
      "2021-02-23 17:50:37,211 Loss for iteration 1760 is 0.020707202512091277\n",
      "2021-02-23 17:51:13,364 Loss for iteration 3360 is 0.013804530621433445\n",
      "2021-02-23 17:51:49,154 Loss for iteration 4960 is 0.010501710601993971\n",
      "2021-02-23 17:52:25,231 Loss for iteration 6560 is 0.00852069782126874\n",
      "2021-02-23 17:53:01,217 Loss for iteration 8160 is 0.007216108303814178\n",
      "2021-02-23 17:53:38,621 Loss for iteration 9760 is 0.00643547729149582\n",
      "2021-02-23 17:54:15,923 Loss for iteration 11360 is 0.005772776927571559\n",
      "2021-02-23 17:54:52,344 Loss for iteration 12960 is 0.0053100823683138254\n",
      "2021-02-23 17:55:28,843 Loss for iteration 14560 is 0.004954003264407004\n",
      "2021-02-23 17:55:44,296 Number of examples for the current task : 7597\n",
      "2021-02-23 17:55:55,419 Loss for iteration 160 is 0.05772497284818779\n",
      "2021-02-23 17:56:33,583 Loss for iteration 1760 is 0.02042967799279067\n",
      "2021-02-23 17:57:11,575 Loss for iteration 3360 is 0.012609824632852828\n",
      "2021-02-23 17:57:49,837 Loss for iteration 4960 is 0.009431161389291627\n",
      "2021-02-23 17:58:27,944 Loss for iteration 6560 is 0.0075452386888888116\n",
      "2021-02-23 17:58:47,227 Number of examples for the current task : 9406\n",
      "2021-02-23 17:58:58,171 Loss for iteration 160 is 0.06193720126016573\n",
      "2021-02-23 17:59:35,454 Loss for iteration 1760 is 0.02363595671674891\n",
      "2021-02-23 18:00:13,143 Loss for iteration 3360 is 0.014862999629551519\n",
      "2021-02-23 18:00:50,880 Loss for iteration 4960 is 0.011205543959801537\n",
      "2021-02-23 18:01:27,740 Loss for iteration 6560 is 0.008948050298335182\n",
      "2021-02-23 18:02:04,959 Loss for iteration 8160 is 0.007596520468072192\n",
      "2021-02-23 18:02:27,676 Number of examples for the current task : 5805\n",
      "2021-02-23 18:02:38,915 Loss for iteration 160 is 0.05996961857784878\n",
      "2021-02-23 18:03:17,149 Loss for iteration 1760 is 0.01562905448346256\n",
      "2021-02-23 18:03:54,822 Loss for iteration 3360 is 0.009770894469334677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-23 18:04:32,832 Loss for iteration 4960 is 0.007380792160413282\n",
      "2021-02-23 18:04:48,604 Number of examples for the current task : 7200\n",
      "2021-02-23 18:04:59,507 Loss for iteration 160 is 0.06237434485757893\n",
      "2021-02-23 18:05:36,912 Loss for iteration 1760 is 0.017422675724594376\n",
      "2021-02-23 18:06:15,046 Loss for iteration 3360 is 0.010977943393791325\n",
      "2021-02-23 18:06:52,034 Loss for iteration 4960 is 0.007920766036307673\n",
      "2021-02-23 18:07:29,862 Loss for iteration 6560 is 0.0065104484240795825\n",
      "2021-02-23 18:07:41,484 Number of examples for the current task : 4418\n",
      "2021-02-23 18:07:52,884 Loss for iteration 160 is 0.05059986290606586\n",
      "2021-02-23 18:08:29,888 Loss for iteration 1760 is 0.014209997782018036\n",
      "2021-02-23 18:09:06,268 Loss for iteration 3360 is 0.008597700044684338\n",
      "2021-02-23 18:09:25,176 Number of examples for the current task : 5844\n",
      "2021-02-23 18:09:36,633 Loss for iteration 160 is 0.06633905837820335\n",
      "2021-02-23 18:10:13,526 Loss for iteration 1760 is 0.017914387567371532\n",
      "2021-02-23 18:10:50,649 Loss for iteration 3360 is 0.010459925664613085\n",
      "2021-02-23 18:11:27,422 Loss for iteration 4960 is 0.00753208809466531\n",
      "2021-02-23 18:11:43,288 Number of examples for the current task : 5909\n",
      "2021-02-23 18:11:54,539 Loss for iteration 160 is 0.05927332490682602\n",
      "2021-02-23 18:12:34,209 Loss for iteration 1760 is 0.01551822357892839\n",
      "2021-02-23 18:13:13,277 Loss for iteration 3360 is 0.00932262264375831\n",
      "2021-02-23 18:13:53,476 Loss for iteration 4960 is 0.006708262600473516\n",
      "2021-02-23 18:14:12,303 Number of examples for the current task : 5208\n",
      "2021-02-23 18:14:23,369 Loss for iteration 160 is 0.05991620130159638\n",
      "2021-02-23 18:15:01,964 Loss for iteration 1760 is 0.014973929922750874\n",
      "2021-02-23 18:15:40,042 Loss for iteration 3360 is 0.010445753092848876\n",
      "2021-02-23 18:16:18,384 Loss for iteration 4960 is 0.007484858624061227\n",
      "2021-02-23 18:16:22,852 Number of examples for the current task : 6006\n",
      "2021-02-23 18:16:34,218 Loss for iteration 160 is 0.04385445660657503\n",
      "2021-02-23 18:17:11,855 Loss for iteration 1760 is 0.014410311122307437\n",
      "2021-02-23 18:17:49,254 Loss for iteration 3360 is 0.008754374458420342\n",
      "2021-02-23 18:18:26,402 Loss for iteration 4960 is 0.006641327892776378\n",
      "2021-02-23 18:18:45,487 Number of examples for the current task : 10955\n",
      "2021-02-23 18:18:56,508 Loss for iteration 160 is 0.058392658829689026\n",
      "2021-02-23 18:19:35,244 Loss for iteration 1760 is 0.022587233449558954\n",
      "2021-02-23 18:20:14,347 Loss for iteration 3360 is 0.01469119038996706\n",
      "2021-02-23 18:20:52,650 Loss for iteration 4960 is 0.01080253489221554\n",
      "2021-02-23 18:21:30,752 Loss for iteration 6560 is 0.008801502785499552\n",
      "2021-02-23 18:22:09,096 Loss for iteration 8160 is 0.007540253392123861\n",
      "2021-02-23 18:22:46,785 Loss for iteration 9760 is 0.006576441448549976\n",
      "2021-02-23 18:23:10,022 Number of examples for the current task : 4822\n",
      "2021-02-23 18:23:21,106 Loss for iteration 160 is 0.07966237752275034\n",
      "2021-02-23 18:23:58,600 Loss for iteration 1760 is 0.01744604598718217\n",
      "2021-02-23 18:24:35,312 Loss for iteration 3360 is 0.010231951713011384\n",
      "2021-02-23 18:25:01,831 Number of examples for the current task : 4724\n",
      "2021-02-23 18:25:12,992 Loss for iteration 160 is 0.06207985668019815\n",
      "2021-02-23 18:25:51,102 Loss for iteration 1760 is 0.014655806534810344\n",
      "2021-02-23 18:26:29,739 Loss for iteration 3360 is 0.008446017734226947\n",
      "2021-02-23 18:26:55,258 Number of examples for the current task : 4422\n",
      "2021-02-23 18:27:06,522 Loss for iteration 160 is 0.0439472147686915\n",
      "2021-02-23 18:27:45,886 Loss for iteration 1760 is 0.011747051034330365\n",
      "2021-02-23 18:28:24,901 Loss for iteration 3360 is 0.007438031518823573\n",
      "2021-02-23 18:28:45,224 Number of examples for the current task : 9005\n",
      "2021-02-23 18:28:56,107 Loss for iteration 160 is 0.0527362111447887\n",
      "2021-02-23 18:29:32,858 Loss for iteration 1760 is 0.01793047600543244\n",
      "2021-02-23 18:30:10,083 Loss for iteration 3360 is 0.011084565132667677\n",
      "2021-02-23 18:30:46,897 Loss for iteration 4960 is 0.008302198215045253\n",
      "2021-02-23 18:31:24,115 Loss for iteration 6560 is 0.006790710290263117\n",
      "2021-02-23 18:32:01,457 Loss for iteration 8160 is 0.0056360270752401345\n",
      "2021-02-23 18:32:16,634 Number of examples for the current task : 6456\n",
      "2021-02-23 18:32:27,918 Loss for iteration 160 is 0.07240848751230673\n",
      "2021-02-23 18:33:06,434 Loss for iteration 1760 is 0.017432410243755993\n",
      "2021-02-23 18:33:44,904 Loss for iteration 3360 is 0.010334114072915008\n",
      "2021-02-23 18:34:22,924 Loss for iteration 4960 is 0.0073681338299752715\n",
      "2021-02-23 18:34:51,205 Number of examples for the current task : 4538\n",
      "2021-02-23 18:35:02,181 Loss for iteration 160 is 0.05343703857877038\n",
      "2021-02-23 18:35:39,765 Loss for iteration 1760 is 0.015484119389208269\n",
      "2021-02-23 18:36:16,759 Loss for iteration 3360 is 0.009360733573803329\n",
      "2021-02-23 18:36:37,675 Number of examples for the current task : 7528\n",
      "2021-02-23 18:36:48,662 Loss for iteration 160 is 0.052780462174930355\n",
      "2021-02-23 18:37:25,909 Loss for iteration 1760 is 0.01670603157096618\n",
      "2021-02-23 18:38:03,254 Loss for iteration 3360 is 0.009838430217844633\n",
      "2021-02-23 18:38:40,277 Loss for iteration 4960 is 0.007116676197694974\n",
      "2021-02-23 18:39:17,368 Loss for iteration 6560 is 0.005725036540836716\n",
      "2021-02-23 18:39:34,851 Number of examples for the current task : 6921\n",
      "2021-02-23 18:39:46,337 Loss for iteration 160 is 0.04949947700581767\n",
      "2021-02-23 18:40:25,330 Loss for iteration 1760 is 0.01555038873171266\n",
      "2021-02-23 18:41:03,709 Loss for iteration 3360 is 0.008894736502637923\n",
      "2021-02-23 18:41:42,859 Loss for iteration 4960 is 0.006439243383558506\n",
      "2021-02-23 18:42:21,717 Loss for iteration 6560 is 0.005079772477616741\n",
      "2021-02-23 18:42:27,927 Number of examples for the current task : 14173\n",
      "2021-02-23 18:42:39,500 Loss for iteration 160 is 0.0784919678487561\n",
      "2021-02-23 18:43:19,287 Loss for iteration 1760 is 0.024969747168355965\n",
      "2021-02-23 18:43:58,521 Loss for iteration 3360 is 0.016221922433877725\n",
      "2021-02-23 18:44:38,036 Loss for iteration 4960 is 0.012320498711563545\n",
      "2021-02-23 18:45:17,676 Loss for iteration 6560 is 0.009917596189840243\n",
      "2021-02-23 18:45:56,540 Loss for iteration 8160 is 0.00833697659315766\n",
      "2021-02-23 18:46:36,121 Loss for iteration 9760 is 0.007363972790613001\n",
      "2021-02-23 18:47:15,097 Loss for iteration 11360 is 0.006662701803610598\n",
      "2021-02-23 18:47:54,639 Loss for iteration 12960 is 0.006050281995033521\n",
      "2021-02-23 18:48:18,055 Number of examples for the current task : 11040\n",
      "2021-02-23 18:48:29,615 Loss for iteration 160 is 0.10799214616417885\n",
      "2021-02-23 18:49:10,453 Loss for iteration 1760 is 0.031741890596930646\n",
      "2021-02-23 18:49:51,845 Loss for iteration 3360 is 0.01973176792774888\n",
      "2021-02-23 18:50:33,797 Loss for iteration 4960 is 0.014515846424643998\n",
      "2021-02-23 18:51:14,694 Loss for iteration 6560 is 0.011404234440857034\n",
      "2021-02-23 18:51:55,764 Loss for iteration 8160 is 0.009521713165479527\n",
      "2021-02-23 18:52:37,115 Loss for iteration 9760 is 0.008228434195361707\n",
      "2021-02-23 18:53:03,397 Number of examples for the current task : 11308\n",
      "2021-02-23 18:53:14,807 Loss for iteration 160 is 0.06325218606401574\n",
      "2021-02-23 18:53:52,740 Loss for iteration 1760 is 0.018977821174262277\n",
      "2021-02-23 18:54:30,674 Loss for iteration 3360 is 0.011600389577320366\n",
      "2021-02-23 18:55:08,182 Loss for iteration 4960 is 0.008512301444477652\n",
      "2021-02-23 18:55:45,930 Loss for iteration 6560 is 0.006797674017877051\n",
      "2021-02-23 18:56:23,602 Loss for iteration 8160 is 0.005670913726508806\n",
      "2021-02-23 18:57:01,399 Loss for iteration 9760 is 0.004824797399102942\n",
      "2021-02-23 18:57:29,774 Number of examples for the current task : 9783\n",
      "2021-02-23 18:57:41,382 Loss for iteration 160 is 0.04847310229458592\n",
      "2021-02-23 18:58:20,006 Loss for iteration 1760 is 0.015514913260221464\n",
      "2021-02-23 18:58:58,336 Loss for iteration 3360 is 0.009640261495151552\n",
      "2021-02-23 18:59:36,587 Loss for iteration 4960 is 0.007031639340478505\n",
      "2021-02-23 19:00:14,940 Loss for iteration 6560 is 0.005562661037446253\n",
      "2021-02-23 19:00:53,003 Loss for iteration 8160 is 0.004646328370059909\n",
      "2021-02-23 19:01:31,164 Loss for iteration 9760 is 0.00405784099387536\n",
      "2021-02-23 19:01:31,285 Number of examples for the current task : 7446\n",
      "2021-02-23 19:01:43,396 Loss for iteration 160 is 0.08180733990262855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-23 19:02:24,413 Loss for iteration 1760 is 0.02084923081041255\n",
      "2021-02-23 19:03:05,625 Loss for iteration 3360 is 0.01253100574141814\n",
      "2021-02-23 19:03:47,002 Loss for iteration 4960 is 0.009017767290037478\n",
      "2021-02-23 19:04:27,928 Loss for iteration 6560 is 0.007057871835488203\n",
      "2021-02-23 19:04:45,454 Number of examples for the current task : 6574\n",
      "2021-02-23 19:04:57,198 Loss for iteration 160 is 0.061539153145118194\n",
      "2021-02-23 19:05:36,121 Loss for iteration 1760 is 0.016693776281475014\n",
      "2021-02-23 19:06:15,219 Loss for iteration 3360 is 0.009817083024417637\n",
      "2021-02-23 19:06:53,598 Loss for iteration 4960 is 0.007096719194992132\n",
      "2021-02-23 19:07:23,874 Number of examples for the current task : 3928\n",
      "2021-02-23 19:07:35,374 Loss for iteration 160 is 0.06176285377957604\n",
      "2021-02-23 19:08:14,811 Loss for iteration 1760 is 0.013930161408073193\n",
      "2021-02-23 19:08:54,201 Loss for iteration 3360 is 0.007818634074653876\n",
      "2021-02-23 19:09:04,773 Number of examples for the current task : 6775\n",
      "2021-02-23 19:09:16,509 Loss for iteration 160 is 0.04929102580486373\n",
      "2021-02-23 19:09:55,817 Loss for iteration 1760 is 0.013024703156924597\n",
      "2021-02-23 19:10:34,966 Loss for iteration 3360 is 0.008244459972719902\n",
      "2021-02-23 19:11:13,985 Loss for iteration 4960 is 0.006292308483490274\n",
      "2021-02-23 19:11:53,751 Loss for iteration 6560 is 0.0050674253905486875\n",
      "2021-02-23 19:11:57,462 Number of examples for the current task : 6964\n",
      "2021-02-23 19:12:09,235 Loss for iteration 160 is 0.0722215302627195\n",
      "2021-02-23 19:12:48,701 Loss for iteration 1760 is 0.0200459654442966\n",
      "2021-02-23 19:13:28,478 Loss for iteration 3360 is 0.012117953050602448\n",
      "2021-02-23 19:14:08,282 Loss for iteration 4960 is 0.008876525846524637\n",
      "2021-02-23 19:14:48,136 Loss for iteration 6560 is 0.0070606216137102575\n",
      "2021-02-23 19:14:55,681 Number of examples for the current task : 5878\n",
      "2021-02-23 19:15:07,205 Loss for iteration 160 is 0.05653308636762879\n",
      "2021-02-23 19:15:44,402 Loss for iteration 1760 is 0.012935347653724052\n",
      "2021-02-23 19:16:21,673 Loss for iteration 3360 is 0.007662164385813141\n",
      "2021-02-23 19:16:58,492 Loss for iteration 4960 is 0.005492105407005762\n",
      "2021-02-23 19:17:14,909 Number of examples for the current task : 5495\n",
      "2021-02-23 19:17:26,444 Loss for iteration 160 is 0.055605148896574974\n",
      "2021-02-23 19:18:01,728 Loss for iteration 1760 is 0.013943979387624643\n",
      "2021-02-23 19:18:37,740 Loss for iteration 3360 is 0.00820850619027762\n",
      "2021-02-23 19:19:13,356 Loss for iteration 4960 is 0.005869099254558204\n",
      "2021-02-23 19:19:22,193 Number of examples for the current task : 3443\n",
      "2021-02-23 19:19:34,037 Loss for iteration 160 is 0.07208251538263126\n",
      "2021-02-23 19:20:11,706 Loss for iteration 1760 is 0.014093703463299174\n",
      "2021-02-23 19:20:50,033 Loss for iteration 3360 is 0.00785082605249062\n",
      "2021-02-23 19:20:51,260 Number of examples for the current task : 6822\n",
      "2021-02-23 19:21:02,650 Loss for iteration 160 is 0.06184330243955959\n",
      "2021-02-23 19:21:39,267 Loss for iteration 1760 is 0.016108501425667388\n",
      "2021-02-23 19:22:15,761 Loss for iteration 3360 is 0.009727750853753993\n",
      "2021-02-23 19:22:52,180 Loss for iteration 4960 is 0.006933597276832121\n",
      "2021-02-23 19:23:28,668 Loss for iteration 6560 is 0.00539325888535895\n",
      "2021-02-23 19:23:32,866 Number of examples for the current task : 7900\n",
      "2021-02-23 19:23:44,752 Loss for iteration 160 is 0.07186664725569161\n",
      "2021-02-23 19:24:24,573 Loss for iteration 1760 is 0.01974045098046059\n",
      "2021-02-23 19:25:04,495 Loss for iteration 3360 is 0.011913181950726603\n",
      "2021-02-23 19:25:44,627 Loss for iteration 4960 is 0.008752875606642835\n",
      "2021-02-23 19:26:24,296 Loss for iteration 6560 is 0.006874339279365061\n",
      "2021-02-23 19:26:49,616 Number of examples for the current task : 6699\n",
      "2021-02-23 19:27:01,188 Loss for iteration 160 is 0.04606265092099255\n",
      "2021-02-23 19:27:40,129 Loss for iteration 1760 is 0.01307381441746419\n",
      "2021-02-23 19:28:19,017 Loss for iteration 3360 is 0.007830150875440022\n",
      "2021-02-23 19:28:57,662 Loss for iteration 4960 is 0.0056117209865093245\n",
      "2021-02-23 19:29:36,283 Loss for iteration 6560 is 0.0044305155902525685\n",
      "2021-02-23 19:29:38,473 Number of examples for the current task : 3092\n",
      "2021-02-23 19:29:50,200 Loss for iteration 160 is 0.05348555802960287\n",
      "2021-02-23 19:30:28,148 Loss for iteration 1760 is 0.011116833913350804\n",
      "2021-02-23 19:30:52,742 Number of examples for the current task : 4771\n",
      "2021-02-23 19:31:04,300 Loss for iteration 160 is 0.051510737870227204\n",
      "2021-02-23 19:31:41,792 Loss for iteration 1760 is 0.014123017394176877\n",
      "2021-02-23 19:32:18,846 Loss for iteration 3360 is 0.008323269310029129\n",
      "2021-02-23 19:32:44,386 Number of examples for the current task : 2812\n",
      "2021-02-23 19:32:56,477 Loss for iteration 160 is 0.04911881532858719\n",
      "2021-02-23 19:33:36,947 Loss for iteration 1760 is 0.00922864947203486\n",
      "2021-02-23 19:33:57,408 Number of examples for the current task : 5687\n",
      "2021-02-23 19:34:09,374 Loss for iteration 160 is 0.053983666129748926\n",
      "2021-02-23 19:34:47,445 Loss for iteration 1760 is 0.01367217472016073\n",
      "2021-02-23 19:35:25,741 Loss for iteration 3360 is 0.007878661059423557\n",
      "2021-02-23 19:36:04,066 Loss for iteration 4960 is 0.005617774027955405\n",
      "2021-02-23 19:36:17,117 Number of examples for the current task : 3616\n",
      "2021-02-23 19:36:29,054 Loss for iteration 160 is 0.029755401509729298\n",
      "2021-02-23 19:37:08,077 Loss for iteration 1760 is 0.007681402301222824\n",
      "2021-02-23 19:37:47,499 Loss for iteration 3360 is 0.004487012120338804\n",
      "2021-02-23 19:37:52,278 Number of examples for the current task : 3289\n",
      "2021-02-23 19:38:04,255 Loss for iteration 160 is 0.052065207707611\n",
      "2021-02-23 19:38:40,981 Loss for iteration 1760 is 0.011554141124334975\n",
      "2021-02-23 19:39:07,589 Number of examples for the current task : 6062\n",
      "2021-02-23 19:39:19,342 Loss for iteration 160 is 0.06412671421739188\n",
      "2021-02-23 19:39:55,775 Loss for iteration 1760 is 0.01790364741647683\n",
      "2021-02-23 19:40:32,652 Loss for iteration 3360 is 0.01058461970977388\n",
      "2021-02-23 19:41:08,821 Loss for iteration 4960 is 0.0074821912096182585\n",
      "2021-02-23 19:41:27,578 Number of examples for the current task : 11287\n",
      "2021-02-23 19:41:39,878 Loss for iteration 160 is 0.03838699133220044\n",
      "2021-02-23 19:42:17,048 Loss for iteration 1760 is 0.01599647877800807\n",
      "2021-02-23 19:42:54,424 Loss for iteration 3360 is 0.01075564593300709\n",
      "2021-02-23 19:43:32,363 Loss for iteration 4960 is 0.008307927877490988\n",
      "2021-02-23 19:44:09,597 Loss for iteration 6560 is 0.006886206984477754\n",
      "2021-02-23 19:44:47,095 Loss for iteration 8160 is 0.00609903161522694\n",
      "2021-02-23 19:45:22,278 Loss for iteration 9760 is 0.005329004537072248\n",
      "2021-02-23 19:45:49,489 Number of examples for the current task : 2805\n",
      "2021-02-23 19:45:58,952 Loss for iteration 160 is 0.04059280412779613\n",
      "2021-02-23 19:46:32,393 Loss for iteration 1760 is 0.008743784262569042\n",
      "2021-02-23 19:46:49,430 Number of examples for the current task : 9780\n",
      "2021-02-23 19:46:59,347 Loss for iteration 160 is 0.05489255978979848\n",
      "2021-02-23 19:47:35,262 Loss for iteration 1760 is 0.01758664023961771\n",
      "2021-02-23 19:48:11,519 Loss for iteration 3360 is 0.011089537572308766\n",
      "2021-02-23 19:48:47,327 Loss for iteration 4960 is 0.007962714090713688\n",
      "2021-02-23 19:49:23,391 Loss for iteration 6560 is 0.0062902039496189575\n",
      "2021-02-23 19:50:00,089 Loss for iteration 8160 is 0.005175849742566593\n",
      "2021-02-23 19:50:36,038 Loss for iteration 9760 is 0.0044165297018501325\n",
      "2021-02-23 19:50:36,114 Number of examples for the current task : 5481\n",
      "2021-02-23 19:50:46,143 Loss for iteration 160 is 0.05461513270115988\n",
      "2021-02-23 19:51:22,821 Loss for iteration 1760 is 0.012817630209217503\n",
      "2021-02-23 19:51:59,987 Loss for iteration 3360 is 0.007439828544007658\n",
      "2021-02-23 19:52:37,085 Loss for iteration 4960 is 0.005408568243098446\n",
      "2021-02-23 19:52:46,626 Number of examples for the current task : 9873\n",
      "2021-02-23 19:52:56,689 Loss for iteration 160 is 0.06739887595176697\n",
      "2021-02-23 19:53:34,948 Loss for iteration 1760 is 0.02054181734209058\n",
      "2021-02-23 19:54:12,920 Loss for iteration 3360 is 0.012558672824870106\n",
      "2021-02-23 19:54:50,881 Loss for iteration 4960 is 0.009076625342953237\n",
      "2021-02-23 19:55:28,971 Loss for iteration 6560 is 0.0072334370457376364\n",
      "2021-02-23 19:56:07,596 Loss for iteration 8160 is 0.0060950323845191615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-23 19:56:46,293 Loss for iteration 9760 is 0.005260575562530952\n",
      "2021-02-23 19:56:48,162 Number of examples for the current task : 5073\n",
      "2021-02-23 19:56:57,749 Loss for iteration 160 is 0.06587525355544957\n",
      "2021-02-23 19:57:31,937 Loss for iteration 1760 is 0.016490744482956593\n",
      "2021-02-23 19:58:06,197 Loss for iteration 3360 is 0.009460491142335008\n",
      "2021-02-23 19:58:40,354 Loss for iteration 4960 is 0.006707644029579408\n",
      "2021-02-23 19:58:42,069 Number of examples for the current task : 2402\n",
      "2021-02-23 19:58:52,370 Loss for iteration 160 is 0.06037991290742701\n",
      "2021-02-23 19:59:31,931 Loss for iteration 1760 is 0.014440158683059921\n",
      "2021-02-23 19:59:44,505 Number of examples for the current task : 3073\n",
      "2021-02-23 19:59:54,558 Loss for iteration 160 is 0.05752591009844433\n",
      "2021-02-23 20:00:32,299 Loss for iteration 1760 is 0.010397443054915146\n",
      "2021-02-23 20:00:56,462 Number of examples for the current task : 2259\n",
      "2021-02-23 20:01:06,528 Loss for iteration 160 is 0.058777478777549484\n",
      "2021-02-23 20:01:42,696 Loss for iteration 1760 is 0.010281209699544209\n",
      "2021-02-23 20:01:51,751 Number of examples for the current task : 5433\n",
      "2021-02-23 20:02:01,972 Loss for iteration 160 is 0.04831528121774847\n",
      "2021-02-23 20:02:38,874 Loss for iteration 1760 is 0.013041487529974532\n",
      "2021-02-23 20:03:14,676 Loss for iteration 3360 is 0.007707484045037801\n",
      "2021-02-23 20:03:51,362 Loss for iteration 4960 is 0.005503545306096399\n",
      "2021-02-23 20:04:00,179 Number of examples for the current task : 6399\n",
      "2021-02-23 20:04:10,566 Loss for iteration 160 is 0.048029841262508526\n",
      "2021-02-23 20:04:48,507 Loss for iteration 1760 is 0.012702059233823355\n",
      "2021-02-23 20:05:27,033 Loss for iteration 3360 is 0.00796066482592224\n",
      "2021-02-23 20:06:05,685 Loss for iteration 4960 is 0.00589812982370186\n",
      "2021-02-23 20:06:34,130 Number of examples for the current task : 11631\n",
      "2021-02-23 20:06:44,021 Loss for iteration 160 is 0.062170330943031746\n",
      "2021-02-23 20:07:19,215 Loss for iteration 1760 is 0.01624915295758763\n",
      "2021-02-23 20:07:53,678 Loss for iteration 3360 is 0.010361018773114256\n",
      "2021-02-23 20:08:28,449 Loss for iteration 4960 is 0.007572448110470692\n",
      "2021-02-23 20:09:03,173 Loss for iteration 6560 is 0.0059593285603472546\n",
      "2021-02-23 20:09:37,560 Loss for iteration 8160 is 0.0049835646946326095\n",
      "2021-02-23 20:10:12,932 Loss for iteration 9760 is 0.00447922160397187\n",
      "2021-02-23 20:10:48,208 Loss for iteration 11360 is 0.004146009257799195\n",
      "2021-02-23 20:10:52,491 Number of examples for the current task : 4528\n",
      "2021-02-23 20:11:02,374 Loss for iteration 160 is 0.07509232024577531\n",
      "2021-02-23 20:11:38,318 Loss for iteration 1760 is 0.016930595489726443\n",
      "2021-02-23 20:12:14,966 Loss for iteration 3360 is 0.010489276684522717\n",
      "2021-02-23 20:12:35,745 Number of examples for the current task : 2399\n",
      "2021-02-23 20:12:45,929 Loss for iteration 160 is 0.04707735582170161\n",
      "2021-02-23 20:13:21,915 Loss for iteration 1760 is 0.008547922316309367\n",
      "2021-02-23 20:13:33,154 Number of examples for the current task : 3943\n",
      "2021-02-23 20:13:42,922 Loss for iteration 160 is 0.04973556592383168\n",
      "2021-02-23 20:14:17,545 Loss for iteration 1760 is 0.010989706401675436\n",
      "2021-02-23 20:14:52,370 Loss for iteration 3360 is 0.006612658397152786\n",
      "2021-02-23 20:15:02,144 Number of examples for the current task : 1707\n",
      "2021-02-23 20:15:11,931 Loss for iteration 160 is 0.050178393806246196\n",
      "2021-02-23 20:15:38,441 Number of examples for the current task : 8637\n",
      "2021-02-23 20:15:48,515 Loss for iteration 160 is 0.056968581693416294\n",
      "2021-02-23 20:16:25,686 Loss for iteration 1760 is 0.01633619130505165\n",
      "2021-02-23 20:17:03,137 Loss for iteration 3360 is 0.010037387372546286\n",
      "2021-02-23 20:17:40,269 Loss for iteration 4960 is 0.007190289846235898\n",
      "2021-02-23 20:18:17,343 Loss for iteration 6560 is 0.005712334783674148\n",
      "2021-02-23 20:18:54,313 Loss for iteration 8160 is 0.004813260833397662\n",
      "2021-02-23 20:19:02,569 Number of examples for the current task : 2219\n",
      "2021-02-23 20:19:12,749 Loss for iteration 160 is 0.04871650150215084\n",
      "2021-02-23 20:19:49,582 Loss for iteration 1760 is 0.00915765297401965\n",
      "2021-02-23 20:19:57,794 Number of examples for the current task : 6966\n",
      "2021-02-23 20:20:08,189 Loss for iteration 160 is 0.06781894239512357\n",
      "2021-02-23 20:20:47,392 Loss for iteration 1760 is 0.0198854235533692\n",
      "2021-02-23 20:21:27,149 Loss for iteration 3360 is 0.011744493262326981\n",
      "2021-02-23 20:22:07,048 Loss for iteration 4960 is 0.008360287701750176\n",
      "2021-02-23 20:22:46,605 Loss for iteration 6560 is 0.006548906919445731\n",
      "2021-02-23 20:22:54,360 Number of examples for the current task : 5638\n",
      "2021-02-23 20:23:04,651 Loss for iteration 160 is 0.058294074101881546\n",
      "2021-02-23 20:23:42,052 Loss for iteration 1760 is 0.013512063932475038\n",
      "2021-02-23 20:24:19,652 Loss for iteration 3360 is 0.007912677858533532\n",
      "2021-02-23 20:24:57,130 Loss for iteration 4960 is 0.00562048350607797\n",
      "2021-02-23 20:25:09,351 Number of examples for the current task : 8199\n",
      "2021-02-23 20:25:18,959 Loss for iteration 160 is 0.04821840944615277\n",
      "2021-02-23 20:25:53,599 Loss for iteration 1760 is 0.016897237395053846\n",
      "2021-02-23 20:26:28,261 Loss for iteration 3360 is 0.01050797154505128\n",
      "2021-02-23 20:27:02,244 Loss for iteration 4960 is 0.007597394560165576\n",
      "2021-02-23 20:27:36,622 Loss for iteration 6560 is 0.006183041928565385\n",
      "2021-02-23 20:28:10,945 Loss for iteration 8160 is 0.005239567299416278\n",
      "2021-02-23 20:28:11,224 Number of examples for the current task : 6899\n",
      "2021-02-23 20:28:21,632 Loss for iteration 160 is 0.05042480567300862\n",
      "2021-02-23 20:29:00,101 Loss for iteration 1760 is 0.014970549176707192\n",
      "2021-02-23 20:29:38,375 Loss for iteration 3360 is 0.008871768018058656\n",
      "2021-02-23 20:30:17,095 Loss for iteration 4960 is 0.006247341640306138\n",
      "2021-02-23 20:30:55,981 Loss for iteration 6560 is 0.005049037551829195\n",
      "2021-02-23 20:31:02,197 Number of examples for the current task : 5919\n",
      "2021-02-23 20:31:12,726 Loss for iteration 160 is 0.053275145844302395\n",
      "2021-02-23 20:31:52,538 Loss for iteration 1760 is 0.014901418773759459\n",
      "2021-02-23 20:32:32,255 Loss for iteration 3360 is 0.008529545644671167\n",
      "2021-02-23 20:33:11,750 Loss for iteration 4960 is 0.006026347404761559\n",
      "2021-02-23 20:33:30,903 Number of examples for the current task : 2013\n",
      "2021-02-23 20:33:41,384 Loss for iteration 160 is 0.0914695303548466\n",
      "2021-02-23 20:34:19,724 Loss for iteration 1760 is 0.013840826242944007\n",
      "2021-02-23 20:34:23,978 Number of examples for the current task : 6754\n",
      "2021-02-23 20:34:34,193 Loss for iteration 160 is 0.05166081034324386\n",
      "2021-02-23 20:35:09,475 Loss for iteration 1760 is 0.01265069122739799\n",
      "2021-02-23 20:35:45,145 Loss for iteration 3360 is 0.007348699551403977\n",
      "2021-02-23 20:36:20,689 Loss for iteration 4960 is 0.005489686755553319\n",
      "2021-02-23 20:36:56,421 Loss for iteration 6560 is 0.004273655386190371\n",
      "2021-02-23 20:36:59,616 Number of examples for the current task : 4923\n",
      "2021-02-23 20:37:09,746 Loss for iteration 160 is 0.06328791041265834\n",
      "2021-02-23 20:37:45,125 Loss for iteration 1760 is 0.011890609126206621\n",
      "2021-02-23 20:38:20,641 Loss for iteration 3360 is 0.006855110030490805\n",
      "2021-02-23 20:38:47,717 Number of examples for the current task : 4582\n",
      "2021-02-23 20:38:58,009 Loss for iteration 160 is 0.053451253439892425\n",
      "2021-02-23 20:39:32,907 Loss for iteration 1760 is 0.010855740926287195\n",
      "2021-02-23 20:40:07,715 Loss for iteration 3360 is 0.006238246177577235\n",
      "2021-02-23 20:40:28,775 Number of examples for the current task : 5645\n",
      "2021-02-23 20:40:39,608 Loss for iteration 160 is 0.06234567934139208\n",
      "2021-02-23 20:41:17,939 Loss for iteration 1760 is 0.01593273638764413\n",
      "2021-02-23 20:41:56,898 Loss for iteration 3360 is 0.009705177720297366\n",
      "2021-02-23 20:42:34,734 Loss for iteration 4960 is 0.007123076627514785\n",
      "2021-02-23 20:42:47,407 Number of examples for the current task : 9935\n",
      "2021-02-23 20:42:57,742 Loss for iteration 160 is 0.07066661280325869\n",
      "2021-02-23 20:43:32,889 Loss for iteration 1760 is 0.0216415550240928\n",
      "2021-02-23 20:44:08,954 Loss for iteration 3360 is 0.013162805921987352\n",
      "2021-02-23 20:44:43,888 Loss for iteration 4960 is 0.009471720720433938\n",
      "2021-02-23 20:45:19,561 Loss for iteration 6560 is 0.007534475045320731\n",
      "2021-02-23 20:45:54,993 Loss for iteration 8160 is 0.006444194642193579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-23 20:46:31,415 Loss for iteration 9760 is 0.005646996746674628\n",
      "2021-02-23 20:46:34,016 Number of examples for the current task : 4141\n",
      "2021-02-23 20:46:44,135 Loss for iteration 160 is 0.04488039914179932\n",
      "2021-02-23 20:47:18,542 Loss for iteration 1760 is 0.010486034383952224\n",
      "2021-02-23 20:47:53,417 Loss for iteration 3360 is 0.0060129570423177675\n",
      "2021-02-23 20:48:06,693 Number of examples for the current task : 4209\n",
      "2021-02-23 20:48:17,261 Loss for iteration 160 is 0.07800680940801447\n",
      "2021-02-23 20:48:55,101 Loss for iteration 1760 is 0.016580006313774532\n",
      "2021-02-23 20:49:32,888 Loss for iteration 3360 is 0.00932348458176662\n",
      "2021-02-23 20:49:49,205 Number of examples for the current task : 3166\n",
      "2021-02-23 20:49:59,804 Loss for iteration 160 is 0.04946992935782129\n",
      "2021-02-23 20:50:37,124 Loss for iteration 1760 is 0.01001704733914209\n",
      "2021-02-23 20:51:03,236 Number of examples for the current task : 1755\n",
      "2021-02-23 20:51:13,211 Loss for iteration 160 is 0.048214370300146664\n",
      "2021-02-23 20:51:39,396 Number of examples for the current task : 2663\n",
      "2021-02-23 20:51:50,130 Loss for iteration 160 is 0.040838192301717674\n",
      "2021-02-23 20:52:28,998 Loss for iteration 1760 is 0.008022189461248549\n",
      "2021-02-23 20:52:46,517 Number of examples for the current task : 3719\n",
      "2021-02-23 20:52:56,968 Loss for iteration 160 is 0.04204276136376641\n",
      "2021-02-23 20:53:32,896 Loss for iteration 1760 is 0.010179046236779876\n",
      "2021-02-23 20:54:09,035 Loss for iteration 3360 is 0.005587216747720683\n",
      "2021-02-23 20:54:15,018 Number of examples for the current task : 2791\n",
      "2021-02-23 20:54:25,407 Loss for iteration 160 is 0.07205537317151373\n",
      "2021-02-23 20:55:02,004 Loss for iteration 1760 is 0.011558997631885807\n",
      "2021-02-23 20:55:20,540 Number of examples for the current task : 8717\n",
      "2021-02-23 20:55:31,153 Loss for iteration 160 is 0.04526386553929611\n",
      "2021-02-23 20:56:08,877 Loss for iteration 1760 is 0.013947183622869555\n",
      "2021-02-23 20:56:46,493 Loss for iteration 3360 is 0.009177573745511914\n",
      "2021-02-23 20:57:24,663 Loss for iteration 4960 is 0.006582692291058295\n",
      "2021-02-23 20:58:02,736 Loss for iteration 6560 is 0.005157720103102458\n",
      "2021-02-23 20:58:40,546 Loss for iteration 8160 is 0.004409036261013224\n",
      "2021-02-23 20:58:50,590 Number of examples for the current task : 3596\n",
      "2021-02-23 20:59:00,970 Loss for iteration 160 is 0.027387434616684914\n",
      "2021-02-23 20:59:38,550 Loss for iteration 1760 is 0.006921401959836068\n",
      "2021-02-23 21:00:15,822 Loss for iteration 3360 is 0.003975863495914864\n",
      "2021-02-23 21:00:19,731 Number of examples for the current task : 2988\n",
      "2021-02-23 21:00:29,930 Loss for iteration 160 is 0.06806242338974368\n",
      "2021-02-23 21:01:05,895 Loss for iteration 1760 is 0.013233487290038669\n",
      "2021-02-23 21:01:27,552 Number of examples for the current task : 3467\n",
      "2021-02-23 21:01:38,153 Loss for iteration 160 is 0.038375691934065384\n",
      "2021-02-23 21:02:17,265 Loss for iteration 1760 is 0.008577314670604595\n",
      "2021-02-23 21:02:56,746 Loss for iteration 3360 is 0.004909415407787944\n",
      "2021-02-23 21:02:58,465 Number of examples for the current task : 3279\n",
      "2021-02-23 21:03:08,849 Loss for iteration 160 is 0.05452430248260498\n",
      "2021-02-23 21:03:46,992 Loss for iteration 1760 is 0.009935710064333712\n",
      "2021-02-23 21:04:15,555 Number of examples for the current task : 6576\n",
      "2021-02-23 21:04:26,047 Loss for iteration 160 is 0.05286556791344827\n",
      "2021-02-23 21:05:02,969 Loss for iteration 1760 is 0.011849305229334932\n",
      "2021-02-23 21:05:39,918 Loss for iteration 3360 is 0.006891336927554014\n",
      "2021-02-23 21:06:16,542 Loss for iteration 4960 is 0.005219447777943066\n",
      "2021-02-23 21:06:53,272 Loss for iteration 6560 is 0.004231480632281706\n",
      "2021-02-23 21:06:53,369 Number of examples for the current task : 2942\n",
      "2021-02-23 21:07:03,696 Loss for iteration 160 is 0.040389017794619904\n",
      "2021-02-23 21:07:39,647 Loss for iteration 1760 is 0.007828873473765992\n",
      "2021-02-23 21:08:00,989 Number of examples for the current task : 2567\n",
      "2021-02-23 21:08:11,184 Loss for iteration 160 is 0.06736943354322152\n",
      "2021-02-23 21:08:45,658 Loss for iteration 1760 is 0.011768467185246918\n",
      "2021-02-23 21:08:59,332 Number of examples for the current task : 5370\n",
      "2021-02-23 21:09:09,503 Loss for iteration 160 is 0.061114158904687924\n",
      "2021-02-23 21:09:46,909 Loss for iteration 1760 is 0.015528135476497564\n",
      "2021-02-23 21:10:24,035 Loss for iteration 3360 is 0.0090913771968517\n",
      "2021-02-23 21:11:01,678 Loss for iteration 4960 is 0.006484846676390016\n",
      "2021-02-23 21:11:09,026 Number of examples for the current task : 2338\n",
      "2021-02-23 21:11:19,213 Loss for iteration 160 is 0.04509925672953779\n",
      "2021-02-23 21:11:56,737 Loss for iteration 1760 is 0.008796433588727965\n",
      "2021-02-23 21:12:07,713 Number of examples for the current task : 3915\n",
      "2021-02-23 21:12:17,827 Loss for iteration 160 is 0.03776598061350259\n",
      "2021-02-23 21:12:54,311 Loss for iteration 1760 is 0.008301686172254031\n",
      "2021-02-23 21:13:30,534 Loss for iteration 3360 is 0.0046358593816277095\n",
      "2021-02-23 21:13:39,855 Number of examples for the current task : 2267\n",
      "2021-02-23 21:13:50,270 Loss for iteration 160 is 0.05705920572985302\n",
      "2021-02-23 21:14:29,781 Loss for iteration 1760 is 0.010516118996321468\n",
      "2021-02-23 21:14:39,427 Number of examples for the current task : 4521\n",
      "2021-02-23 21:14:49,805 Loss for iteration 160 is 0.047568900595334446\n",
      "2021-02-23 21:15:26,076 Loss for iteration 1760 is 0.009618919990644246\n",
      "2021-02-23 21:16:02,543 Loss for iteration 3360 is 0.005474267438666782\n",
      "2021-02-23 21:16:23,445 Number of examples for the current task : 6127\n",
      "2021-02-23 21:16:33,894 Loss for iteration 160 is 0.030552525581284004\n",
      "2021-02-23 21:17:10,500 Loss for iteration 1760 is 0.009835107423478385\n",
      "2021-02-23 21:17:47,107 Loss for iteration 3360 is 0.006106428647287228\n",
      "2021-02-23 21:18:23,550 Loss for iteration 4960 is 0.004713558039514361\n",
      "2021-02-23 21:18:44,181 Number of examples for the current task : 4022\n",
      "2021-02-23 21:18:54,362 Loss for iteration 160 is 0.06624923019923946\n",
      "2021-02-23 21:19:30,226 Loss for iteration 1760 is 0.014142837299365416\n",
      "2021-02-23 21:20:05,866 Loss for iteration 3360 is 0.008217307700648942\n",
      "2021-02-23 21:20:17,321 Number of examples for the current task : 6019\n",
      "2021-02-23 21:20:27,712 Loss for iteration 160 is 0.055857302621006966\n",
      "2021-02-23 21:21:03,030 Loss for iteration 1760 is 0.013404789528608121\n",
      "2021-02-23 21:21:37,870 Loss for iteration 3360 is 0.007707450296939461\n",
      "2021-02-23 21:22:13,172 Loss for iteration 4960 is 0.005553560143265672\n",
      "2021-02-23 21:22:31,550 Number of examples for the current task : 1893\n",
      "2021-02-23 21:22:42,207 Loss for iteration 160 is 0.07016798896207051\n",
      "2021-02-23 21:23:20,164 Loss for iteration 1760 is 0.011002595836757095\n",
      "2021-02-23 21:23:22,401 Number of examples for the current task : 7306\n",
      "2021-02-23 21:23:33,141 Loss for iteration 160 is 0.04521951705894687\n",
      "2021-02-23 21:24:13,273 Loss for iteration 1760 is 0.014437592894610789\n",
      "2021-02-23 21:24:53,491 Loss for iteration 3360 is 0.00872106164107204\n",
      "2021-02-23 21:25:33,956 Loss for iteration 4960 is 0.006544902342360366\n",
      "2021-02-23 21:26:14,025 Loss for iteration 6560 is 0.005536592616366553\n",
      "2021-02-23 21:26:29,163 Number of examples for the current task : 4141\n",
      "2021-02-23 21:26:39,899 Loss for iteration 160 is 0.0371726050295613\n",
      "2021-02-23 21:27:19,242 Loss for iteration 1760 is 0.009126102789761373\n",
      "2021-02-23 21:27:58,243 Loss for iteration 3360 is 0.005443602407433069\n",
      "2021-02-23 21:28:13,346 Number of examples for the current task : 2334\n",
      "2021-02-23 21:28:23,536 Loss for iteration 160 is 0.06816014528951862\n",
      "2021-02-23 21:29:00,330 Loss for iteration 1760 is 0.010359951231506997\n",
      "2021-02-23 21:29:10,461 Number of examples for the current task : 4085\n",
      "2021-02-23 21:29:20,808 Loss for iteration 160 is 0.050575591454451736\n",
      "2021-02-23 21:29:58,866 Loss for iteration 1760 is 0.01029326493507314\n",
      "2021-02-23 21:30:37,253 Loss for iteration 3360 is 0.0060608021793440115\n",
      "2021-02-23 21:30:50,473 Number of examples for the current task : 13042\n",
      "2021-02-23 21:31:01,165 Loss for iteration 160 is 0.0525663247839971\n",
      "2021-02-23 21:31:38,385 Loss for iteration 1760 is 0.018834851013060223\n",
      "2021-02-23 21:32:15,700 Loss for iteration 3360 is 0.012785893138851177\n",
      "2021-02-23 21:32:52,839 Loss for iteration 4960 is 0.009659997121065217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-23 21:33:30,409 Loss for iteration 6560 is 0.007936040924237747\n",
      "2021-02-23 21:34:08,030 Loss for iteration 8160 is 0.006851299406511006\n",
      "2021-02-23 21:34:45,322 Loss for iteration 9760 is 0.006078720813545756\n",
      "2021-02-23 21:35:22,126 Loss for iteration 11360 is 0.0056310468396720765\n",
      "2021-02-23 21:35:59,095 Loss for iteration 12960 is 0.00524483560262585\n",
      "2021-02-23 21:36:00,262 Number of examples for the current task : 3037\n",
      "2021-02-23 21:36:10,603 Loss for iteration 160 is 0.04610644860870459\n",
      "2021-02-23 21:36:44,700 Loss for iteration 1760 is 0.00917267286075019\n",
      "2021-02-23 21:37:05,430 Number of examples for the current task : 5574\n",
      "2021-02-23 21:37:16,182 Loss for iteration 160 is 0.05483844021165913\n",
      "2021-02-23 21:37:54,543 Loss for iteration 1760 is 0.012724519203844908\n",
      "2021-02-23 21:38:32,415 Loss for iteration 3360 is 0.007462272520371689\n",
      "2021-02-23 21:39:10,728 Loss for iteration 4960 is 0.005378112088332729\n",
      "2021-02-23 21:39:22,503 Number of examples for the current task : 3238\n",
      "2021-02-23 21:39:33,291 Loss for iteration 160 is 0.050679365088316525\n",
      "2021-02-23 21:40:10,618 Loss for iteration 1760 is 0.010835265450238309\n",
      "2021-02-23 21:40:38,417 Number of examples for the current task : 2433\n",
      "2021-02-23 21:40:48,995 Loss for iteration 160 is 0.04585017069158229\n",
      "2021-02-23 21:41:25,220 Loss for iteration 1760 is 0.007333994779220849\n",
      "2021-02-23 21:41:37,171 Number of examples for the current task : 4374\n",
      "2021-02-23 21:41:47,872 Loss for iteration 160 is 0.04077950966629115\n",
      "2021-02-23 21:42:25,886 Loss for iteration 1760 is 0.009841963436941171\n",
      "2021-02-23 21:43:03,759 Loss for iteration 3360 is 0.005497004499258799\n",
      "2021-02-23 21:43:22,728 Number of examples for the current task : 4340\n",
      "2021-02-23 21:43:33,703 Loss for iteration 160 is 0.03118889939717271\n",
      "2021-02-23 21:44:12,674 Loss for iteration 1760 is 0.0076377784648725576\n",
      "2021-02-23 21:44:51,355 Loss for iteration 3360 is 0.004527363491407927\n",
      "2021-02-23 21:45:10,182 Number of examples for the current task : 2283\n",
      "2021-02-23 21:45:20,513 Loss for iteration 160 is 0.04453155449168249\n",
      "2021-02-23 21:45:56,133 Loss for iteration 1760 is 0.00866987377833596\n",
      "2021-02-23 21:46:05,051 Number of examples for the current task : 17931\n",
      "2021-02-23 21:46:15,680 Loss for iteration 160 is 0.042476101355119186\n",
      "2021-02-23 21:46:53,067 Loss for iteration 1760 is 0.01725931543655492\n",
      "2021-02-23 21:47:30,740 Loss for iteration 3360 is 0.012224090070699857\n",
      "2021-02-23 21:48:08,124 Loss for iteration 4960 is 0.009440185851417482\n",
      "2021-02-23 21:48:45,887 Loss for iteration 6560 is 0.007804668969245301\n",
      "2021-02-23 21:49:23,065 Loss for iteration 8160 is 0.006599855409832769\n",
      "2021-02-23 21:50:01,324 Loss for iteration 9760 is 0.005968255219365798\n",
      "2021-02-23 21:50:38,744 Loss for iteration 11360 is 0.005543880282834547\n",
      "2021-02-23 21:51:16,652 Loss for iteration 12960 is 0.005104060152253869\n",
      "2021-02-23 21:51:54,112 Loss for iteration 14560 is 0.004724964720681452\n",
      "2021-02-23 21:52:31,330 Loss for iteration 16160 is 0.004506760543229833\n",
      "2021-02-23 21:53:08,525 Loss for iteration 17760 is 0.004324312359361852\n",
      "2021-02-23 21:53:11,392 Number of examples for the current task : 19906\n",
      "2021-02-23 21:53:22,354 Loss for iteration 160 is 0.053968527100302956\n",
      "2021-02-23 21:54:01,063 Loss for iteration 1760 is 0.019812927547808702\n",
      "2021-02-23 21:54:39,512 Loss for iteration 3360 is 0.014170621275147026\n",
      "2021-02-23 21:55:17,639 Loss for iteration 4960 is 0.01119328425748729\n",
      "2021-02-23 21:55:56,408 Loss for iteration 6560 is 0.009419279922068417\n",
      "2021-02-23 21:56:35,188 Loss for iteration 8160 is 0.008249321485775726\n",
      "2021-02-23 21:57:14,276 Loss for iteration 9760 is 0.007474576630946836\n",
      "2021-02-23 21:57:53,160 Loss for iteration 11360 is 0.006738981827877399\n",
      "2021-02-23 21:58:31,516 Loss for iteration 12960 is 0.006199209868128334\n",
      "2021-02-23 21:59:09,911 Loss for iteration 14560 is 0.00580218470523504\n",
      "2021-02-23 21:59:48,797 Loss for iteration 16160 is 0.005526433969684289\n",
      "2021-02-23 22:00:27,532 Loss for iteration 17760 is 0.005229519601338084\n",
      "2021-02-23 22:01:06,092 Loss for iteration 19360 is 0.0050278231041289345\n",
      "2021-02-23 22:01:16,324 Number of examples for the current task : 6428\n",
      "2021-02-23 22:01:26,771 Loss for iteration 160 is 0.037984425625340504\n",
      "2021-02-23 22:02:03,678 Loss for iteration 1760 is 0.011311446596304089\n",
      "2021-02-23 22:02:40,234 Loss for iteration 3360 is 0.006640474354387294\n",
      "2021-02-23 22:03:17,262 Loss for iteration 4960 is 0.0049383316957798875\n",
      "2021-02-23 22:03:43,710 Number of examples for the current task : 4011\n",
      "2021-02-23 22:03:54,257 Loss for iteration 160 is 0.04338555633810095\n",
      "2021-02-23 22:04:29,075 Loss for iteration 1760 is 0.007718203018780227\n",
      "2021-02-23 22:05:03,380 Loss for iteration 3360 is 0.004453914611533373\n",
      "2021-02-23 22:05:13,911 Number of examples for the current task : 4276\n",
      "2021-02-23 22:05:24,574 Loss for iteration 160 is 0.06032615408978679\n",
      "2021-02-23 22:06:01,816 Loss for iteration 1760 is 0.012821883443262646\n",
      "2021-02-23 22:06:39,264 Loss for iteration 3360 is 0.007170314896421841\n",
      "2021-02-23 22:06:56,203 Number of examples for the current task : 4394\n",
      "2021-02-23 22:07:07,235 Loss for iteration 160 is 0.05637817084789276\n",
      "2021-02-23 22:07:44,964 Loss for iteration 1760 is 0.011600591968073769\n",
      "2021-02-23 22:08:22,077 Loss for iteration 3360 is 0.006504908326149243\n",
      "2021-02-23 22:08:41,039 Number of examples for the current task : 2973\n",
      "2021-02-23 22:08:52,091 Loss for iteration 160 is 0.05685305781662464\n",
      "2021-02-23 22:09:31,250 Loss for iteration 1760 is 0.010670797819133243\n",
      "2021-02-23 22:09:54,271 Number of examples for the current task : 4353\n",
      "2021-02-23 22:10:05,289 Loss for iteration 160 is 0.04964079335331917\n",
      "2021-02-23 22:10:46,099 Loss for iteration 1760 is 0.01318293637417777\n",
      "2021-02-23 22:11:27,063 Loss for iteration 3360 is 0.007658492740563953\n",
      "2021-02-23 22:11:47,697 Number of examples for the current task : 8180\n",
      "2021-02-23 22:11:58,063 Loss for iteration 160 is 0.06046385318040848\n",
      "2021-02-23 22:12:33,824 Loss for iteration 1760 is 0.014768619641456081\n",
      "2021-02-23 22:13:09,750 Loss for iteration 3360 is 0.008980872312155707\n",
      "2021-02-23 22:13:45,510 Loss for iteration 4960 is 0.006364915141415815\n",
      "2021-02-23 22:14:21,383 Loss for iteration 6560 is 0.005135270224819643\n",
      "2021-02-23 22:14:57,283 Loss for iteration 8160 is 0.004253988746190744\n",
      "2021-02-23 22:14:57,289 Number of examples for the current task : 4534\n",
      "2021-02-23 22:15:07,873 Loss for iteration 160 is 0.07616672614081339\n",
      "2021-02-23 22:15:44,628 Loss for iteration 1760 is 0.014363590473212671\n",
      "2021-02-23 22:16:21,650 Loss for iteration 3360 is 0.00803003353505369\n",
      "2021-02-23 22:16:42,980 Number of examples for the current task : 3584\n",
      "2021-02-23 22:16:53,926 Loss for iteration 160 is 0.03664287255907601\n",
      "2021-02-23 22:17:31,982 Loss for iteration 1760 is 0.00811378033947082\n",
      "2021-02-23 22:18:10,202 Loss for iteration 3360 is 0.004691754794479304\n",
      "2021-02-23 22:18:14,128 Number of examples for the current task : 2983\n",
      "2021-02-23 22:18:25,185 Loss for iteration 160 is 0.04656609664247795\n",
      "2021-02-23 22:19:03,876 Loss for iteration 1760 is 0.009608016051257634\n",
      "2021-02-23 22:19:27,671 Number of examples for the current task : 4159\n",
      "2021-02-23 22:19:38,034 Loss for iteration 160 is 0.07132161882790652\n",
      "2021-02-23 22:20:10,994 Loss for iteration 1760 is 0.01748115615866793\n",
      "2021-02-23 22:20:44,241 Loss for iteration 3360 is 0.010161366230212052\n",
      "2021-02-23 22:20:56,671 Number of examples for the current task : 2838\n",
      "2021-02-23 22:21:07,797 Loss for iteration 160 is 0.038459653669798914\n",
      "2021-02-23 22:21:44,385 Loss for iteration 1760 is 0.006856785805097273\n",
      "2021-02-23 22:22:03,910 Number of examples for the current task : 4119\n",
      "2021-02-23 22:22:14,698 Loss for iteration 160 is 0.049130178056657314\n",
      "2021-02-23 22:22:52,067 Loss for iteration 1760 is 0.011539296564585235\n",
      "2021-02-23 22:23:29,183 Loss for iteration 3360 is 0.006792979869809883\n",
      "2021-02-23 22:23:42,764 Number of examples for the current task : 2475\n",
      "2021-02-23 22:23:53,657 Loss for iteration 160 is 0.038880776444619354\n",
      "2021-02-23 22:24:28,915 Loss for iteration 1760 is 0.007504994256344838\n",
      "2021-02-23 22:24:40,666 Number of examples for the current task : 4357\n",
      "2021-02-23 22:24:51,374 Loss for iteration 160 is 0.05580358956517144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-23 22:25:27,854 Loss for iteration 1760 is 0.01322530852032926\n",
      "2021-02-23 22:26:04,325 Loss for iteration 3360 is 0.007511741481652435\n",
      "2021-02-23 22:26:21,770 Number of examples for the current task : 3519\n",
      "2021-02-23 22:26:32,995 Loss for iteration 160 is 0.05659427172081037\n",
      "2021-02-23 22:27:12,696 Loss for iteration 1760 is 0.01329724699796832\n",
      "2021-02-23 22:27:52,372 Loss for iteration 3360 is 0.007594812035142533\n",
      "2021-02-23 22:27:55,030 Number of examples for the current task : 2856\n",
      "2021-02-23 22:28:05,821 Loss for iteration 160 is 0.054122907956215466\n",
      "2021-02-23 22:28:41,978 Loss for iteration 1760 is 0.01179654817731405\n",
      "2021-02-23 22:29:01,468 Number of examples for the current task : 4196\n",
      "2021-02-23 22:29:12,044 Loss for iteration 160 is 0.033619875088334084\n",
      "2021-02-23 22:29:47,613 Loss for iteration 1760 is 0.008911803141493257\n",
      "2021-02-23 22:30:23,005 Loss for iteration 3360 is 0.005259410690108504\n",
      "2021-02-23 22:30:37,399 Number of examples for the current task : 2096\n",
      "2021-02-23 22:30:48,373 Loss for iteration 160 is 0.07038186194205825\n",
      "2021-02-23 22:31:25,603 Loss for iteration 1760 is 0.011588915015218427\n",
      "2021-02-23 22:31:31,910 Number of examples for the current task : 2876\n",
      "2021-02-23 22:31:42,867 Loss for iteration 160 is 0.03543514248237691\n",
      "2021-02-23 22:32:20,337 Loss for iteration 1760 is 0.007745342937965147\n",
      "2021-02-23 22:32:40,365 Number of examples for the current task : 9641\n",
      "2021-02-23 22:32:51,465 Loss for iteration 160 is 0.03957036281512542\n",
      "2021-02-23 22:33:28,743 Loss for iteration 1760 is 0.015510780476704912\n",
      "2021-02-23 22:34:05,748 Loss for iteration 3360 is 0.010549355933599404\n",
      "2021-02-23 22:34:42,555 Loss for iteration 4960 is 0.008257505100756101\n",
      "2021-02-23 22:35:19,296 Loss for iteration 6560 is 0.00686115583330829\n",
      "2021-02-23 22:35:56,226 Loss for iteration 8160 is 0.005796106976939403\n",
      "2021-02-23 22:36:23,048 Number of examples for the current task : 5004\n",
      "2021-02-23 22:36:34,204 Loss for iteration 160 is 0.06334433027289131\n",
      "2021-02-23 22:37:12,800 Loss for iteration 1760 is 0.01436216778621111\n",
      "2021-02-23 22:37:51,459 Loss for iteration 3360 is 0.008370391386025916\n",
      "2021-02-23 22:38:29,929 Loss for iteration 4960 is 0.006192530520168437\n",
      "2021-02-23 22:38:30,305 Number of examples for the current task : 2898\n",
      "2021-02-23 22:38:41,092 Loss for iteration 160 is 0.06864777177741582\n",
      "2021-02-23 22:39:19,430 Loss for iteration 1760 is 0.013103109010157536\n",
      "2021-02-23 22:39:40,814 Number of examples for the current task : 2129\n",
      "2021-02-23 22:39:52,406 Loss for iteration 160 is 0.05429113131355156\n",
      "2021-02-23 22:40:32,461 Loss for iteration 1760 is 0.010752715886704758\n",
      "2021-02-23 22:40:39,733 Number of examples for the current task : 3166\n",
      "2021-02-23 22:40:50,830 Loss for iteration 160 is 0.04459913523698395\n",
      "2021-02-23 22:41:28,810 Loss for iteration 1760 is 0.00966984865141336\n",
      "2021-02-23 22:41:55,071 Number of examples for the current task : 3834\n",
      "2021-02-23 22:42:05,634 Loss for iteration 160 is 0.047453549605878914\n",
      "2021-02-23 22:42:42,223 Loss for iteration 1760 is 0.010676974133873644\n",
      "2021-02-23 22:43:17,922 Loss for iteration 3360 is 0.006120248877734963\n",
      "2021-02-23 22:43:25,847 Number of examples for the current task : 3555\n",
      "2021-02-23 22:43:36,689 Loss for iteration 160 is 0.04617552751336585\n",
      "2021-02-23 22:44:12,349 Loss for iteration 1760 is 0.010803296403783197\n",
      "2021-02-23 22:44:47,868 Loss for iteration 3360 is 0.005958911763589045\n",
      "2021-02-23 22:44:50,899 Number of examples for the current task : 5558\n",
      "2021-02-23 22:45:01,991 Loss for iteration 160 is 0.06499944187023422\n",
      "2021-02-23 22:45:40,252 Loss for iteration 1760 is 0.016954070160601725\n",
      "2021-02-23 22:46:18,261 Loss for iteration 3360 is 0.010226182835911769\n",
      "2021-02-23 22:46:56,661 Loss for iteration 4960 is 0.007349121936769026\n",
      "2021-02-23 22:47:07,836 Number of examples for the current task : 16190\n",
      "2021-02-23 22:47:18,800 Loss for iteration 160 is 0.06452674029225652\n",
      "2021-02-23 22:47:55,039 Loss for iteration 1760 is 0.0214865577600158\n",
      "2021-02-23 22:48:31,309 Loss for iteration 3360 is 0.014856844753141722\n",
      "2021-02-23 22:49:08,567 Loss for iteration 4960 is 0.011287703310211348\n",
      "2021-02-23 22:49:45,017 Loss for iteration 6560 is 0.009209196705651842\n",
      "2021-02-23 22:50:21,420 Loss for iteration 8160 is 0.008057751090934685\n",
      "2021-02-23 22:50:58,147 Loss for iteration 9760 is 0.007154351931380461\n",
      "2021-02-23 22:51:34,499 Loss for iteration 11360 is 0.00640615670438675\n",
      "2021-02-23 22:52:10,571 Loss for iteration 12960 is 0.005943881158261884\n",
      "2021-02-23 22:52:47,216 Loss for iteration 14560 is 0.005475034858838335\n",
      "2021-02-23 22:53:23,636 Loss for iteration 16160 is 0.005098556147249415\n",
      "2021-02-23 22:53:23,644 Number of examples for the current task : 2757\n",
      "2021-02-23 22:53:35,406 Loss for iteration 160 is 0.03785971205004237\n",
      "2021-02-23 22:54:15,998 Loss for iteration 1760 is 0.008003089743454496\n",
      "2021-02-23 22:54:35,908 Number of examples for the current task : 6727\n",
      "2021-02-23 22:54:46,805 Loss for iteration 160 is 0.05579185164110227\n",
      "2021-02-23 22:55:20,992 Loss for iteration 1760 is 0.014741099139083983\n",
      "2021-02-23 22:55:55,455 Loss for iteration 3360 is 0.00873738828405376\n",
      "2021-02-23 22:56:29,597 Loss for iteration 4960 is 0.006261797756170671\n",
      "2021-02-23 22:57:03,934 Loss for iteration 6560 is 0.00514139350919736\n",
      "2021-02-23 22:57:06,283 Number of examples for the current task : 3071\n",
      "2021-02-23 22:57:17,754 Loss for iteration 160 is 0.07280401242050258\n",
      "2021-02-23 22:57:53,709 Loss for iteration 1760 is 0.013293775594934847\n",
      "2021-02-23 22:58:16,526 Number of examples for the current task : 1692\n",
      "2021-02-23 22:58:27,740 Loss for iteration 160 is 0.03424197100949558\n",
      "2021-02-23 22:58:55,278 Number of examples for the current task : 2069\n",
      "2021-02-23 22:59:06,642 Loss for iteration 160 is 0.03670776906338605\n",
      "2021-02-23 22:59:43,987 Loss for iteration 1760 is 0.006718666952544915\n",
      "2021-02-23 22:59:49,346 Number of examples for the current task : 1966\n",
      "2021-02-23 23:00:00,842 Loss for iteration 160 is 0.04226656241173094\n",
      "2021-02-23 23:00:43,256 Loss for iteration 1760 is 0.008460492177930375\n",
      "2021-02-23 23:00:47,163 Number of examples for the current task : 8756\n",
      "2021-02-23 23:00:58,473 Loss for iteration 160 is 0.05521194806153124\n",
      "2021-02-23 23:01:37,652 Loss for iteration 1760 is 0.017045829500709368\n",
      "2021-02-23 23:02:17,576 Loss for iteration 3360 is 0.01048640047709824\n",
      "2021-02-23 23:02:57,751 Loss for iteration 4960 is 0.007528849892600677\n",
      "2021-02-23 23:03:37,134 Loss for iteration 6560 is 0.006459297472096796\n",
      "2021-02-23 23:04:17,139 Loss for iteration 8160 is 0.005579317802223831\n",
      "2021-02-23 23:04:28,686 Number of examples for the current task : 1317\n",
      "2021-02-23 23:04:40,063 Loss for iteration 160 is 0.026989971795542675\n",
      "2021-02-23 23:05:00,709 Number of examples for the current task : 2900\n",
      "2021-02-23 23:05:12,085 Loss for iteration 160 is 0.061708698218519036\n",
      "2021-02-23 23:05:52,538 Loss for iteration 1760 is 0.011592656781693484\n",
      "2021-02-23 23:06:15,369 Number of examples for the current task : 4603\n",
      "2021-02-23 23:06:26,771 Loss for iteration 160 is 0.07000268741764805\n",
      "2021-02-23 23:07:04,198 Loss for iteration 1760 is 0.015533401848442081\n",
      "2021-02-23 23:07:41,860 Loss for iteration 3360 is 0.008876825505368283\n",
      "2021-02-23 23:08:04,898 Number of examples for the current task : 2777\n",
      "2021-02-23 23:08:16,457 Loss for iteration 160 is 0.04155303072184324\n",
      "2021-02-23 23:08:57,265 Loss for iteration 1760 is 0.009686714185345164\n",
      "2021-02-23 23:09:17,865 Number of examples for the current task : 5825\n",
      "2021-02-23 23:09:29,626 Loss for iteration 160 is 0.0742952527309006\n",
      "2021-02-23 23:10:08,752 Loss for iteration 1760 is 0.019329601486091846\n",
      "2021-02-23 23:10:47,463 Loss for iteration 3360 is 0.011647558320585144\n",
      "2021-02-23 23:11:26,427 Loss for iteration 4960 is 0.008282340217958739\n",
      "2021-02-23 23:11:42,496 Number of examples for the current task : 3497\n",
      "2021-02-23 23:11:53,905 Loss for iteration 160 is 0.059547782452269035\n",
      "2021-02-23 23:12:31,704 Loss for iteration 1760 is 0.010702665278799244\n",
      "2021-02-23 23:13:08,877 Loss for iteration 3360 is 0.006120641485476751\n",
      "2021-02-23 23:13:10,925 Number of examples for the current task : 3944\n",
      "2021-02-23 23:13:22,113 Loss for iteration 160 is 0.06362526389685544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-23 23:13:59,468 Loss for iteration 1760 is 0.014335467798167604\n",
      "2021-02-23 23:14:36,660 Loss for iteration 3360 is 0.008218859993895159\n",
      "2021-02-23 23:14:46,903 Number of examples for the current task : 1512\n",
      "2021-02-23 23:14:58,306 Loss for iteration 160 is 0.03937515650283207\n",
      "2021-02-23 23:15:25,419 Number of examples for the current task : 2973\n",
      "2021-02-23 23:15:36,934 Loss for iteration 160 is 0.03703936765139753\n",
      "2021-02-23 23:16:15,007 Loss for iteration 1760 is 0.009567280068376358\n",
      "2021-02-23 23:16:37,341 Number of examples for the current task : 4326\n",
      "2021-02-23 23:16:48,866 Loss for iteration 160 is 0.04994197807867418\n",
      "2021-02-23 23:17:26,960 Loss for iteration 1760 is 0.01532419235130923\n",
      "2021-02-23 23:18:05,019 Loss for iteration 3360 is 0.008633617322957607\n",
      "2021-02-23 23:18:22,788 Number of examples for the current task : 3331\n",
      "2021-02-23 23:18:34,134 Loss for iteration 160 is 0.05524661417373202\n",
      "2021-02-23 23:19:11,632 Loss for iteration 1760 is 0.012480178796256229\n",
      "2021-02-23 23:19:40,018 Number of examples for the current task : 8294\n",
      "2021-02-23 23:19:51,371 Loss for iteration 160 is 0.04858686250041832\n",
      "2021-02-23 23:20:30,656 Loss for iteration 1760 is 0.015579314084895593\n",
      "2021-02-23 23:21:09,057 Loss for iteration 3360 is 0.009997368735174088\n",
      "2021-02-23 23:21:48,386 Loss for iteration 4960 is 0.007471572891190473\n",
      "2021-02-23 23:22:26,747 Loss for iteration 6560 is 0.006083655926140681\n",
      "2021-02-23 23:23:05,964 Loss for iteration 8160 is 0.005308472478524825\n",
      "2021-02-23 23:23:08,177 Number of examples for the current task : 1378\n",
      "2021-02-23 23:23:19,985 Loss for iteration 160 is 0.03656093939207494\n",
      "2021-02-23 23:23:42,341 Number of examples for the current task : 2090\n",
      "2021-02-23 23:23:54,370 Loss for iteration 160 is 0.04899285107173703\n",
      "2021-02-23 23:24:34,365 Loss for iteration 1760 is 0.009709581528105438\n",
      "2021-02-23 23:24:40,495 Number of examples for the current task : 6393\n",
      "2021-02-23 23:24:52,082 Loss for iteration 160 is 0.04622991590506651\n",
      "2021-02-23 23:25:31,867 Loss for iteration 1760 is 0.013148906422560816\n",
      "2021-02-23 23:26:11,160 Loss for iteration 3360 is 0.008178512223994015\n",
      "2021-02-23 23:26:50,418 Loss for iteration 4960 is 0.00617552953721245\n",
      "2021-02-23 23:27:17,670 Number of examples for the current task : 1563\n",
      "2021-02-23 23:27:29,048 Loss for iteration 160 is 0.05371253437955271\n",
      "2021-02-23 23:27:53,768 Number of examples for the current task : 4682\n",
      "2021-02-23 23:28:05,445 Loss for iteration 160 is 0.03740023482929577\n",
      "2021-02-23 23:28:44,068 Loss for iteration 1760 is 0.010149112897862151\n",
      "2021-02-23 23:29:23,249 Loss for iteration 3360 is 0.006282003049659255\n",
      "2021-02-23 23:29:47,870 Number of examples for the current task : 6628\n",
      "2021-02-23 23:29:59,613 Loss for iteration 160 is 0.0694813605567271\n",
      "2021-02-23 23:30:40,557 Loss for iteration 1760 is 0.02273492304071247\n",
      "2021-02-23 23:31:21,132 Loss for iteration 3360 is 0.013287475952143137\n",
      "2021-02-23 23:32:02,218 Loss for iteration 4960 is 0.009918994687685152\n",
      "2021-02-23 23:32:43,144 Loss for iteration 6560 is 0.008500419664127274\n",
      "2021-02-23 23:32:44,283 Number of examples for the current task : 2484\n",
      "2021-02-23 23:32:55,866 Loss for iteration 160 is 0.044944937764243645\n",
      "2021-02-23 23:33:35,054 Loss for iteration 1760 is 0.011462590157427732\n",
      "2021-02-23 23:33:48,314 Number of examples for the current task : 2244\n",
      "2021-02-23 23:34:00,189 Loss for iteration 160 is 0.0465293943204663\n",
      "2021-02-23 23:34:38,370 Loss for iteration 1760 is 0.00867973354583195\n",
      "2021-02-23 23:34:46,895 Number of examples for the current task : 7548\n",
      "2021-02-23 23:34:58,774 Loss for iteration 160 is 0.03929238173771988\n",
      "2021-02-23 23:35:38,116 Loss for iteration 1760 is 0.015583629732498445\n",
      "2021-02-23 23:36:17,361 Loss for iteration 3360 is 0.009910522411291606\n",
      "2021-02-23 23:36:55,391 Loss for iteration 4960 is 0.007506419047113423\n",
      "2021-02-23 23:37:31,955 Loss for iteration 6560 is 0.006221962226774977\n",
      "2021-02-23 23:37:50,114 Number of examples for the current task : 3563\n",
      "2021-02-23 23:38:00,409 Loss for iteration 160 is 0.04871859892525456\n",
      "2021-02-23 23:38:41,372 Loss for iteration 1760 is 0.010603279897131378\n",
      "2021-02-23 23:39:21,460 Loss for iteration 3360 is 0.006037748386437609\n",
      "2021-02-23 23:39:25,012 Number of examples for the current task : 4433\n",
      "2021-02-23 23:39:34,778 Loss for iteration 160 is 0.07180332290855321\n",
      "2021-02-23 23:40:12,752 Loss for iteration 1760 is 0.014406370687579552\n",
      "2021-02-23 23:40:50,177 Loss for iteration 3360 is 0.008605187576594078\n",
      "2021-02-23 23:41:11,238 Number of examples for the current task : 3594\n",
      "2021-02-23 23:41:20,865 Loss for iteration 160 is 0.04380400969900868\n",
      "2021-02-23 23:41:57,288 Loss for iteration 1760 is 0.010971907424522346\n",
      "2021-02-23 23:42:33,790 Loss for iteration 3360 is 0.0068701270661373085\n",
      "2021-02-23 23:42:37,801 Number of examples for the current task : 6180\n",
      "2021-02-23 23:42:47,533 Loss for iteration 160 is 0.06071537360548973\n",
      "2021-02-23 23:43:23,374 Loss for iteration 1760 is 0.016407349816028465\n",
      "2021-02-23 23:43:59,377 Loss for iteration 3360 is 0.010033405502671618\n",
      "2021-02-23 23:44:35,779 Loss for iteration 4960 is 0.007220515256107132\n",
      "2021-02-23 23:44:58,099 Number of examples for the current task : 11900\n",
      "2021-02-23 23:45:07,518 Loss for iteration 160 is 0.05776143175634471\n",
      "2021-02-23 23:45:42,139 Loss for iteration 1760 is 0.01650749881718274\n",
      "2021-02-23 23:46:16,569 Loss for iteration 3360 is 0.010611651051498208\n",
      "2021-02-23 23:46:51,072 Loss for iteration 4960 is 0.008239197007642828\n",
      "2021-02-23 23:47:25,839 Loss for iteration 6560 is 0.006796108833968061\n",
      "2021-02-23 23:48:00,142 Loss for iteration 8160 is 0.005812411550770042\n",
      "2021-02-23 23:48:35,181 Loss for iteration 9760 is 0.005268812485301526\n",
      "2021-02-23 23:49:09,461 Loss for iteration 11360 is 0.0047635342663478976\n",
      "2021-02-23 23:49:18,405 Number of examples for the current task : 3063\n",
      "2021-02-23 23:49:27,931 Loss for iteration 160 is 0.04812199309129606\n",
      "2021-02-23 23:50:04,411 Loss for iteration 1760 is 0.011438001032844227\n",
      "2021-02-23 23:50:28,461 Number of examples for the current task : 12637\n",
      "2021-02-23 23:50:38,084 Loss for iteration 160 is 0.04201911220496351\n",
      "2021-02-23 23:51:12,693 Loss for iteration 1760 is 0.013645738349631764\n",
      "2021-02-23 23:51:47,361 Loss for iteration 3360 is 0.009029531929204991\n",
      "2021-02-23 23:52:22,429 Loss for iteration 4960 is 0.006599341457085669\n",
      "2021-02-23 23:52:57,018 Loss for iteration 6560 is 0.005651877406739416\n",
      "2021-02-23 23:53:31,476 Loss for iteration 8160 is 0.0049517352222468395\n",
      "2021-02-23 23:54:05,802 Loss for iteration 9760 is 0.004428325779274001\n",
      "2021-02-23 23:54:40,790 Loss for iteration 11360 is 0.004126375624181174\n",
      "2021-02-23 23:55:02,573 Number of examples for the current task : 2478\n",
      "2021-02-23 23:55:11,940 Loss for iteration 160 is 0.06015138666738163\n",
      "2021-02-23 23:55:46,878 Loss for iteration 1760 is 0.013773742625080803\n",
      "2021-02-23 23:55:59,026 Number of examples for the current task : 2975\n",
      "2021-02-23 23:56:08,385 Loss for iteration 160 is 0.06161810135976835\n",
      "2021-02-23 23:56:42,972 Loss for iteration 1760 is 0.013198509850635755\n",
      "2021-02-23 23:57:03,855 Number of examples for the current task : 3165\n",
      "2021-02-23 23:57:13,876 Loss for iteration 160 is 0.048859215680171146\n",
      "2021-02-23 23:57:53,630 Loss for iteration 1760 is 0.0093914593260149\n",
      "2021-02-23 23:58:21,910 Number of examples for the current task : 4441\n",
      "2021-02-23 23:58:31,671 Loss for iteration 160 is 0.056866085664792496\n",
      "2021-02-23 23:59:07,976 Loss for iteration 1760 is 0.012649689409315485\n",
      "2021-02-23 23:59:43,733 Loss for iteration 3360 is 0.0072740197585638\n",
      "2021-02-24 00:00:03,586 Number of examples for the current task : 2710\n",
      "2021-02-24 00:00:13,359 Loss for iteration 160 is 0.06764951204373078\n",
      "2021-02-24 00:00:50,103 Loss for iteration 1760 is 0.012754680810135428\n",
      "2021-02-24 00:01:07,235 Number of examples for the current task : 2901\n",
      "2021-02-24 00:01:17,562 Loss for iteration 160 is 0.0500970133156939\n",
      "2021-02-24 00:01:54,413 Loss for iteration 1760 is 0.011235481809021335\n",
      "2021-02-24 00:02:15,608 Number of examples for the current task : 2543\n",
      "2021-02-24 00:02:25,595 Loss for iteration 160 is 0.049329627135937866\n",
      "2021-02-24 00:03:01,942 Loss for iteration 1760 is 0.009700856657136601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-24 00:03:15,930 Number of examples for the current task : 5528\n",
      "2021-02-24 00:03:25,829 Loss for iteration 160 is 0.06903798387131908\n",
      "2021-02-24 00:04:01,487 Loss for iteration 1760 is 0.015654149577483064\n",
      "2021-02-24 00:04:36,572 Loss for iteration 3360 is 0.00942919534640453\n",
      "2021-02-24 00:05:11,461 Loss for iteration 4960 is 0.006612937275864093\n",
      "2021-02-24 00:05:21,377 Number of examples for the current task : 5781\n",
      "2021-02-24 00:05:31,505 Loss for iteration 160 is 0.07229630293493922\n",
      "2021-02-24 00:06:08,600 Loss for iteration 1760 is 0.01641117228899856\n",
      "2021-02-24 00:06:45,730 Loss for iteration 3360 is 0.009601549802114084\n",
      "2021-02-24 00:07:23,169 Loss for iteration 4960 is 0.006831511936634869\n",
      "2021-02-24 00:07:38,281 Number of examples for the current task : 3433\n",
      "2021-02-24 00:07:48,036 Loss for iteration 160 is 0.05022600987418131\n",
      "2021-02-24 00:08:22,810 Loss for iteration 1760 is 0.010280094870632549\n",
      "2021-02-24 00:08:57,102 Loss for iteration 3360 is 0.005766152502939666\n",
      "2021-02-24 00:08:58,017 Number of examples for the current task : 1816\n",
      "2021-02-24 00:09:08,491 Loss for iteration 160 is 0.06496372158554467\n",
      "2021-02-24 00:09:46,794 Loss for iteration 1760 is 0.009622600346529408\n",
      "2021-02-24 00:09:47,475 Number of examples for the current task : 3181\n",
      "2021-02-24 00:09:57,853 Loss for iteration 160 is 0.04727095738053322\n",
      "2021-02-24 00:10:35,507 Loss for iteration 1760 is 0.011178389800710368\n",
      "2021-02-24 00:11:01,932 Number of examples for the current task : 2966\n",
      "2021-02-24 00:11:11,898 Loss for iteration 160 is 0.04384982687505809\n",
      "2021-02-24 00:11:46,659 Loss for iteration 1760 is 0.008750803606466155\n",
      "2021-02-24 00:12:07,716 Number of examples for the current task : 3960\n",
      "2021-02-24 00:12:18,336 Loss for iteration 160 is 0.0653485811569474\n",
      "2021-02-24 00:12:57,318 Loss for iteration 1760 is 0.014967337450503397\n",
      "2021-02-24 00:13:36,477 Loss for iteration 3360 is 0.009393231404139175\n",
      "2021-02-24 00:13:48,209 Number of examples for the current task : 2797\n",
      "2021-02-24 00:13:58,140 Loss for iteration 160 is 0.05664441175758839\n",
      "2021-02-24 00:14:32,737 Loss for iteration 1760 is 0.011106219300255599\n",
      "2021-02-24 00:14:50,034 Number of examples for the current task : 11497\n",
      "2021-02-24 00:14:59,992 Loss for iteration 160 is 0.045707920735532585\n",
      "2021-02-24 00:15:35,991 Loss for iteration 1760 is 0.015272578533931828\n",
      "2021-02-24 00:16:12,590 Loss for iteration 3360 is 0.009908886182081104\n",
      "2021-02-24 00:16:49,188 Loss for iteration 4960 is 0.007619151446472897\n",
      "2021-02-24 00:17:25,289 Loss for iteration 6560 is 0.006521945823932298\n",
      "2021-02-24 00:18:01,565 Loss for iteration 8160 is 0.005665457145575607\n",
      "2021-02-24 00:18:37,283 Loss for iteration 9760 is 0.005006417925284761\n",
      "2021-02-24 00:19:13,500 Loss for iteration 11360 is 0.004483660177031885\n",
      "2021-02-24 00:19:15,743 Number of examples for the current task : 3222\n",
      "2021-02-24 00:19:26,548 Loss for iteration 160 is 0.08732203766703606\n",
      "2021-02-24 00:20:06,711 Loss for iteration 1760 is 0.015736866221387726\n",
      "2021-02-24 00:20:37,328 Number of examples for the current task : 3164\n",
      "2021-02-24 00:20:47,733 Loss for iteration 160 is 0.044934443198144436\n",
      "2021-02-24 00:21:25,311 Loss for iteration 1760 is 0.00820170126164223\n",
      "2021-02-24 00:21:53,064 Number of examples for the current task : 3209\n",
      "2021-02-24 00:22:03,428 Loss for iteration 160 is 0.04037048921666362\n",
      "2021-02-24 00:22:40,553 Loss for iteration 1760 is 0.008420638820819289\n",
      "2021-02-24 00:23:07,286 Number of examples for the current task : 5162\n",
      "2021-02-24 00:23:17,509 Loss for iteration 160 is 0.03147381362081929\n",
      "2021-02-24 00:23:52,184 Loss for iteration 1760 is 0.008774061382880333\n",
      "2021-02-24 00:24:27,469 Loss for iteration 3360 is 0.005236434555997751\n",
      "2021-02-24 00:25:02,097 Loss for iteration 4960 is 0.0038357998413501735\n",
      "2021-02-24 00:25:05,423 Number of examples for the current task : 2233\n",
      "2021-02-24 00:25:16,424 Loss for iteration 160 is 0.06585188514807007\n",
      "2021-02-24 00:25:54,227 Loss for iteration 1760 is 0.01165802770599446\n",
      "2021-02-24 00:26:02,841 Number of examples for the current task : 4385\n",
      "2021-02-24 00:26:13,514 Loss for iteration 160 is 0.056767491793090645\n",
      "2021-02-24 00:26:51,699 Loss for iteration 1760 is 0.014188670899366608\n",
      "2021-02-24 00:27:30,464 Loss for iteration 3360 is 0.007980813121666094\n",
      "2021-02-24 00:27:50,208 Number of examples for the current task : 2411\n",
      "2021-02-24 00:28:00,389 Loss for iteration 160 is 0.054242099381305954\n",
      "2021-02-24 00:28:35,978 Loss for iteration 1760 is 0.009957906261751394\n",
      "2021-02-24 00:28:46,854 Number of examples for the current task : 9356\n",
      "2021-02-24 00:28:57,080 Loss for iteration 160 is 0.05272715449841185\n",
      "2021-02-24 00:29:31,847 Loss for iteration 1760 is 0.016837210588467617\n",
      "2021-02-24 00:30:06,743 Loss for iteration 3360 is 0.010971803739926253\n",
      "2021-02-24 00:30:41,638 Loss for iteration 4960 is 0.008110234283551482\n",
      "2021-02-24 00:31:16,148 Loss for iteration 6560 is 0.006723916567277478\n",
      "2021-02-24 00:31:50,274 Loss for iteration 8160 is 0.005617430403498369\n",
      "2021-02-24 00:32:09,758 Number of examples for the current task : 6428\n",
      "2021-02-24 00:32:20,935 Loss for iteration 160 is 0.06831125153059309\n",
      "2021-02-24 00:33:00,668 Loss for iteration 1760 is 0.01782020841418925\n",
      "2021-02-24 00:33:40,524 Loss for iteration 3360 is 0.010678596052526544\n",
      "2021-02-24 00:34:20,646 Loss for iteration 4960 is 0.007840125050981172\n",
      "2021-02-24 00:34:49,695 Number of examples for the current task : 5767\n",
      "2021-02-24 00:35:00,446 Loss for iteration 160 is 0.06162707964805039\n",
      "2021-02-24 00:35:38,513 Loss for iteration 1760 is 0.014100881854779096\n",
      "2021-02-24 00:36:16,477 Loss for iteration 3360 is 0.008420510675484464\n",
      "2021-02-24 00:36:54,734 Loss for iteration 4960 is 0.005979678331957318\n",
      "2021-02-24 00:37:09,606 Number of examples for the current task : 2113\n",
      "2021-02-24 00:37:20,236 Loss for iteration 160 is 0.04718302452767437\n",
      "2021-02-24 00:37:59,096 Loss for iteration 1760 is 0.009819723615110307\n",
      "2021-02-24 00:38:05,713 Number of examples for the current task : 4671\n",
      "2021-02-24 00:38:16,263 Loss for iteration 160 is 0.058222042227333244\n",
      "2021-02-24 00:38:52,253 Loss for iteration 1760 is 0.013377491865088945\n",
      "2021-02-24 00:39:28,111 Loss for iteration 3360 is 0.007554973341253977\n",
      "2021-02-24 00:39:50,896 Number of examples for the current task : 4869\n",
      "2021-02-24 00:40:01,664 Loss for iteration 160 is 0.05288701131939888\n",
      "2021-02-24 00:40:38,253 Loss for iteration 1760 is 0.012338567514660524\n",
      "2021-02-24 00:41:14,082 Loss for iteration 3360 is 0.007163010208861421\n",
      "2021-02-24 00:41:40,564 Number of examples for the current task : 5304\n",
      "2021-02-24 00:41:51,388 Loss for iteration 160 is 0.058292010460387574\n",
      "2021-02-24 00:42:30,135 Loss for iteration 1760 is 0.014198256219183473\n",
      "2021-02-24 00:43:07,599 Loss for iteration 3360 is 0.008060933233299998\n",
      "2021-02-24 00:43:46,033 Loss for iteration 4960 is 0.005728602915509754\n",
      "2021-02-24 00:43:52,253 Number of examples for the current task : 2366\n",
      "2021-02-24 00:44:03,349 Loss for iteration 160 is 0.05208189065822146\n",
      "2021-02-24 00:44:41,632 Loss for iteration 1760 is 0.00911464488321521\n",
      "2021-02-24 00:44:52,895 Number of examples for the current task : 2366\n",
      "2021-02-24 00:45:03,530 Loss for iteration 160 is 0.0697615784169598\n",
      "2021-02-24 00:45:38,903 Loss for iteration 1760 is 0.01218067432191592\n",
      "2021-02-24 00:45:48,960 Number of examples for the current task : 4107\n",
      "2021-02-24 00:45:59,471 Loss for iteration 160 is 0.056327998976815834\n",
      "2021-02-24 00:46:35,995 Loss for iteration 1760 is 0.014117777035812856\n",
      "2021-02-24 00:47:12,509 Loss for iteration 3360 is 0.008031394151300114\n",
      "2021-02-24 00:47:25,573 Number of examples for the current task : 6212\n",
      "2021-02-24 00:47:36,125 Loss for iteration 160 is 0.051852521232583305\n",
      "2021-02-24 00:48:14,423 Loss for iteration 1760 is 0.01481036084645369\n",
      "2021-02-24 00:48:52,590 Loss for iteration 3360 is 0.008765206969496507\n",
      "2021-02-24 00:49:31,811 Loss for iteration 4960 is 0.006201561233921069\n",
      "2021-02-24 00:49:56,542 Number of examples for the current task : 1922\n",
      "2021-02-24 00:50:07,178 Loss for iteration 160 is 0.03693815917623314\n",
      "2021-02-24 00:50:43,624 Loss for iteration 1760 is 0.006243630308646872\n",
      "2021-02-24 00:50:46,361 Number of examples for the current task : 2429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-24 00:50:57,428 Loss for iteration 160 is 0.04391539376229048\n",
      "2021-02-24 00:51:36,043 Loss for iteration 1760 is 0.008357254165955735\n",
      "2021-02-24 00:51:48,921 Number of examples for the current task : 3107\n",
      "2021-02-24 00:51:59,820 Loss for iteration 160 is 0.03793245596303181\n",
      "2021-02-24 00:52:37,944 Loss for iteration 1760 is 0.009071503483495247\n",
      "2021-02-24 00:53:04,035 Number of examples for the current task : 5081\n",
      "2021-02-24 00:53:14,499 Loss for iteration 160 is 0.04794344526122917\n",
      "2021-02-24 00:53:49,365 Loss for iteration 1760 is 0.012742965845055543\n",
      "2021-02-24 00:54:25,021 Loss for iteration 3360 is 0.0073013505177716926\n",
      "2021-02-24 00:55:00,149 Loss for iteration 4960 is 0.005361738434897839\n",
      "2021-02-24 00:55:01,855 Number of examples for the current task : 2321\n",
      "2021-02-24 00:55:12,484 Loss for iteration 160 is 0.024204176681285553\n",
      "2021-02-24 00:55:50,283 Loss for iteration 1760 is 0.00471465742789856\n",
      "2021-02-24 00:56:00,690 Number of examples for the current task : 2320\n",
      "2021-02-24 00:56:11,481 Loss for iteration 160 is 0.03481729497963732\n",
      "2021-02-24 00:56:48,648 Loss for iteration 1760 is 0.006809450171679166\n",
      "2021-02-24 00:56:58,921 Number of examples for the current task : 3124\n",
      "2021-02-24 00:57:09,717 Loss for iteration 160 is 0.05691570746289058\n",
      "2021-02-24 00:57:48,613 Loss for iteration 1760 is 0.009462939265266958\n",
      "2021-02-24 00:58:15,084 Number of examples for the current task : 1849\n",
      "2021-02-24 00:58:25,757 Loss for iteration 160 is 0.03882218033752658\n",
      "2021-02-24 00:59:01,356 Loss for iteration 1760 is 0.006635542549620286\n",
      "2021-02-24 00:59:02,530 Number of examples for the current task : 3410\n",
      "2021-02-24 00:59:13,301 Loss for iteration 160 is 0.04225560015236789\n",
      "2021-02-24 00:59:50,836 Loss for iteration 1760 is 0.010071179455645417\n",
      "2021-02-24 01:00:28,634 Loss for iteration 3360 is 0.005730440780951893\n",
      "2021-02-24 01:00:29,315 Number of examples for the current task : 5582\n",
      "2021-02-24 01:00:39,948 Loss for iteration 160 is 0.03953075798397714\n",
      "2021-02-24 01:01:17,401 Loss for iteration 1760 is 0.013164510201612438\n",
      "2021-02-24 01:01:54,502 Loss for iteration 3360 is 0.008014062747813902\n",
      "2021-02-24 01:02:31,838 Loss for iteration 4960 is 0.005686738966167152\n",
      "2021-02-24 01:02:42,871 Number of examples for the current task : 3352\n",
      "2021-02-24 01:02:53,216 Loss for iteration 160 is 0.04486387697133151\n",
      "2021-02-24 01:03:27,303 Loss for iteration 1760 is 0.01106740589314434\n",
      "2021-02-24 01:03:53,421 Number of examples for the current task : 7495\n",
      "2021-02-24 01:04:04,260 Loss for iteration 160 is 0.06958453348753127\n",
      "2021-02-24 01:04:40,438 Loss for iteration 1760 is 0.017426443815877376\n",
      "2021-02-24 01:05:15,790 Loss for iteration 3360 is 0.010732323710472318\n",
      "2021-02-24 01:05:51,934 Loss for iteration 4960 is 0.007571164817166353\n",
      "2021-02-24 01:06:27,573 Loss for iteration 6560 is 0.006013956081333251\n",
      "2021-02-24 01:06:43,892 Number of examples for the current task : 3616\n",
      "2021-02-24 01:06:54,025 Loss for iteration 160 is 0.0230944866145199\n",
      "2021-02-24 01:07:28,046 Loss for iteration 1760 is 0.006576540124385517\n",
      "2021-02-24 01:08:02,298 Loss for iteration 3360 is 0.003877184930405708\n",
      "2021-02-24 01:08:06,320 Number of examples for the current task : 2473\n",
      "2021-02-24 01:08:17,550 Loss for iteration 160 is 0.040834145472300326\n",
      "2021-02-24 01:08:56,304 Loss for iteration 1760 is 0.007478338789033833\n",
      "2021-02-24 01:09:09,974 Number of examples for the current task : 597\n",
      "2021-02-24 01:09:20,474 Loss for iteration 160 is 0.05025611204010519\n",
      "2021-02-24 01:09:28,024 Number of examples for the current task : 4199\n",
      "2021-02-24 01:09:38,642 Loss for iteration 160 is 0.032556552524593746\n",
      "2021-02-24 01:10:14,642 Loss for iteration 1760 is 0.008878252113202319\n",
      "2021-02-24 01:10:50,810 Loss for iteration 3360 is 0.005639492372029916\n",
      "2021-02-24 01:11:05,515 Number of examples for the current task : 5315\n",
      "2021-02-24 01:11:16,302 Loss for iteration 160 is 0.0728917529975826\n",
      "2021-02-24 01:11:54,195 Loss for iteration 1760 is 0.016830698437023874\n",
      "2021-02-24 01:12:32,294 Loss for iteration 3360 is 0.00965909209125565\n",
      "2021-02-24 01:13:09,776 Loss for iteration 4960 is 0.007146312440594866\n",
      "2021-02-24 01:13:16,093 Number of examples for the current task : 2555\n",
      "2021-02-24 01:13:27,094 Loss for iteration 160 is 0.04592177420007912\n",
      "2021-02-24 01:14:04,551 Loss for iteration 1760 is 0.008457300206672543\n",
      "2021-02-24 01:14:19,371 Number of examples for the current task : 2573\n",
      "2021-02-24 01:14:30,051 Loss for iteration 160 is 0.05249206323853948\n",
      "2021-02-24 01:15:06,551 Loss for iteration 1760 is 0.01020010839426693\n",
      "2021-02-24 01:15:21,453 Number of examples for the current task : 7019\n",
      "2021-02-24 01:15:32,329 Loss for iteration 160 is 0.050834847360172054\n",
      "2021-02-24 01:16:07,833 Loss for iteration 1760 is 0.014482550071300688\n",
      "2021-02-24 01:16:42,989 Loss for iteration 3360 is 0.009006875041614053\n",
      "2021-02-24 01:17:18,675 Loss for iteration 4960 is 0.0067040903703244085\n",
      "2021-02-24 01:17:54,069 Loss for iteration 6560 is 0.005185540843999362\n",
      "2021-02-24 01:18:01,632 Number of examples for the current task : 4450\n",
      "2021-02-24 01:18:12,970 Loss for iteration 160 is 0.043523458425294266\n",
      "2021-02-24 01:18:52,392 Loss for iteration 1760 is 0.012452655930181142\n",
      "2021-02-24 01:19:31,547 Loss for iteration 3360 is 0.007433749048380839\n",
      "2021-02-24 01:19:52,071 Number of examples for the current task : 2286\n",
      "2021-02-24 01:20:02,834 Loss for iteration 160 is 0.03501396719366312\n",
      "2021-02-24 01:20:37,588 Loss for iteration 1760 is 0.007362949139614766\n",
      "2021-02-24 01:20:45,985 Number of examples for the current task : 4308\n",
      "2021-02-24 01:20:56,323 Loss for iteration 160 is 0.042935790663415734\n",
      "2021-02-24 01:21:32,846 Loss for iteration 1760 is 0.011219871275521277\n",
      "2021-02-24 01:22:09,000 Loss for iteration 3360 is 0.006393296198646102\n",
      "2021-02-24 01:22:25,875 Number of examples for the current task : 5393\n",
      "2021-02-24 01:22:36,696 Loss for iteration 160 is 0.036945772865279156\n",
      "2021-02-24 01:23:12,659 Loss for iteration 1760 is 0.009982746063788244\n",
      "2021-02-24 01:23:48,341 Loss for iteration 3360 is 0.005871631303048744\n",
      "2021-02-24 01:24:24,449 Loss for iteration 4960 is 0.004336108388883621\n",
      "2021-02-24 01:24:32,016 Number of examples for the current task : 6507\n",
      "2021-02-24 01:24:42,944 Loss for iteration 160 is 0.05239203640005805\n",
      "2021-02-24 01:25:19,130 Loss for iteration 1760 is 0.013075738302841028\n",
      "2021-02-24 01:25:55,695 Loss for iteration 3360 is 0.008083587745342501\n",
      "2021-02-24 01:26:31,804 Loss for iteration 4960 is 0.005872793464246786\n",
      "2021-02-24 01:26:58,335 Number of examples for the current task : 8792\n",
      "2021-02-24 01:27:08,982 Loss for iteration 160 is 0.05083010917190801\n",
      "2021-02-24 01:27:45,977 Loss for iteration 1760 is 0.017188648083918467\n",
      "2021-02-24 01:28:23,561 Loss for iteration 3360 is 0.010812261857294314\n",
      "2021-02-24 01:29:01,119 Loss for iteration 4960 is 0.00876793078858281\n",
      "2021-02-24 01:29:38,273 Loss for iteration 6560 is 0.006963180797163951\n",
      "2021-02-24 01:30:15,467 Loss for iteration 8160 is 0.006064159319387045\n",
      "2021-02-24 01:30:26,794 Number of examples for the current task : 2362\n",
      "2021-02-24 01:30:38,206 Loss for iteration 160 is 0.05681468580256809\n",
      "2021-02-24 01:31:19,007 Loss for iteration 1760 is 0.011166190695246212\n",
      "2021-02-24 01:31:31,240 Number of examples for the current task : 1729\n",
      "2021-02-24 01:31:42,248 Loss for iteration 160 is 0.025448843159458855\n",
      "2021-02-24 01:32:11,880 Number of examples for the current task : 2168\n",
      "2021-02-24 01:32:22,608 Loss for iteration 160 is 0.039682515134865586\n",
      "2021-02-24 01:33:00,745 Loss for iteration 1760 is 0.00878927505982574\n",
      "2021-02-24 01:33:08,069 Number of examples for the current task : 1980\n",
      "2021-02-24 01:33:19,020 Loss for iteration 160 is 0.039023447781801224\n",
      "2021-02-24 01:33:56,679 Loss for iteration 1760 is 0.006942643752129673\n",
      "2021-02-24 01:34:00,200 Number of examples for the current task : 5204\n",
      "2021-02-24 01:34:11,389 Loss for iteration 160 is 0.052667004987597466\n",
      "2021-02-24 01:34:51,324 Loss for iteration 1760 is 0.01564316256160571\n",
      "2021-02-24 01:35:31,219 Loss for iteration 3360 is 0.008822573124238934\n",
      "2021-02-24 01:36:11,124 Loss for iteration 4960 is 0.006429224088793296\n",
      "2021-02-24 01:36:15,647 Number of examples for the current task : 2933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-24 01:36:26,861 Loss for iteration 160 is 0.04984652979130095\n",
      "2021-02-24 01:37:05,056 Loss for iteration 1760 is 0.00919252696097304\n",
      "2021-02-24 01:37:26,973 Number of examples for the current task : 2768\n",
      "2021-02-24 01:37:38,105 Loss for iteration 160 is 0.029690774242309006\n",
      "2021-02-24 01:38:16,953 Loss for iteration 1760 is 0.007316852610234196\n",
      "2021-02-24 01:38:36,453 Number of examples for the current task : 1397\n",
      "2021-02-24 01:38:47,641 Loss for iteration 160 is 0.04598118297078393\n",
      "2021-02-24 01:39:09,675 Number of examples for the current task : 2453\n",
      "2021-02-24 01:39:20,583 Loss for iteration 160 is 0.043170487211847845\n",
      "2021-02-24 01:39:57,661 Loss for iteration 1760 is 0.009509148408183178\n",
      "2021-02-24 01:40:10,020 Number of examples for the current task : 7398\n",
      "2021-02-24 01:40:21,153 Loss for iteration 160 is 0.058255644430491055\n",
      "2021-02-24 01:41:00,375 Loss for iteration 1760 is 0.017602270083115983\n",
      "2021-02-24 01:41:40,125 Loss for iteration 3360 is 0.010815033520863201\n",
      "2021-02-24 01:42:19,516 Loss for iteration 4960 is 0.007975916412708213\n",
      "2021-02-24 01:42:59,110 Loss for iteration 6560 is 0.007173969968625853\n",
      "2021-02-24 01:43:15,086 Number of examples for the current task : 1809\n",
      "2021-02-24 01:43:25,811 Loss for iteration 160 is 0.05923379771411419\n",
      "2021-02-24 01:44:00,139 Loss for iteration 1760 is 0.010397648952323481\n",
      "2021-02-24 01:44:00,697 Number of examples for the current task : 3220\n",
      "2021-02-24 01:44:11,803 Loss for iteration 160 is 0.04347550225528804\n",
      "2021-02-24 01:44:50,758 Loss for iteration 1760 is 0.010244967236848514\n",
      "2021-02-24 01:45:19,487 Number of examples for the current task : 4653\n",
      "2021-02-24 01:45:30,612 Loss for iteration 160 is 0.04499076493084431\n",
      "2021-02-24 01:46:08,377 Loss for iteration 1760 is 0.01166541777527052\n",
      "2021-02-24 01:46:46,155 Loss for iteration 3360 is 0.006686720374586934\n",
      "2021-02-24 01:47:10,223 Number of examples for the current task : 4917\n",
      "2021-02-24 01:47:21,755 Loss for iteration 160 is 0.06689124012535269\n",
      "2021-02-24 01:48:02,788 Loss for iteration 1760 is 0.015129502614653829\n",
      "2021-02-24 01:48:43,603 Loss for iteration 3360 is 0.008725774370785472\n",
      "2021-02-24 01:49:15,788 Number of examples for the current task : 5373\n",
      "2021-02-24 01:49:26,801 Loss for iteration 160 is 0.051479097455739975\n",
      "2021-02-24 01:50:04,878 Loss for iteration 1760 is 0.01594478180233456\n",
      "2021-02-24 01:50:42,397 Loss for iteration 3360 is 0.009048766849377145\n",
      "2021-02-24 01:51:20,731 Loss for iteration 4960 is 0.00653448385600872\n",
      "2021-02-24 01:51:28,059 Number of examples for the current task : 3492\n",
      "2021-02-24 01:51:39,017 Loss for iteration 160 is 0.06795261292295023\n",
      "2021-02-24 01:52:17,204 Loss for iteration 1760 is 0.013359648797730286\n",
      "2021-02-24 01:52:55,691 Loss for iteration 3360 is 0.007733785381141479\n",
      "2021-02-24 01:52:57,822 Number of examples for the current task : 3131\n",
      "2021-02-24 01:53:09,072 Loss for iteration 160 is 0.05947245454246348\n",
      "2021-02-24 01:53:47,317 Loss for iteration 1760 is 0.011657875790554393\n",
      "2021-02-24 01:54:13,571 Number of examples for the current task : 3361\n",
      "2021-02-24 01:54:24,826 Loss for iteration 160 is 0.05356538769873706\n",
      "2021-02-24 01:55:03,993 Loss for iteration 1760 is 0.011137039983500348\n",
      "2021-02-24 01:55:35,171 Number of examples for the current task : 2340\n",
      "2021-02-24 01:55:46,433 Loss for iteration 160 is 0.06286582206799225\n",
      "2021-02-24 01:56:24,668 Loss for iteration 1760 is 0.009839060025573877\n",
      "2021-02-24 01:56:35,431 Number of examples for the current task : 1696\n",
      "2021-02-24 01:56:46,473 Loss for iteration 160 is 0.031087752008302647\n",
      "2021-02-24 01:57:15,165 Number of examples for the current task : 4109\n",
      "2021-02-24 01:57:26,720 Loss for iteration 160 is 0.052815927361900154\n",
      "2021-02-24 01:58:07,191 Loss for iteration 1760 is 0.012493533328246863\n",
      "2021-02-24 01:58:46,675 Loss for iteration 3360 is 0.007233947205759466\n",
      "2021-02-24 01:59:01,242 Number of examples for the current task : 3663\n",
      "2021-02-24 01:59:12,529 Loss for iteration 160 is 0.05687520085749301\n",
      "2021-02-24 01:59:48,745 Loss for iteration 1760 is 0.010625718375022925\n",
      "2021-02-24 02:00:25,053 Loss for iteration 3360 is 0.006037568685095079\n",
      "2021-02-24 02:00:30,073 Number of examples for the current task : 2950\n",
      "2021-02-24 02:00:41,396 Loss for iteration 160 is 0.08552533532069488\n",
      "2021-02-24 02:01:19,466 Loss for iteration 1760 is 0.017009561273651115\n",
      "2021-02-24 02:01:41,104 Number of examples for the current task : 4341\n",
      "2021-02-24 02:01:52,238 Loss for iteration 160 is 0.05143183816901662\n",
      "2021-02-24 02:02:30,137 Loss for iteration 1760 is 0.011785905732173339\n",
      "2021-02-24 02:03:07,514 Loss for iteration 3360 is 0.007026716789278429\n",
      "2021-02-24 02:03:25,280 Number of examples for the current task : 2461\n",
      "2021-02-24 02:03:36,769 Loss for iteration 160 is 0.050029075450517914\n",
      "2021-02-24 02:04:16,131 Loss for iteration 1760 is 0.0082269503305921\n",
      "2021-02-24 02:04:29,382 Number of examples for the current task : 1499\n",
      "2021-02-24 02:04:40,430 Loss for iteration 160 is 0.043809378731318495\n",
      "2021-02-24 02:05:04,388 Number of examples for the current task : 5505\n",
      "2021-02-24 02:05:15,233 Loss for iteration 160 is 0.04719220237298445\n",
      "2021-02-24 02:05:51,588 Loss for iteration 1760 is 0.012833227829269326\n",
      "2021-02-24 02:06:27,777 Loss for iteration 3360 is 0.00754056842005933\n",
      "2021-02-24 02:07:04,455 Loss for iteration 4960 is 0.005697712482048096\n",
      "2021-02-24 02:07:13,852 Number of examples for the current task : 6293\n",
      "2021-02-24 02:07:25,033 Loss for iteration 160 is 0.0690794891085137\n",
      "2021-02-24 02:08:02,311 Loss for iteration 1760 is 0.019969571308392258\n",
      "2021-02-24 02:08:39,851 Loss for iteration 3360 is 0.011736303599404007\n",
      "2021-02-24 02:09:17,477 Loss for iteration 4960 is 0.008596347785063336\n",
      "2021-02-24 02:09:41,451 Number of examples for the current task : 5530\n",
      "2021-02-24 02:09:52,833 Loss for iteration 160 is 0.05421777137301185\n",
      "2021-02-24 02:10:32,086 Loss for iteration 1760 is 0.014612561234415651\n",
      "2021-02-24 02:11:11,395 Loss for iteration 3360 is 0.008620020414976943\n",
      "2021-02-24 02:11:51,642 Loss for iteration 4960 is 0.00609937768905356\n",
      "2021-02-24 02:12:02,782 Number of examples for the current task : 2130\n",
      "2021-02-24 02:12:14,299 Loss for iteration 160 is 0.059841362441974605\n",
      "2021-02-24 02:12:52,245 Loss for iteration 1760 is 0.011783470773701305\n",
      "2021-02-24 02:12:58,840 Number of examples for the current task : 4993\n",
      "2021-02-24 02:13:10,318 Loss for iteration 160 is 0.04256272011182525\n",
      "2021-02-24 02:13:50,025 Loss for iteration 1760 is 0.010113887055168717\n",
      "2021-02-24 02:14:29,110 Loss for iteration 3360 is 0.0059957349002835785\n",
      "2021-02-24 02:15:08,625 Loss for iteration 4960 is 0.004547952042166857\n",
      "2021-02-24 02:15:08,954 Number of examples for the current task : 10086\n",
      "2021-02-24 02:15:20,205 Loss for iteration 160 is 0.05842549523169344\n",
      "2021-02-24 02:15:57,603 Loss for iteration 1760 is 0.016409986072454835\n",
      "2021-02-24 02:16:34,883 Loss for iteration 3360 is 0.010536083199030839\n",
      "2021-02-24 02:17:11,673 Loss for iteration 4960 is 0.007848285362219896\n",
      "2021-02-24 02:17:48,594 Loss for iteration 6560 is 0.006158499365319541\n",
      "2021-02-24 02:18:26,042 Loss for iteration 8160 is 0.005211480276023318\n",
      "2021-02-24 02:19:03,434 Loss for iteration 9760 is 0.0049898496296670305\n",
      "2021-02-24 02:19:09,093 Number of examples for the current task : 4032\n",
      "2021-02-24 02:19:20,121 Loss for iteration 160 is 0.04486482150175355\n",
      "2021-02-24 02:19:57,682 Loss for iteration 1760 is 0.009413886578283735\n",
      "2021-02-24 02:20:35,488 Loss for iteration 3360 is 0.005633104898962297\n",
      "2021-02-24 02:20:47,561 Number of examples for the current task : 2485\n",
      "2021-02-24 02:20:59,030 Loss for iteration 160 is 0.04437773799608377\n",
      "2021-02-24 02:21:37,436 Loss for iteration 1760 is 0.008312197469766377\n",
      "2021-02-24 02:21:50,874 Number of examples for the current task : 3023\n",
      "2021-02-24 02:22:02,111 Loss for iteration 160 is 0.04690664244646376\n",
      "2021-02-24 02:22:38,634 Loss for iteration 1760 is 0.011547831015877408\n",
      "2021-02-24 02:23:00,697 Number of examples for the current task : 6365\n",
      "2021-02-24 02:23:12,352 Loss for iteration 160 is 0.03925207401202484\n",
      "2021-02-24 02:23:52,019 Loss for iteration 1760 is 0.012663139419437194\n",
      "2021-02-24 02:24:30,897 Loss for iteration 3360 is 0.007784211238556799\n",
      "2021-02-24 02:25:09,994 Loss for iteration 4960 is 0.0066470517994951265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-24 02:25:36,493 Number of examples for the current task : 3278\n",
      "2021-02-24 02:25:47,370 Loss for iteration 160 is 0.04456473620269786\n",
      "2021-02-24 02:26:22,074 Loss for iteration 1760 is 0.011015216258054532\n",
      "2021-02-24 02:26:46,556 Number of examples for the current task : 4699\n",
      "2021-02-24 02:26:58,076 Loss for iteration 160 is 0.053471390665932136\n",
      "2021-02-24 02:27:37,138 Loss for iteration 1760 is 0.01222338118440438\n",
      "2021-02-24 02:28:16,246 Loss for iteration 3360 is 0.006845397846402227\n",
      "2021-02-24 02:28:41,405 Number of examples for the current task : 5666\n",
      "2021-02-24 02:28:52,752 Loss for iteration 160 is 0.06742056332189929\n",
      "2021-02-24 02:29:29,136 Loss for iteration 1760 is 0.01655671819609271\n",
      "2021-02-24 02:30:05,541 Loss for iteration 3360 is 0.00971803157850333\n",
      "2021-02-24 02:30:41,808 Loss for iteration 4960 is 0.006885167152595211\n",
      "2021-02-24 02:30:53,900 Number of examples for the current task : 1717\n",
      "2021-02-24 02:31:05,707 Loss for iteration 160 is 0.06066585196690126\n",
      "2021-02-24 02:31:36,194 Number of examples for the current task : 2740\n",
      "2021-02-24 02:31:47,587 Loss for iteration 160 is 0.03383197245950049\n",
      "2021-02-24 02:32:26,050 Loss for iteration 1760 is 0.005983766718051262\n",
      "2021-02-24 02:32:43,927 Number of examples for the current task : 4601\n",
      "2021-02-24 02:32:55,404 Loss for iteration 160 is 0.0515749150548469\n",
      "2021-02-24 02:33:32,731 Loss for iteration 1760 is 0.011742946584834843\n",
      "2021-02-24 02:34:09,938 Loss for iteration 3360 is 0.006887809361679583\n",
      "2021-02-24 02:34:32,032 Number of examples for the current task : 3499\n",
      "2021-02-24 02:34:43,482 Loss for iteration 160 is 0.03668224735354835\n",
      "2021-02-24 02:35:20,761 Loss for iteration 1760 is 0.009236976334553523\n",
      "2021-02-24 02:35:58,648 Loss for iteration 3360 is 0.0054300416739619176\n",
      "2021-02-24 02:36:00,775 Number of examples for the current task : 4303\n",
      "2021-02-24 02:36:12,350 Loss for iteration 160 is 0.0639473565600135\n",
      "2021-02-24 02:36:49,249 Loss for iteration 1760 is 0.011748619831737701\n",
      "2021-02-24 02:37:26,399 Loss for iteration 3360 is 0.006667399841006054\n",
      "2021-02-24 02:37:42,677 Number of examples for the current task : 2820\n",
      "2021-02-24 02:37:54,048 Loss for iteration 160 is 0.03895278656008569\n",
      "2021-02-24 02:38:32,997 Loss for iteration 1760 is 0.007653172161267034\n",
      "2021-02-24 02:38:52,831 Number of examples for the current task : 2870\n",
      "2021-02-24 02:39:04,334 Loss for iteration 160 is 0.06391208449547942\n",
      "2021-02-24 02:39:42,285 Loss for iteration 1760 is 0.011512183061714896\n",
      "2021-02-24 02:40:02,536 Number of examples for the current task : 10298\n",
      "2021-02-24 02:40:13,756 Loss for iteration 160 is 0.05337541296400807\n",
      "2021-02-24 02:40:49,677 Loss for iteration 1760 is 0.017833745061956882\n",
      "2021-02-24 02:41:26,044 Loss for iteration 3360 is 0.011702993448401065\n",
      "2021-02-24 02:42:01,972 Loss for iteration 4960 is 0.008600181044911115\n",
      "2021-02-24 02:42:37,873 Loss for iteration 6560 is 0.006992467954622623\n",
      "2021-02-24 02:43:13,690 Loss for iteration 8160 is 0.0061140779671903095\n",
      "2021-02-24 02:43:49,824 Loss for iteration 9760 is 0.005701704241788773\n",
      "2021-02-24 02:43:58,699 Number of examples for the current task : 9698\n",
      "2021-02-24 02:44:10,179 Loss for iteration 160 is 0.04087951186705719\n",
      "2021-02-24 02:44:46,609 Loss for iteration 1760 is 0.01253856613451766\n",
      "2021-02-24 02:45:23,670 Loss for iteration 3360 is 0.008091601654509263\n",
      "2021-02-24 02:46:00,648 Loss for iteration 4960 is 0.006002491652509617\n",
      "2021-02-24 02:46:37,506 Loss for iteration 6560 is 0.005354119128467733\n",
      "2021-02-24 02:47:14,440 Loss for iteration 8160 is 0.00462169273839107\n",
      "2021-02-24 02:47:41,605 Number of examples for the current task : 2584\n",
      "2021-02-24 02:47:53,451 Loss for iteration 160 is 0.05453560361638665\n",
      "2021-02-24 02:48:32,870 Loss for iteration 1760 is 0.00957693595626404\n",
      "2021-02-24 02:48:48,478 Number of examples for the current task : 3120\n",
      "2021-02-24 02:49:00,471 Loss for iteration 160 is 0.03867041150277311\n",
      "2021-02-24 02:49:40,203 Loss for iteration 1760 is 0.008763547503779567\n",
      "2021-02-24 02:50:06,317 Number of examples for the current task : 4482\n",
      "2021-02-24 02:50:17,818 Loss for iteration 160 is 0.05430900559506633\n",
      "2021-02-24 02:50:53,630 Loss for iteration 1760 is 0.01164605775465224\n",
      "2021-02-24 02:51:29,591 Loss for iteration 3360 is 0.006985953500541534\n",
      "2021-02-24 02:51:48,467 Number of examples for the current task : 5184\n",
      "2021-02-24 02:52:00,401 Loss for iteration 160 is 0.051782614466818894\n",
      "2021-02-24 02:52:41,618 Loss for iteration 1760 is 0.014380054585090294\n",
      "2021-02-24 02:53:22,836 Loss for iteration 3360 is 0.00847393247582762\n",
      "2021-02-24 02:54:03,073 Loss for iteration 4960 is 0.006161693062193817\n",
      "2021-02-24 02:54:07,328 Number of examples for the current task : 5661\n",
      "2021-02-24 02:54:18,845 Loss for iteration 160 is 0.0413090694187717\n",
      "2021-02-24 02:54:55,415 Loss for iteration 1760 is 0.014803938849237622\n",
      "2021-02-24 02:55:31,931 Loss for iteration 3360 is 0.008914032045123388\n",
      "2021-02-24 02:56:08,482 Loss for iteration 4960 is 0.0063751751003259405\n",
      "2021-02-24 02:56:20,099 Number of examples for the current task : 6031\n",
      "2021-02-24 02:56:31,565 Loss for iteration 160 is 0.06613223864273592\n",
      "2021-02-24 02:57:08,733 Loss for iteration 1760 is 0.01596226648486278\n",
      "2021-02-24 02:57:46,235 Loss for iteration 3360 is 0.00963387012129133\n",
      "2021-02-24 02:58:23,948 Loss for iteration 4960 is 0.007049879016756332\n",
      "2021-02-24 02:58:42,960 Number of examples for the current task : 4022\n",
      "2021-02-24 02:58:54,253 Loss for iteration 160 is 0.05376916641200131\n",
      "2021-02-24 02:59:29,660 Loss for iteration 1760 is 0.010181857567953851\n",
      "2021-02-24 03:00:06,186 Loss for iteration 3360 is 0.00591546143125222\n",
      "2021-02-24 03:00:17,299 Number of examples for the current task : 2955\n",
      "2021-02-24 03:00:29,048 Loss for iteration 160 is 0.04209841601550579\n",
      "2021-02-24 03:01:07,546 Loss for iteration 1760 is 0.0096660600804728\n",
      "2021-02-24 03:01:29,662 Number of examples for the current task : 2851\n",
      "2021-02-24 03:01:41,571 Loss for iteration 160 is 0.05639578300443562\n",
      "2021-02-24 03:02:21,529 Loss for iteration 1760 is 0.010279405801656639\n",
      "2021-02-24 03:02:42,594 Number of examples for the current task : 1268\n",
      "2021-02-24 03:02:54,490 Loss for iteration 160 is 0.04855087314817039\n",
      "2021-02-24 03:03:15,135 Number of examples for the current task : 2719\n",
      "2021-02-24 03:03:27,081 Loss for iteration 160 is 0.024888520171357828\n",
      "2021-02-24 03:04:05,707 Loss for iteration 1760 is 0.005427707776664295\n",
      "2021-02-24 03:04:22,841 Number of examples for the current task : 1928\n",
      "2021-02-24 03:04:34,639 Loss for iteration 160 is 0.084385977930982\n",
      "2021-02-24 03:05:12,885 Loss for iteration 1760 is 0.013132983274329957\n",
      "2021-02-24 03:05:15,542 Number of examples for the current task : 2219\n",
      "2021-02-24 03:05:27,585 Loss for iteration 160 is 0.03238890878856182\n",
      "2021-02-24 03:06:08,480 Loss for iteration 1760 is 0.006646815106783457\n",
      "2021-02-24 03:06:17,259 Number of examples for the current task : 2190\n",
      "2021-02-24 03:06:28,950 Loss for iteration 160 is 0.04918028295717456\n",
      "2021-02-24 03:07:06,039 Loss for iteration 1760 is 0.009299363745093296\n",
      "2021-02-24 03:07:13,341 Number of examples for the current task : 1569\n",
      "2021-02-24 03:07:25,216 Loss for iteration 160 is 0.04125065521591089\n",
      "2021-02-24 03:07:51,832 Number of examples for the current task : 3669\n",
      "2021-02-24 03:08:03,706 Loss for iteration 160 is 0.044189004227519035\n",
      "2021-02-24 03:08:42,299 Loss for iteration 1760 is 0.01005470700381609\n",
      "2021-02-24 03:09:20,937 Loss for iteration 3360 is 0.005636547532686947\n",
      "2021-02-24 03:09:26,382 Number of examples for the current task : 3401\n",
      "2021-02-24 03:09:38,274 Loss for iteration 160 is 0.06315595487302\n",
      "2021-02-24 03:10:17,125 Loss for iteration 1760 is 0.012233013795848164\n",
      "2021-02-24 03:10:55,526 Loss for iteration 3360 is 0.00700171884463524\n",
      "2021-02-24 03:10:55,855 Number of examples for the current task : 1873\n",
      "2021-02-24 03:11:07,686 Loss for iteration 160 is 0.06879501273347573\n",
      "2021-02-24 03:11:46,155 Loss for iteration 1760 is 0.011500284950162896\n",
      "2021-02-24 03:11:47,973 Number of examples for the current task : 5256\n",
      "2021-02-24 03:11:59,585 Loss for iteration 160 is 0.05470048924061385\n",
      "2021-02-24 03:12:37,529 Loss for iteration 1760 is 0.013539873221406696\n",
      "2021-02-24 03:13:15,324 Loss for iteration 3360 is 0.007681448790152343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-24 03:13:53,198 Loss for iteration 4960 is 0.0053059097967201865\n",
      "2021-02-24 03:13:58,562 Number of examples for the current task : 3154\n",
      "2021-02-24 03:14:10,672 Loss for iteration 160 is 0.06742343450473114\n",
      "2021-02-24 03:14:48,511 Loss for iteration 1760 is 0.014261822797502769\n",
      "2021-02-24 03:15:13,762 Number of examples for the current task : 7316\n",
      "2021-02-24 03:15:25,755 Loss for iteration 160 is 0.05450344492088665\n",
      "2021-02-24 03:16:05,217 Loss for iteration 1760 is 0.016775588797581557\n",
      "2021-02-24 03:16:44,459 Loss for iteration 3360 is 0.009700018508272616\n",
      "2021-02-24 03:17:23,749 Loss for iteration 4960 is 0.008475159669907537\n",
      "2021-02-24 03:18:03,553 Loss for iteration 6560 is 0.006976408184897379\n",
      "2021-02-24 03:18:17,857 Number of examples for the current task : 2252\n",
      "2021-02-24 03:18:29,816 Loss for iteration 160 is 0.05388251573524692\n",
      "2021-02-24 03:19:07,728 Loss for iteration 1760 is 0.008617087391227157\n",
      "2021-02-24 03:19:16,398 Number of examples for the current task : 4730\n",
      "2021-02-24 03:19:28,434 Loss for iteration 160 is 0.052753687582232735\n",
      "2021-02-24 03:20:06,756 Loss for iteration 1760 is 0.012141052279558435\n",
      "2021-02-24 03:20:44,878 Loss for iteration 3360 is 0.007293382957006663\n",
      "2021-02-24 03:21:09,371 Number of examples for the current task : 7246\n",
      "2021-02-24 03:21:21,385 Loss for iteration 160 is 0.07294600355354222\n",
      "2021-02-24 03:22:01,028 Loss for iteration 1760 is 0.01796537562827202\n",
      "2021-02-24 03:22:40,732 Loss for iteration 3360 is 0.010379098127008487\n",
      "2021-02-24 03:23:20,416 Loss for iteration 4960 is 0.007445105391911454\n",
      "2021-02-24 03:23:59,993 Loss for iteration 6560 is 0.006583607365625126\n",
      "2021-02-24 03:24:12,568 Number of examples for the current task : 4329\n",
      "2021-02-24 03:24:24,853 Loss for iteration 160 is 0.05462620762938803\n",
      "2021-02-24 03:25:03,398 Loss for iteration 1760 is 0.013561850057616998\n",
      "2021-02-24 03:25:41,326 Loss for iteration 3360 is 0.007603729922714493\n",
      "2021-02-24 03:25:59,041 Number of examples for the current task : 2851\n",
      "2021-02-24 03:26:11,317 Loss for iteration 160 is 0.05247051268815994\n",
      "2021-02-24 03:26:50,037 Loss for iteration 1760 is 0.009474161377560499\n",
      "2021-02-24 03:27:10,090 Number of examples for the current task : 3341\n",
      "2021-02-24 03:27:22,658 Loss for iteration 160 is 0.04515979142690247\n",
      "2021-02-24 03:28:02,695 Loss for iteration 1760 is 0.00830235408969341\n",
      "2021-02-24 03:28:31,947 Number of examples for the current task : 1893\n",
      "2021-02-24 03:28:43,100 Loss for iteration 160 is 0.042875821109522476\n",
      "2021-02-24 03:29:18,163 Loss for iteration 1760 is 0.006612719428721083\n",
      "2021-02-24 03:29:20,251 Number of examples for the current task : 3762\n",
      "2021-02-24 03:29:30,721 Loss for iteration 160 is 0.05440317568453876\n",
      "2021-02-24 03:30:08,983 Loss for iteration 1760 is 0.013489951247974459\n",
      "2021-02-24 03:30:47,036 Loss for iteration 3360 is 0.007656517912527785\n",
      "2021-02-24 03:30:54,589 Number of examples for the current task : 3143\n",
      "2021-02-24 03:31:04,625 Loss for iteration 160 is 0.0542692570862445\n",
      "2021-02-24 03:31:40,612 Loss for iteration 1760 is 0.009953575737764933\n",
      "2021-02-24 03:32:06,422 Number of examples for the current task : 7224\n",
      "2021-02-24 03:32:16,665 Loss for iteration 160 is 0.060030980882319535\n",
      "2021-02-24 03:32:53,419 Loss for iteration 1760 is 0.014907521544431278\n",
      "2021-02-24 03:33:30,255 Loss for iteration 3360 is 0.009153858210639895\n",
      "2021-02-24 03:34:06,990 Loss for iteration 4960 is 0.006696820807656991\n",
      "2021-02-24 03:34:43,743 Loss for iteration 6560 is 0.0061037111717337595\n",
      "2021-02-24 03:34:55,545 Number of examples for the current task : 3510\n",
      "2021-02-24 03:35:06,041 Loss for iteration 160 is 0.037147409028627655\n",
      "2021-02-24 03:35:43,760 Loss for iteration 1760 is 0.01003504933019563\n",
      "2021-02-24 03:36:21,708 Loss for iteration 3360 is 0.005486784903093278\n",
      "2021-02-24 03:36:24,279 Number of examples for the current task : 1033\n",
      "2021-02-24 03:36:34,826 Loss for iteration 160 is 0.05644442903047258\n",
      "2021-02-24 03:36:53,371 Number of examples for the current task : 1702\n",
      "2021-02-24 03:37:03,695 Loss for iteration 160 is 0.03760294893502512\n",
      "2021-02-24 03:37:32,993 Number of examples for the current task : 4793\n",
      "2021-02-24 03:37:43,233 Loss for iteration 160 is 0.050361659547144715\n",
      "2021-02-24 03:38:19,764 Loss for iteration 1760 is 0.015720355349618034\n",
      "2021-02-24 03:38:56,559 Loss for iteration 3360 is 0.008979158875310653\n",
      "2021-02-24 03:39:23,151 Number of examples for the current task : 1332\n",
      "2021-02-24 03:39:33,492 Loss for iteration 160 is 0.05100516877560453\n",
      "2021-02-24 03:39:54,370 Number of examples for the current task : 2591\n",
      "2021-02-24 03:40:04,756 Loss for iteration 160 is 0.041041509586979046\n",
      "2021-02-24 03:40:42,341 Loss for iteration 1760 is 0.009289946718569341\n",
      "2021-02-24 03:40:57,558 Number of examples for the current task : 3359\n",
      "2021-02-24 03:41:07,639 Loss for iteration 160 is 0.06182666444642977\n",
      "2021-02-24 03:41:44,311 Loss for iteration 1760 is 0.01225179059799081\n",
      "2021-02-24 03:42:12,862 Number of examples for the current task : 2870\n",
      "2021-02-24 03:42:22,989 Loss for iteration 160 is 0.03046023583208973\n",
      "2021-02-24 03:42:59,568 Loss for iteration 1760 is 0.006100303349625277\n",
      "2021-02-24 03:43:19,667 Number of examples for the current task : 3387\n",
      "2021-02-24 03:43:29,691 Loss for iteration 160 is 0.07362579215656627\n",
      "2021-02-24 03:44:05,452 Loss for iteration 1760 is 0.013105804544531632\n",
      "2021-02-24 03:44:40,487 Loss for iteration 3360 is 0.007463313335401361\n",
      "2021-02-24 03:44:40,495 Number of examples for the current task : 3979\n",
      "2021-02-24 03:44:50,752 Loss for iteration 160 is 0.05671523121947592\n",
      "2021-02-24 03:45:28,360 Loss for iteration 1760 is 0.012961621786916611\n",
      "2021-02-24 03:46:05,604 Loss for iteration 3360 is 0.007777636912742396\n",
      "2021-02-24 03:46:16,959 Number of examples for the current task : 2122\n",
      "2021-02-24 03:46:27,132 Loss for iteration 160 is 0.05706390933218328\n",
      "2021-02-24 03:47:03,354 Loss for iteration 1760 is 0.009882990149587082\n",
      "2021-02-24 03:47:09,194 Number of examples for the current task : 3198\n",
      "2021-02-24 03:47:19,669 Loss for iteration 160 is 0.03807201075621627\n",
      "2021-02-24 03:47:55,709 Loss for iteration 1760 is 0.007870792269681555\n",
      "2021-02-24 03:48:20,835 Number of examples for the current task : 2509\n",
      "2021-02-24 03:48:30,935 Loss for iteration 160 is 0.05213709573515437\n",
      "2021-02-24 03:49:04,846 Loss for iteration 1760 is 0.009378638804018674\n",
      "2021-02-24 03:49:16,786 Number of examples for the current task : 1595\n",
      "2021-02-24 03:49:27,006 Loss for iteration 160 is 0.055424127727746964\n",
      "2021-02-24 03:49:53,406 Number of examples for the current task : 3288\n",
      "2021-02-24 03:50:03,925 Loss for iteration 160 is 0.0567139077254317\n",
      "2021-02-24 03:50:41,077 Loss for iteration 1760 is 0.010501052726803217\n",
      "2021-02-24 03:51:09,934 Number of examples for the current task : 1636\n",
      "2021-02-24 03:51:19,849 Loss for iteration 160 is 0.03178179475732825\n",
      "2021-02-24 03:51:46,592 Number of examples for the current task : 1102\n",
      "2021-02-24 03:51:56,907 Loss for iteration 160 is 0.05074149353260344\n",
      "2021-02-24 03:52:14,239 Number of examples for the current task : 3638\n",
      "2021-02-24 03:52:24,383 Loss for iteration 160 is 0.044970005242661995\n",
      "2021-02-24 03:52:59,896 Loss for iteration 1760 is 0.010278715397313086\n",
      "2021-02-24 03:53:35,184 Loss for iteration 3360 is 0.005703863265708601\n",
      "2021-02-24 03:53:39,657 Number of examples for the current task : 3860\n",
      "2021-02-24 03:53:49,807 Loss for iteration 160 is 0.08829002217812972\n",
      "2021-02-24 03:54:26,658 Loss for iteration 1760 is 0.0183065931788889\n",
      "2021-02-24 03:55:03,410 Loss for iteration 3360 is 0.010549759037996272\n",
      "2021-02-24 03:55:12,165 Number of examples for the current task : 3574\n",
      "2021-02-24 03:55:22,263 Loss for iteration 160 is 0.039519781712442636\n",
      "2021-02-24 03:55:58,675 Loss for iteration 1760 is 0.007657153861228007\n",
      "2021-02-24 03:56:34,843 Loss for iteration 3360 is 0.004517181436149172\n",
      "2021-02-24 03:56:38,387 Number of examples for the current task : 2329\n",
      "2021-02-24 03:56:48,936 Loss for iteration 160 is 0.031323743645440445\n",
      "2021-02-24 03:57:27,354 Loss for iteration 1760 is 0.006667748114646324\n",
      "2021-02-24 03:57:38,058 Number of examples for the current task : 6001\n",
      "2021-02-24 03:57:48,260 Loss for iteration 160 is 0.06749936167828062\n",
      "2021-02-24 03:58:23,630 Loss for iteration 1760 is 0.016259690855738817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-24 03:58:58,588 Loss for iteration 3360 is 0.009295294008612792\n",
      "2021-02-24 03:59:34,008 Loss for iteration 4960 is 0.006459851631024588\n",
      "2021-02-24 03:59:51,956 Number of examples for the current task : 4761\n",
      "2021-02-24 04:00:02,672 Loss for iteration 160 is 0.06732241352173415\n",
      "2021-02-24 04:00:41,752 Loss for iteration 1760 is 0.015876235186567042\n",
      "2021-02-24 04:01:20,490 Loss for iteration 3360 is 0.009177529322307875\n",
      "2021-02-24 04:01:47,838 Number of examples for the current task : 7654\n",
      "2021-02-24 04:01:58,176 Loss for iteration 160 is 0.06294238347221505\n",
      "2021-02-24 04:02:35,714 Loss for iteration 1760 is 0.017580451236131625\n",
      "2021-02-24 04:03:13,394 Loss for iteration 3360 is 0.011277971025920468\n",
      "2021-02-24 04:03:50,414 Loss for iteration 4960 is 0.008381235924044248\n",
      "2021-02-24 04:04:27,593 Loss for iteration 6560 is 0.0067597823940504145\n",
      "2021-02-24 04:04:47,623 Number of examples for the current task : 1717\n",
      "2021-02-24 04:04:57,754 Loss for iteration 160 is 0.04434698151255196\n",
      "2021-02-24 04:05:25,840 Number of examples for the current task : 2370\n",
      "2021-02-24 04:05:36,244 Loss for iteration 160 is 0.04711225357922641\n",
      "2021-02-24 04:06:13,665 Loss for iteration 1760 is 0.008719510437610248\n",
      "2021-02-24 04:06:24,899 Number of examples for the current task : 6620\n",
      "2021-02-24 04:06:35,008 Loss for iteration 160 is 0.07168819467452439\n",
      "2021-02-24 04:07:10,546 Loss for iteration 1760 is 0.016762640419508423\n",
      "2021-02-24 04:07:45,985 Loss for iteration 3360 is 0.009875781564880604\n",
      "2021-02-24 04:08:21,594 Loss for iteration 4960 is 0.0071211161149452905\n",
      "2021-02-24 04:08:57,038 Loss for iteration 6560 is 0.006036414888061381\n",
      "2021-02-24 04:08:57,638 Number of examples for the current task : 2180\n",
      "2021-02-24 04:09:08,200 Loss for iteration 160 is 0.03326994822021912\n",
      "2021-02-24 04:09:44,965 Loss for iteration 1760 is 0.006946272888747338\n",
      "2021-02-24 04:09:52,361 Number of examples for the current task : 1899\n",
      "2021-02-24 04:10:02,900 Loss for iteration 160 is 0.037385100456462664\n",
      "2021-02-24 04:10:40,585 Loss for iteration 1760 is 0.00750434779518007\n",
      "2021-02-24 04:10:42,860 Number of examples for the current task : 1609\n",
      "2021-02-24 04:10:52,814 Loss for iteration 160 is 0.03718348647552458\n",
      "2021-02-24 04:11:17,106 Number of examples for the current task : 2611\n",
      "2021-02-24 04:11:27,335 Loss for iteration 160 is 0.05068139850415967\n",
      "2021-02-24 04:12:03,139 Loss for iteration 1760 is 0.010054489712782524\n",
      "2021-02-24 04:12:18,466 Number of examples for the current task : 7704\n",
      "2021-02-24 04:12:29,115 Loss for iteration 160 is 0.041904512623494324\n",
      "2021-02-24 04:13:08,080 Loss for iteration 1760 is 0.014945410353525935\n",
      "2021-02-24 04:13:47,345 Loss for iteration 3360 is 0.009479707273088011\n",
      "2021-02-24 04:14:26,104 Loss for iteration 4960 is 0.007184509802146922\n",
      "2021-02-24 04:15:06,050 Loss for iteration 6560 is 0.006027982976259491\n",
      "2021-02-24 04:15:28,155 Number of examples for the current task : 2478\n",
      "2021-02-24 04:15:38,834 Loss for iteration 160 is 0.0520145691592585\n",
      "2021-02-24 04:16:16,418 Loss for iteration 1760 is 0.008983379607204231\n",
      "2021-02-24 04:16:29,346 Number of examples for the current task : 1721\n",
      "2021-02-24 04:16:39,963 Loss for iteration 160 is 0.04678012007339434\n",
      "2021-02-24 04:17:07,482 Number of examples for the current task : 10888\n",
      "2021-02-24 04:17:18,067 Loss for iteration 160 is 0.04370103810321201\n",
      "2021-02-24 04:17:56,458 Loss for iteration 1760 is 0.012777428608387709\n",
      "2021-02-24 04:18:34,023 Loss for iteration 3360 is 0.008322117258866332\n",
      "2021-02-24 04:19:11,768 Loss for iteration 4960 is 0.006388983762967348\n",
      "2021-02-24 04:19:49,598 Loss for iteration 6560 is 0.00504535648548268\n",
      "2021-02-24 04:20:27,593 Loss for iteration 8160 is 0.004434960718773733\n",
      "2021-02-24 04:21:05,789 Loss for iteration 9760 is 0.004401943062437064\n",
      "2021-02-24 04:21:26,863 Number of examples for the current task : 2028\n",
      "2021-02-24 04:21:37,276 Loss for iteration 160 is 0.03567611476914449\n",
      "2021-02-24 04:22:13,044 Loss for iteration 1760 is 0.006462190736390374\n",
      "2021-02-24 04:22:17,322 Number of examples for the current task : 3178\n",
      "2021-02-24 04:22:27,716 Loss for iteration 160 is 0.054073604331775146\n",
      "2021-02-24 04:23:03,906 Loss for iteration 1760 is 0.01052997625673605\n",
      "2021-02-24 04:23:29,185 Number of examples for the current task : 1962\n",
      "2021-02-24 04:23:39,776 Loss for iteration 160 is 0.04669198910282417\n",
      "2021-02-24 04:24:16,949 Loss for iteration 1760 is 0.008142343018416877\n",
      "2021-02-24 04:24:20,182 Number of examples for the current task : 4821\n",
      "2021-02-24 04:24:30,670 Loss for iteration 160 is 0.05040119520642541\n",
      "2021-02-24 04:25:07,856 Loss for iteration 1760 is 0.013176448131661845\n",
      "2021-02-24 04:25:45,069 Loss for iteration 3360 is 0.007577589470102789\n",
      "2021-02-24 04:26:11,622 Number of examples for the current task : 3917\n",
      "2021-02-24 04:26:21,999 Loss for iteration 160 is 0.07434785095128146\n",
      "2021-02-24 04:26:55,796 Loss for iteration 1760 is 0.012846900066539125\n",
      "2021-02-24 04:27:29,679 Loss for iteration 3360 is 0.0074956774767057895\n",
      "2021-02-24 04:27:38,417 Number of examples for the current task : 1813\n",
      "2021-02-24 04:27:49,477 Loss for iteration 160 is 0.039017288606952534\n",
      "2021-02-24 04:28:29,882 Loss for iteration 1760 is 0.007052875663256609\n",
      "2021-02-24 04:28:30,604 Number of examples for the current task : 3387\n",
      "2021-02-24 04:28:41,094 Loss for iteration 160 is 0.04571815673261881\n",
      "2021-02-24 04:29:18,621 Loss for iteration 1760 is 0.008204458771640045\n",
      "2021-02-24 04:29:56,744 Loss for iteration 3360 is 0.004562257210192773\n",
      "2021-02-24 04:29:56,751 Number of examples for the current task : 2918\n",
      "2021-02-24 04:30:07,639 Loss for iteration 160 is 0.06853717362338846\n",
      "2021-02-24 04:30:44,183 Loss for iteration 1760 is 0.01139424511059481\n",
      "2021-02-24 04:31:04,785 Number of examples for the current task : 2061\n",
      "2021-02-24 04:31:15,284 Loss for iteration 160 is 0.04540470572696491\n",
      "2021-02-24 04:31:52,154 Loss for iteration 1760 is 0.007888222746259999\n",
      "2021-02-24 04:31:57,341 Number of examples for the current task : 2945\n",
      "2021-02-24 04:32:08,299 Loss for iteration 160 is 0.037785771895538674\n",
      "2021-02-24 04:32:46,607 Loss for iteration 1760 is 0.007737078916712513\n",
      "2021-02-24 04:33:09,617 Number of examples for the current task : 2725\n",
      "2021-02-24 04:33:20,490 Loss for iteration 160 is 0.03264686880125241\n",
      "2021-02-24 04:34:00,255 Loss for iteration 1760 is 0.007129748133331042\n",
      "2021-02-24 04:34:19,226 Number of examples for the current task : 5565\n",
      "2021-02-24 04:34:30,008 Loss for iteration 160 is 0.06449750336733731\n",
      "2021-02-24 04:35:08,493 Loss for iteration 1760 is 0.01551300379959447\n",
      "2021-02-24 04:35:46,912 Loss for iteration 3360 is 0.009103910331225346\n",
      "2021-02-24 04:36:25,749 Loss for iteration 4960 is 0.006581327060357112\n",
      "2021-02-24 04:36:37,368 Number of examples for the current task : 1859\n",
      "2021-02-24 04:36:47,865 Loss for iteration 160 is 0.06364210830493407\n",
      "2021-02-24 04:37:24,425 Loss for iteration 1760 is 0.01051881322115429\n",
      "2021-02-24 04:37:25,992 Number of examples for the current task : 1707\n",
      "2021-02-24 04:37:36,604 Loss for iteration 160 is 0.031534705734388394\n",
      "2021-02-24 04:38:03,436 Number of examples for the current task : 3475\n",
      "2021-02-24 04:38:13,945 Loss for iteration 160 is 0.05780633437362584\n",
      "2021-02-24 04:38:50,491 Loss for iteration 1760 is 0.011334504938720062\n",
      "2021-02-24 04:39:26,840 Loss for iteration 3360 is 0.006217534191112235\n",
      "2021-02-24 04:39:28,720 Number of examples for the current task : 14955\n",
      "2021-02-24 04:39:39,215 Loss for iteration 160 is 0.05704507248645479\n",
      "2021-02-24 04:40:17,776 Loss for iteration 1760 is 0.02106846956460661\n",
      "2021-02-24 04:40:54,877 Loss for iteration 3360 is 0.015074662073577149\n",
      "2021-02-24 04:41:32,174 Loss for iteration 4960 is 0.012096431059351077\n",
      "2021-02-24 04:42:10,100 Loss for iteration 6560 is 0.010287441011606613\n",
      "2021-02-24 04:42:48,396 Loss for iteration 8160 is 0.00919952066121427\n",
      "2021-02-24 04:43:26,083 Loss for iteration 9760 is 0.008264831642812893\n",
      "2021-02-24 04:44:04,172 Loss for iteration 11360 is 0.007497040309928973\n",
      "2021-02-24 04:44:42,043 Loss for iteration 12960 is 0.006846040032055369\n",
      "2021-02-24 04:45:19,193 Loss for iteration 14560 is 0.00658546509894362\n",
      "2021-02-24 04:45:26,137 Number of examples for the current task : 3955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-24 04:45:36,610 Loss for iteration 160 is 0.03120885645462708\n",
      "2021-02-24 04:46:12,932 Loss for iteration 1760 is 0.008673550253382649\n",
      "2021-02-24 04:46:49,446 Loss for iteration 3360 is 0.005098847587462829\n",
      "2021-02-24 04:47:00,126 Number of examples for the current task : 5891\n",
      "2021-02-24 04:47:10,998 Loss for iteration 160 is 0.04099399655718695\n",
      "2021-02-24 04:47:47,707 Loss for iteration 1760 is 0.011830119026493415\n",
      "2021-02-24 04:48:24,937 Loss for iteration 3360 is 0.007361434114215811\n",
      "2021-02-24 04:49:02,563 Loss for iteration 4960 is 0.005227905399312105\n",
      "2021-02-24 04:49:19,931 Number of examples for the current task : 1100\n",
      "2021-02-24 04:49:31,051 Loss for iteration 160 is 0.06862872178581628\n",
      "2021-02-24 04:49:49,944 Number of examples for the current task : 974\n",
      "2021-02-24 04:50:00,274 Loss for iteration 160 is 0.023375564635815946\n",
      "2021-02-24 04:50:14,289 Number of examples for the current task : 2607\n",
      "2021-02-24 04:50:24,927 Loss for iteration 160 is 0.04199383560229431\n",
      "2021-02-24 04:51:03,050 Loss for iteration 1760 is 0.008529044415260065\n",
      "2021-02-24 04:51:18,535 Number of examples for the current task : 2578\n",
      "2021-02-24 04:51:29,153 Loss for iteration 160 is 0.044710781827399675\n",
      "2021-02-24 04:52:05,613 Loss for iteration 1760 is 0.0079669178072059\n",
      "2021-02-24 04:52:20,155 Number of examples for the current task : 1900\n",
      "2021-02-24 04:52:30,747 Loss for iteration 160 is 0.03255323186220432\n",
      "2021-02-24 04:53:07,443 Loss for iteration 1760 is 0.005910103938092377\n",
      "2021-02-24 04:53:09,506 Number of examples for the current task : 2119\n",
      "2021-02-24 04:53:20,085 Loss for iteration 160 is 0.05897117109799927\n",
      "2021-02-24 04:53:57,152 Loss for iteration 1760 is 0.010152969501235077\n",
      "2021-02-24 04:54:03,313 Number of examples for the current task : 1269\n",
      "2021-02-24 04:54:14,179 Loss for iteration 160 is 0.031051116360520773\n",
      "2021-02-24 04:54:34,837 Number of examples for the current task : 3003\n",
      "2021-02-24 04:54:45,137 Loss for iteration 160 is 0.040461429437114435\n",
      "2021-02-24 04:55:20,286 Loss for iteration 1760 is 0.008566924656488563\n",
      "2021-02-24 04:55:41,401 Number of examples for the current task : 1724\n",
      "2021-02-24 04:55:52,247 Loss for iteration 160 is 0.05130326671695167\n",
      "2021-02-24 04:56:20,778 Number of examples for the current task : 1931\n",
      "2021-02-24 04:56:31,797 Loss for iteration 160 is 0.03621815703809261\n",
      "2021-02-24 04:57:11,622 Loss for iteration 1760 is 0.007596922343607829\n",
      "2021-02-24 04:57:14,566 Number of examples for the current task : 1325\n",
      "2021-02-24 04:57:25,462 Loss for iteration 160 is 0.025843095415356485\n",
      "2021-02-24 04:57:47,946 Number of examples for the current task : 6122\n",
      "2021-02-24 04:57:58,996 Loss for iteration 160 is 0.057410277764905586\n",
      "2021-02-24 04:58:37,587 Loss for iteration 1760 is 0.016083528459709777\n",
      "2021-02-24 04:59:16,384 Loss for iteration 3360 is 0.00972584217675891\n",
      "2021-02-24 04:59:55,239 Loss for iteration 4960 is 0.007180966608233973\n",
      "2021-02-24 05:00:17,467 Number of examples for the current task : 2600\n",
      "2021-02-24 05:00:28,593 Loss for iteration 160 is 0.05241446586495096\n",
      "2021-02-24 05:01:09,334 Loss for iteration 1760 is 0.00865959434966433\n",
      "2021-02-24 05:01:26,065 Number of examples for the current task : 1038\n",
      "2021-02-24 05:01:36,874 Loss for iteration 160 is 0.04575034019283273\n",
      "2021-02-24 05:01:51,666 Number of examples for the current task : 3643\n",
      "2021-02-24 05:02:02,260 Loss for iteration 160 is 0.0613408371467482\n",
      "2021-02-24 05:02:39,352 Loss for iteration 1760 is 0.013023366086575127\n",
      "2021-02-24 05:03:16,283 Loss for iteration 3360 is 0.007238370362032394\n",
      "2021-02-24 05:03:21,179 Number of examples for the current task : 2098\n",
      "2021-02-24 05:03:32,391 Loss for iteration 160 is 0.04041136852042242\n",
      "2021-02-24 05:04:12,147 Loss for iteration 1760 is 0.00756582473761293\n",
      "2021-02-24 05:04:18,446 Number of examples for the current task : 2791\n",
      "2021-02-24 05:04:29,049 Loss for iteration 160 is 0.053638901582665065\n",
      "2021-02-24 05:05:05,465 Loss for iteration 1760 is 0.01163105488854968\n",
      "2021-02-24 05:05:23,839 Number of examples for the current task : 1157\n",
      "2021-02-24 05:05:34,790 Loss for iteration 160 is 0.023753111938607286\n",
      "2021-02-24 05:05:53,875 Number of examples for the current task : 3088\n",
      "2021-02-24 05:06:04,949 Loss for iteration 160 is 0.05170205599543723\n",
      "2021-02-24 05:06:43,635 Loss for iteration 1760 is 0.011813647155954109\n",
      "2021-02-24 05:07:09,697 Number of examples for the current task : 2430\n",
      "2021-02-24 05:07:20,431 Loss for iteration 160 is 0.04727708188478242\n",
      "2021-02-24 05:07:55,316 Loss for iteration 1760 is 0.00966928430244553\n",
      "2021-02-24 05:08:06,307 Number of examples for the current task : 2148\n",
      "2021-02-24 05:08:17,029 Loss for iteration 160 is 0.041438724464652216\n",
      "2021-02-24 05:08:52,377 Loss for iteration 1760 is 0.008175225209679166\n",
      "2021-02-24 05:08:58,956 Number of examples for the current task : 2448\n",
      "2021-02-24 05:09:09,481 Loss for iteration 160 is 0.028792454353110356\n",
      "2021-02-24 05:09:46,685 Loss for iteration 1760 is 0.006639165984705795\n",
      "2021-02-24 05:09:58,815 Number of examples for the current task : 1837\n",
      "2021-02-24 05:10:09,765 Loss for iteration 160 is 0.04237990995699709\n",
      "2021-02-24 05:10:49,070 Loss for iteration 1760 is 0.007740439830068918\n",
      "2021-02-24 05:10:50,077 Number of examples for the current task : 4734\n",
      "2021-02-24 05:11:01,148 Loss for iteration 160 is 0.05768475512211973\n",
      "2021-02-24 05:11:40,294 Loss for iteration 1760 is 0.013040217521143154\n",
      "2021-02-24 05:12:19,548 Loss for iteration 3360 is 0.008076202341601455\n",
      "2021-02-24 05:12:45,642 Number of examples for the current task : 4500\n",
      "2021-02-24 05:12:56,477 Loss for iteration 160 is 0.04405404220927845\n",
      "2021-02-24 05:13:31,590 Loss for iteration 1760 is 0.009896547827494188\n",
      "2021-02-24 05:14:07,296 Loss for iteration 3360 is 0.005851643207779273\n",
      "2021-02-24 05:14:26,344 Number of examples for the current task : 3316\n",
      "2021-02-24 05:14:37,165 Loss for iteration 160 is 0.04960166163403879\n",
      "2021-02-24 05:15:12,883 Loss for iteration 1760 is 0.010180625656036543\n",
      "2021-02-24 05:15:39,932 Number of examples for the current task : 1446\n",
      "2021-02-24 05:15:51,095 Loss for iteration 160 is 0.04488262411376292\n",
      "2021-02-24 05:16:14,355 Number of examples for the current task : 3443\n",
      "2021-02-24 05:16:25,094 Loss for iteration 160 is 0.04248815876516429\n",
      "2021-02-24 05:17:02,099 Loss for iteration 1760 is 0.010466279467092702\n",
      "2021-02-24 05:17:40,189 Loss for iteration 3360 is 0.0057687837291298975\n",
      "2021-02-24 05:17:41,444 Number of examples for the current task : 3897\n",
      "2021-02-24 05:17:52,408 Loss for iteration 160 is 0.05921992600302805\n",
      "2021-02-24 05:18:28,690 Loss for iteration 1760 is 0.01119397784180012\n",
      "2021-02-24 05:19:05,573 Loss for iteration 3360 is 0.0062751705219801016\n",
      "2021-02-24 05:19:15,096 Number of examples for the current task : 2797\n",
      "2021-02-24 05:19:25,819 Loss for iteration 160 is 0.061493063176220115\n",
      "2021-02-24 05:20:00,362 Loss for iteration 1760 is 0.009987095893315577\n",
      "2021-02-24 05:20:16,995 Number of examples for the current task : 1700\n",
      "2021-02-24 05:20:28,385 Loss for iteration 160 is 0.03796864057552408\n",
      "2021-02-24 05:20:57,361 Number of examples for the current task : 927\n",
      "2021-02-24 05:21:07,833 Loss for iteration 160 is 0.03262638147200712\n",
      "2021-02-24 05:21:20,324 Number of examples for the current task : 2966\n",
      "2021-02-24 05:21:31,657 Loss for iteration 160 is 0.04541195573454553\n",
      "2021-02-24 05:22:12,044 Loss for iteration 1760 is 0.008875507511343254\n",
      "2021-02-24 05:22:36,014 Number of examples for the current task : 3016\n",
      "2021-02-24 05:22:47,100 Loss for iteration 160 is 0.07228494689545849\n",
      "2021-02-24 05:23:24,670 Loss for iteration 1760 is 0.012692144094978937\n",
      "2021-02-24 05:23:47,288 Number of examples for the current task : 3085\n",
      "2021-02-24 05:23:58,482 Loss for iteration 160 is 0.03310298479416154\n",
      "2021-02-24 05:24:35,043 Loss for iteration 1760 is 0.009333000146848913\n",
      "2021-02-24 05:24:58,748 Number of examples for the current task : 3491\n",
      "2021-02-24 05:25:09,462 Loss for iteration 160 is 0.04697922701862725\n",
      "2021-02-24 05:25:46,292 Loss for iteration 1760 is 0.007550376421784529\n",
      "2021-02-24 05:26:22,334 Loss for iteration 3360 is 0.004181836972851437\n",
      "2021-02-24 05:26:24,532 Number of examples for the current task : 2953\n",
      "2021-02-24 05:26:35,643 Loss for iteration 160 is 0.04261386140503667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-24 05:27:15,104 Loss for iteration 1760 is 0.009346337917130776\n",
      "2021-02-24 05:27:39,107 Number of examples for the current task : 641\n",
      "2021-02-24 05:27:49,880 Loss for iteration 160 is 0.038598242766139185\n",
      "2021-02-24 05:27:57,834 Number of examples for the current task : 2124\n",
      "2021-02-24 05:28:08,983 Loss for iteration 160 is 0.0524267220700329\n",
      "2021-02-24 05:28:45,672 Loss for iteration 1760 is 0.009196525109842155\n",
      "2021-02-24 05:28:51,777 Number of examples for the current task : 1500\n",
      "2021-02-24 05:29:02,608 Loss for iteration 160 is 0.024012912382287057\n",
      "2021-02-24 05:29:26,942 Number of examples for the current task : 1626\n",
      "2021-02-24 05:29:37,733 Loss for iteration 160 is 0.055625019425695595\n",
      "2021-02-24 05:30:05,201 Number of examples for the current task : 4376\n",
      "2021-02-24 05:30:16,066 Loss for iteration 160 is 0.03851745980368419\n",
      "2021-02-24 05:30:52,648 Loss for iteration 1760 is 0.010319522356767594\n",
      "2021-02-24 05:31:29,071 Loss for iteration 3360 is 0.0058785153574131485\n",
      "2021-02-24 05:31:46,682 Number of examples for the current task : 4497\n",
      "2021-02-24 05:31:57,976 Loss for iteration 160 is 0.0621746809814464\n",
      "2021-02-24 05:32:37,091 Loss for iteration 1760 is 0.016602601917347293\n",
      "2021-02-24 05:33:16,159 Loss for iteration 3360 is 0.009273298826986997\n",
      "2021-02-24 05:33:38,079 Number of examples for the current task : 10316\n",
      "2021-02-24 05:33:48,918 Loss for iteration 160 is 0.06369595144960014\n",
      "2021-02-24 05:34:25,141 Loss for iteration 1760 is 0.018985244063744404\n",
      "2021-02-24 05:35:01,941 Loss for iteration 3360 is 0.01207469132416043\n",
      "2021-02-24 05:35:37,743 Loss for iteration 4960 is 0.00875041789981513\n",
      "2021-02-24 05:36:14,353 Loss for iteration 6560 is 0.007206287544851694\n",
      "2021-02-24 05:36:51,271 Loss for iteration 8160 is 0.006194057601305313\n",
      "2021-02-24 05:37:27,758 Loss for iteration 9760 is 0.0055875055855102935\n",
      "2021-02-24 05:37:37,104 Number of examples for the current task : 1386\n",
      "2021-02-24 05:37:48,086 Loss for iteration 160 is 0.045015203512527725\n",
      "2021-02-24 05:38:10,458 Number of examples for the current task : 973\n",
      "2021-02-24 05:38:21,528 Loss for iteration 160 is 0.037299915978854355\n",
      "2021-02-24 05:38:35,166 Number of examples for the current task : 3134\n",
      "2021-02-24 05:38:46,578 Loss for iteration 160 is 0.03951881758191369\n",
      "2021-02-24 05:39:25,560 Loss for iteration 1760 is 0.009308723479153815\n",
      "2021-02-24 05:39:51,802 Number of examples for the current task : 2466\n",
      "2021-02-24 05:40:03,230 Loss for iteration 160 is 0.05275777964429422\n",
      "2021-02-24 05:40:43,056 Loss for iteration 1760 is 0.008141895798601312\n",
      "2021-02-24 05:40:56,883 Number of examples for the current task : 1158\n",
      "2021-02-24 05:41:07,968 Loss for iteration 160 is 0.03712473487989469\n",
      "2021-02-24 05:41:26,710 Number of examples for the current task : 3201\n",
      "2021-02-24 05:41:37,580 Loss for iteration 160 is 0.048680142584172165\n",
      "2021-02-24 05:42:13,828 Loss for iteration 1760 is 0.010246676052259785\n",
      "2021-02-24 05:42:38,605 Number of examples for the current task : 3178\n",
      "2021-02-24 05:42:50,207 Loss for iteration 160 is 0.05859429778700525\n",
      "2021-02-24 05:43:31,166 Loss for iteration 1760 is 0.010698119366409358\n",
      "2021-02-24 05:44:00,979 Number of examples for the current task : 6499\n",
      "2021-02-24 05:44:12,519 Loss for iteration 160 is 0.07082976536317305\n",
      "2021-02-24 05:44:54,695 Loss for iteration 1760 is 0.01697424992285501\n",
      "2021-02-24 05:45:37,405 Loss for iteration 3360 is 0.009763429481617531\n",
      "2021-02-24 05:46:20,517 Loss for iteration 4960 is 0.0071207524086223314\n",
      "2021-02-24 05:46:53,542 Number of examples for the current task : 3264\n",
      "2021-02-24 05:47:04,531 Loss for iteration 160 is 0.07333667000586336\n",
      "2021-02-24 05:47:40,990 Loss for iteration 1760 is 0.013477876593373498\n",
      "2021-02-24 05:48:07,697 Number of examples for the current task : 9859\n",
      "2021-02-24 05:48:18,975 Loss for iteration 160 is 0.054426868467337707\n",
      "2021-02-24 05:48:55,674 Loss for iteration 1760 is 0.018468551327231096\n",
      "2021-02-24 05:49:32,751 Loss for iteration 3360 is 0.011427243055968315\n",
      "2021-02-24 05:50:09,824 Loss for iteration 4960 is 0.008300570823503748\n",
      "2021-02-24 05:50:46,435 Loss for iteration 6560 is 0.0067870188324948445\n",
      "2021-02-24 05:51:23,387 Loss for iteration 8160 is 0.00631667783796487\n",
      "2021-02-24 05:52:00,158 Loss for iteration 9760 is 0.005847856290899953\n",
      "2021-02-24 05:52:01,771 Number of examples for the current task : 15324\n",
      "2021-02-24 05:52:12,760 Loss for iteration 160 is 0.061375927688045936\n",
      "2021-02-24 05:52:49,646 Loss for iteration 1760 is 0.020960098965227738\n",
      "2021-02-24 05:53:26,506 Loss for iteration 3360 is 0.013986665066397832\n",
      "2021-02-24 05:54:04,183 Loss for iteration 4960 is 0.010521721211355629\n",
      "2021-02-24 05:54:41,653 Loss for iteration 6560 is 0.008524164323422519\n",
      "2021-02-24 05:55:19,169 Loss for iteration 8160 is 0.007438908787559283\n",
      "2021-02-24 05:55:56,656 Loss for iteration 9760 is 0.00691451239598242\n",
      "2021-02-24 05:56:34,167 Loss for iteration 11360 is 0.006363711576681563\n",
      "2021-02-24 05:57:12,198 Loss for iteration 12960 is 0.00592975353290061\n",
      "2021-02-24 05:57:49,571 Loss for iteration 14560 is 0.0055086216193233895\n",
      "2021-02-24 05:58:02,983 Number of examples for the current task : 7153\n",
      "2021-02-24 05:58:14,120 Loss for iteration 160 is 0.035825603641569614\n",
      "2021-02-24 05:58:51,919 Loss for iteration 1760 is 0.011133770135373951\n",
      "2021-02-24 05:59:29,666 Loss for iteration 3360 is 0.006693161616670972\n",
      "2021-02-24 06:00:07,946 Loss for iteration 4960 is 0.004892666431821784\n",
      "2021-02-24 06:00:46,076 Loss for iteration 6560 is 0.0038277681030466984\n",
      "2021-02-24 06:00:57,237 Number of examples for the current task : 2602\n",
      "2021-02-24 06:01:08,328 Loss for iteration 160 is 0.04847117331386967\n",
      "2021-02-24 06:01:44,557 Loss for iteration 1760 is 0.008424933159698706\n",
      "2021-02-24 06:01:59,001 Number of examples for the current task : 1596\n",
      "2021-02-24 06:02:10,464 Loss for iteration 160 is 0.05434741215272383\n",
      "2021-02-24 06:02:38,326 Number of examples for the current task : 1333\n",
      "2021-02-24 06:02:49,901 Loss for iteration 160 is 0.04705718368694017\n",
      "2021-02-24 06:03:11,746 Number of examples for the current task : 2298\n",
      "2021-02-24 06:03:22,987 Loss for iteration 160 is 0.04320731445808302\n",
      "2021-02-24 06:03:59,632 Loss for iteration 1760 is 0.008189065050992376\n",
      "2021-02-24 06:04:08,686 Number of examples for the current task : 2323\n",
      "2021-02-24 06:04:20,015 Loss for iteration 160 is 0.03325214524838058\n",
      "2021-02-24 06:04:58,336 Loss for iteration 1760 is 0.006429658628221987\n",
      "2021-02-24 06:05:08,950 Number of examples for the current task : 3107\n",
      "2021-02-24 06:05:19,934 Loss for iteration 160 is 0.05258152917535468\n",
      "2021-02-24 06:05:55,195 Loss for iteration 1760 is 0.010499699606281193\n",
      "2021-02-24 06:06:17,492 Number of examples for the current task : 4644\n",
      "2021-02-24 06:06:28,517 Loss for iteration 160 is 0.048156491946429014\n",
      "2021-02-24 06:07:04,251 Loss for iteration 1760 is 0.010320594201812538\n",
      "2021-02-24 06:07:39,464 Loss for iteration 3360 is 0.006030413334737079\n",
      "2021-02-24 06:08:01,373 Number of examples for the current task : 2576\n",
      "2021-02-24 06:08:13,539 Loss for iteration 160 is 0.04441283287649805\n",
      "2021-02-24 06:08:54,026 Loss for iteration 1760 is 0.008589118953898071\n",
      "2021-02-24 06:09:10,389 Number of examples for the current task : 2480\n",
      "2021-02-24 06:09:21,658 Loss for iteration 160 is 0.04652199267663739\n",
      "2021-02-24 06:10:00,601 Loss for iteration 1760 is 0.007388736893525683\n",
      "2021-02-24 06:10:14,167 Number of examples for the current task : 1308\n",
      "2021-02-24 06:10:25,184 Loss for iteration 160 is 0.03783806117082184\n",
      "2021-02-24 06:10:45,297 Number of examples for the current task : 4317\n",
      "2021-02-24 06:10:56,746 Loss for iteration 160 is 0.05900830148973248\n",
      "2021-02-24 06:11:36,146 Loss for iteration 1760 is 0.013862747010493835\n",
      "2021-02-24 06:12:15,065 Loss for iteration 3360 is 0.007662133430596303\n",
      "2021-02-24 06:12:33,176 Number of examples for the current task : 3409\n",
      "2021-02-24 06:12:44,472 Loss for iteration 160 is 0.058266714977269825\n",
      "2021-02-24 06:13:21,965 Loss for iteration 1760 is 0.011410151709889498\n",
      "2021-02-24 06:13:59,632 Loss for iteration 3360 is 0.0066122007664019666\n",
      "2021-02-24 06:14:00,295 Number of examples for the current task : 1843\n",
      "2021-02-24 06:14:11,365 Loss for iteration 160 is 0.042219013991681015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-24 06:14:48,965 Loss for iteration 1760 is 0.007124757971551672\n",
      "2021-02-24 06:14:50,191 Number of examples for the current task : 2110\n",
      "2021-02-24 06:15:01,568 Loss for iteration 160 is 0.027696475873447278\n",
      "2021-02-24 06:15:42,740 Loss for iteration 1760 is 0.004768721370481643\n",
      "2021-02-24 06:15:49,104 Number of examples for the current task : 2394\n",
      "2021-02-24 06:16:00,521 Loss for iteration 160 is 0.03508121947842566\n",
      "2021-02-24 06:16:38,385 Loss for iteration 1760 is 0.007330250698073582\n",
      "2021-02-24 06:16:49,484 Number of examples for the current task : 2814\n",
      "2021-02-24 06:17:00,614 Loss for iteration 160 is 0.040873073041439056\n",
      "2021-02-24 06:17:35,957 Loss for iteration 1760 is 0.007597930825090976\n",
      "2021-02-24 06:17:53,491 Number of examples for the current task : 1813\n",
      "2021-02-24 06:18:05,010 Loss for iteration 160 is 0.0393271612172777\n",
      "2021-02-24 06:18:42,790 Loss for iteration 1760 is 0.006346043506070871\n",
      "2021-02-24 06:18:43,426 Number of examples for the current task : 2537\n",
      "2021-02-24 06:18:54,682 Loss for iteration 160 is 0.043875101445750755\n",
      "2021-02-24 06:19:30,866 Loss for iteration 1760 is 0.007993564957900703\n",
      "2021-02-24 06:19:44,184 Number of examples for the current task : 1748\n",
      "2021-02-24 06:19:55,247 Loss for iteration 160 is 0.038752092979848385\n",
      "2021-02-24 06:20:21,471 Number of examples for the current task : 1439\n",
      "2021-02-24 06:20:32,747 Loss for iteration 160 is 0.025355523452162743\n",
      "2021-02-24 06:20:54,220 Number of examples for the current task : 3109\n",
      "2021-02-24 06:21:05,690 Loss for iteration 160 is 0.0325689839029854\n",
      "2021-02-24 06:21:43,967 Loss for iteration 1760 is 0.007476827864331164\n",
      "2021-02-24 06:22:09,158 Number of examples for the current task : 1751\n",
      "2021-02-24 06:22:20,420 Loss for iteration 160 is 0.038193095538934525\n",
      "2021-02-24 06:22:50,173 Number of examples for the current task : 3972\n",
      "2021-02-24 06:23:01,377 Loss for iteration 160 is 0.03882054260677912\n",
      "2021-02-24 06:23:36,624 Loss for iteration 1760 is 0.009864545670319098\n",
      "2021-02-24 06:24:11,696 Loss for iteration 3360 is 0.005838881746786636\n",
      "2021-02-24 06:24:21,719 Number of examples for the current task : 9831\n",
      "2021-02-24 06:24:33,266 Loss for iteration 160 is 0.05200370748273351\n",
      "2021-02-24 06:25:12,640 Loss for iteration 1760 is 0.017624473373346904\n",
      "2021-02-24 06:25:52,382 Loss for iteration 3360 is 0.011097122278942845\n",
      "2021-02-24 06:26:31,450 Loss for iteration 4960 is 0.008519787645121658\n",
      "2021-02-24 06:27:10,360 Loss for iteration 6560 is 0.006912643424788116\n",
      "2021-02-24 06:27:49,554 Loss for iteration 8160 is 0.006224349165817875\n",
      "2021-02-24 06:28:28,899 Loss for iteration 9760 is 0.0056663089472108\n",
      "2021-02-24 06:28:29,937 Number of examples for the current task : 3288\n",
      "2021-02-24 06:28:41,446 Loss for iteration 160 is 0.021585606038570404\n",
      "2021-02-24 06:29:19,887 Loss for iteration 1760 is 0.0049630840704750125\n",
      "2021-02-24 06:29:48,667 Number of examples for the current task : 1197\n",
      "2021-02-24 06:30:00,007 Loss for iteration 160 is 0.040692621672695335\n",
      "2021-02-24 06:30:17,446 Number of examples for the current task : 2386\n",
      "2021-02-24 06:30:29,180 Loss for iteration 160 is 0.04246621223335916\n",
      "2021-02-24 06:31:08,641 Loss for iteration 1760 is 0.007640438214120631\n",
      "2021-02-24 06:31:20,554 Number of examples for the current task : 1245\n",
      "2021-02-24 06:31:31,721 Loss for iteration 160 is 0.023255179535639894\n",
      "2021-02-24 06:31:50,754 Number of examples for the current task : 1911\n",
      "2021-02-24 06:32:02,270 Loss for iteration 160 is 0.04384563897143711\n",
      "2021-02-24 06:32:40,137 Loss for iteration 1760 is 0.006612908804887772\n",
      "2021-02-24 06:32:42,531 Number of examples for the current task : 3093\n",
      "2021-02-24 06:32:53,844 Loss for iteration 160 is 0.03548634915866635\n",
      "2021-02-24 06:33:31,302 Loss for iteration 1760 is 0.007264131828272078\n",
      "2021-02-24 06:33:55,072 Number of examples for the current task : 2593\n",
      "2021-02-24 06:34:06,739 Loss for iteration 160 is 0.0374290047806095\n",
      "2021-02-24 06:34:45,817 Loss for iteration 1760 is 0.0069624180568106575\n",
      "2021-02-24 06:35:00,690 Number of examples for the current task : 3732\n",
      "2021-02-24 06:35:12,261 Loss for iteration 160 is 0.03472144190560688\n",
      "2021-02-24 06:35:51,243 Loss for iteration 1760 is 0.007909729305931644\n",
      "2021-02-24 06:36:30,578 Loss for iteration 3360 is 0.004616558179050677\n",
      "2021-02-24 06:36:37,612 Number of examples for the current task : 4259\n",
      "2021-02-24 06:36:49,211 Loss for iteration 160 is 0.051130665889517826\n",
      "2021-02-24 06:37:26,978 Loss for iteration 1760 is 0.01173071588283648\n",
      "2021-02-24 06:38:04,529 Loss for iteration 3360 is 0.006757602667852837\n",
      "2021-02-24 06:38:21,062 Number of examples for the current task : 4639\n",
      "2021-02-24 06:38:32,343 Loss for iteration 160 is 0.08321044136854736\n",
      "2021-02-24 06:39:08,353 Loss for iteration 1760 is 0.01624699556562412\n",
      "2021-02-24 06:39:44,486 Loss for iteration 3360 is 0.009299254576345659\n",
      "2021-02-24 06:40:06,070 Number of examples for the current task : 2856\n",
      "2021-02-24 06:40:17,420 Loss for iteration 160 is 0.028207652863453735\n",
      "2021-02-24 06:40:53,831 Loss for iteration 1760 is 0.005822186843992362\n",
      "2021-02-24 06:41:12,963 Number of examples for the current task : 5542\n",
      "2021-02-24 06:41:24,694 Loss for iteration 160 is 0.052005737173286354\n",
      "2021-02-24 06:42:03,043 Loss for iteration 1760 is 0.012557563743751228\n",
      "2021-02-24 06:42:41,261 Loss for iteration 3360 is 0.007501998212121171\n",
      "2021-02-24 06:43:19,081 Loss for iteration 4960 is 0.0053748671786247705\n",
      "2021-02-24 06:43:29,111 Number of examples for the current task : 2783\n",
      "2021-02-24 06:43:40,634 Loss for iteration 160 is 0.06449037417769432\n",
      "2021-02-24 06:44:18,684 Loss for iteration 1760 is 0.011821922085768133\n",
      "2021-02-24 06:44:36,743 Number of examples for the current task : 2867\n",
      "2021-02-24 06:44:48,344 Loss for iteration 160 is 0.05367514893243259\n",
      "2021-02-24 06:45:25,995 Loss for iteration 1760 is 0.01011289647811298\n",
      "2021-02-24 06:45:45,956 Number of examples for the current task : 1996\n",
      "2021-02-24 06:45:57,757 Loss for iteration 160 is 0.04941171983426267\n",
      "2021-02-24 06:46:37,476 Loss for iteration 1760 is 0.008611553759790794\n",
      "2021-02-24 06:46:41,593 Number of examples for the current task : 2475\n",
      "2021-02-24 06:46:53,372 Loss for iteration 160 is 0.049330581572245465\n",
      "2021-02-24 06:47:32,748 Loss for iteration 1760 is 0.009070433923625387\n",
      "2021-02-24 06:47:46,521 Number of examples for the current task : 2698\n",
      "2021-02-24 06:47:58,093 Loss for iteration 160 is 0.050153868907893244\n",
      "2021-02-24 06:48:35,859 Loss for iteration 1760 is 0.009483076519802682\n",
      "2021-02-24 06:48:52,764 Number of examples for the current task : 4148\n",
      "2021-02-24 06:49:04,599 Loss for iteration 160 is 0.05448741309175437\n",
      "2021-02-24 06:49:43,085 Loss for iteration 1760 is 0.012958297299888186\n",
      "2021-02-24 06:50:21,530 Loss for iteration 3360 is 0.007574647475090275\n",
      "2021-02-24 06:50:36,030 Number of examples for the current task : 5240\n",
      "2021-02-24 06:50:47,642 Loss for iteration 160 is 0.04328193528239022\n",
      "2021-02-24 06:51:24,012 Loss for iteration 1760 is 0.01008483165610783\n",
      "2021-02-24 06:52:00,365 Loss for iteration 3360 is 0.005833752583449577\n",
      "2021-02-24 06:52:37,124 Loss for iteration 4960 is 0.004189290923271357\n",
      "2021-02-24 06:52:41,603 Number of examples for the current task : 1715\n",
      "2021-02-24 06:52:53,397 Loss for iteration 160 is 0.040347982324998484\n",
      "2021-02-24 06:53:21,755 Number of examples for the current task : 2618\n",
      "2021-02-24 06:53:33,767 Loss for iteration 160 is 0.04173338023776358\n",
      "2021-02-24 06:54:12,773 Loss for iteration 1760 is 0.0077790223471524075\n",
      "2021-02-24 06:54:28,496 Number of examples for the current task : 4685\n",
      "2021-02-24 06:54:40,074 Loss for iteration 160 is 0.0465914184989577\n",
      "2021-02-24 06:55:18,668 Loss for iteration 1760 is 0.01115449529606849\n",
      "2021-02-24 06:55:56,312 Loss for iteration 3360 is 0.006767948581918498\n",
      "2021-02-24 06:56:20,080 Number of examples for the current task : 1445\n",
      "2021-02-24 06:56:31,962 Loss for iteration 160 is 0.0467261198527095\n",
      "2021-02-24 06:56:56,406 Number of examples for the current task : 4541\n",
      "2021-02-24 06:57:08,570 Loss for iteration 160 is 0.06207437203689055\n",
      "2021-02-24 06:57:47,762 Loss for iteration 1760 is 0.013151284585694305\n",
      "2021-02-24 06:58:26,760 Loss for iteration 3360 is 0.007598025136732362\n",
      "2021-02-24 06:58:48,661 Number of examples for the current task : 1051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-24 06:59:00,305 Loss for iteration 160 is 0.03194131440779364\n",
      "2021-02-24 06:59:16,468 Number of examples for the current task : 856\n",
      "2021-02-24 06:59:28,115 Loss for iteration 160 is 0.038055914813991294\n",
      "2021-02-24 06:59:40,024 Number of examples for the current task : 2488\n",
      "2021-02-24 06:59:52,114 Loss for iteration 160 is 0.045734393325718964\n",
      "2021-02-24 07:00:30,835 Loss for iteration 1760 is 0.008669802182519741\n",
      "2021-02-24 07:00:44,416 Number of examples for the current task : 3134\n",
      "2021-02-24 07:00:56,358 Loss for iteration 160 is 0.05760316889394413\n",
      "2021-02-24 07:01:36,252 Loss for iteration 1760 is 0.012083568626244946\n",
      "2021-02-24 07:02:02,519 Number of examples for the current task : 4300\n",
      "2021-02-24 07:02:14,393 Loss for iteration 160 is 0.04421058271757581\n",
      "2021-02-24 07:02:51,288 Loss for iteration 1760 is 0.009264814492256322\n",
      "2021-02-24 07:03:28,378 Loss for iteration 3360 is 0.005635719995147613\n",
      "2021-02-24 07:03:44,462 Number of examples for the current task : 2727\n",
      "2021-02-24 07:03:56,609 Loss for iteration 160 is 0.03452378138899803\n",
      "2021-02-24 07:04:36,383 Loss for iteration 1760 is 0.008954447082352409\n",
      "2021-02-24 07:04:55,046 Number of examples for the current task : 1940\n",
      "2021-02-24 07:05:07,009 Loss for iteration 160 is 0.04720846059816805\n",
      "2021-02-24 07:05:46,038 Loss for iteration 1760 is 0.00894786152665646\n",
      "2021-02-24 07:05:49,212 Number of examples for the current task : 9038\n",
      "2021-02-24 07:06:01,184 Loss for iteration 160 is 0.04621002027256922\n",
      "2021-02-24 07:06:37,927 Loss for iteration 1760 is 0.015253352823558155\n",
      "2021-02-24 07:07:14,529 Loss for iteration 3360 is 0.009331781950586706\n",
      "2021-02-24 07:07:51,473 Loss for iteration 4960 is 0.00684686778798964\n",
      "2021-02-24 07:08:28,164 Loss for iteration 6560 is 0.006115427305013568\n",
      "2021-02-24 07:09:05,502 Loss for iteration 8160 is 0.005328778064334424\n",
      "2021-02-24 07:09:20,404 Number of examples for the current task : 2687\n",
      "2021-02-24 07:09:32,561 Loss for iteration 160 is 0.04919842359694568\n",
      "2021-02-24 07:10:09,536 Loss for iteration 1760 is 0.008923430925786815\n",
      "2021-02-24 07:10:25,811 Number of examples for the current task : 1607\n",
      "2021-02-24 07:10:37,987 Loss for iteration 160 is 0.056506025858900764\n",
      "2021-02-24 07:11:04,186 Number of examples for the current task : 3045\n",
      "2021-02-24 07:11:16,017 Loss for iteration 160 is 0.04742452887479554\n",
      "2021-02-24 07:11:53,935 Loss for iteration 1760 is 0.010234075166056402\n",
      "2021-02-24 07:12:16,545 Number of examples for the current task : 1627\n",
      "2021-02-24 07:12:28,915 Loss for iteration 160 is 0.03532926175235347\n",
      "2021-02-24 07:12:56,317 Number of examples for the current task : 1961\n",
      "2021-02-24 07:13:08,465 Loss for iteration 160 is 0.047501309774816036\n",
      "2021-02-24 07:13:46,933 Loss for iteration 1760 is 0.008446432438103255\n",
      "2021-02-24 07:13:50,211 Number of examples for the current task : 4558\n",
      "2021-02-24 07:14:02,634 Loss for iteration 160 is 0.048536330461502075\n",
      "2021-02-24 07:14:43,351 Loss for iteration 1760 is 0.013067063659970724\n",
      "2021-02-24 07:15:24,696 Loss for iteration 3360 is 0.007920710988918217\n",
      "2021-02-24 07:15:48,393 Number of examples for the current task : 3152\n",
      "2021-02-24 07:16:01,009 Loss for iteration 160 is 0.04291251775893298\n",
      "2021-02-24 07:16:40,431 Loss for iteration 1760 is 0.010270050907222269\n",
      "2021-02-24 07:17:06,819 Number of examples for the current task : 2220\n",
      "2021-02-24 07:17:17,598 Loss for iteration 160 is 0.04705906422300772\n",
      "2021-02-24 07:17:54,505 Loss for iteration 1760 is 0.0082284257220742\n",
      "2021-02-24 07:18:02,733 Number of examples for the current task : 2850\n",
      "2021-02-24 07:18:13,192 Loss for iteration 160 is 0.05552142147313465\n",
      "2021-02-24 07:18:51,407 Loss for iteration 1760 is 0.01135029942861775\n",
      "2021-02-24 07:19:12,990 Number of examples for the current task : 2729\n",
      "2021-02-24 07:19:23,077 Loss for iteration 160 is 0.05406377176669511\n",
      "2021-02-24 07:20:01,192 Loss for iteration 1760 is 0.01179905189738739\n",
      "2021-02-24 07:20:19,350 Number of examples for the current task : 2028\n",
      "2021-02-24 07:20:29,195 Loss for iteration 160 is 0.03943240701813589\n",
      "2021-02-24 07:21:06,734 Loss for iteration 1760 is 0.006571098302883256\n",
      "2021-02-24 07:21:11,342 Number of examples for the current task : 5276\n",
      "2021-02-24 07:21:21,154 Loss for iteration 160 is 0.04498481327159838\n",
      "2021-02-24 07:21:56,145 Loss for iteration 1760 is 0.012421093522182977\n",
      "2021-02-24 07:22:31,360 Loss for iteration 3360 is 0.007951839564239024\n",
      "2021-02-24 07:23:06,320 Loss for iteration 4960 is 0.005699678068369769\n",
      "2021-02-24 07:23:11,645 Number of examples for the current task : 2045\n",
      "2021-02-24 07:23:21,634 Loss for iteration 160 is 0.04919970869510011\n",
      "2021-02-24 07:23:57,757 Loss for iteration 1760 is 0.007378258648263458\n",
      "2021-02-24 07:24:02,510 Number of examples for the current task : 3844\n",
      "2021-02-24 07:24:12,327 Loss for iteration 160 is 0.050103159198029476\n",
      "2021-02-24 07:24:48,206 Loss for iteration 1760 is 0.011490136828338559\n",
      "2021-02-24 07:25:24,286 Loss for iteration 3360 is 0.006433018117768456\n",
      "2021-02-24 07:25:32,682 Number of examples for the current task : 2468\n",
      "2021-02-24 07:25:43,150 Loss for iteration 160 is 0.053705073723738846\n",
      "2021-02-24 07:26:20,829 Loss for iteration 1760 is 0.0105179351436442\n",
      "2021-02-24 07:26:34,122 Number of examples for the current task : 969\n",
      "2021-02-24 07:26:44,150 Loss for iteration 160 is 0.04495275266129862\n",
      "2021-02-24 07:26:57,732 Number of examples for the current task : 10299\n",
      "2021-02-24 07:27:07,519 Loss for iteration 160 is 0.05397084355354309\n",
      "2021-02-24 07:27:40,542 Loss for iteration 1760 is 0.018828291550849204\n",
      "2021-02-24 07:28:14,123 Loss for iteration 3360 is 0.011958020036075687\n",
      "2021-02-24 07:28:48,177 Loss for iteration 4960 is 0.009035284951423935\n",
      "2021-02-24 07:29:21,847 Loss for iteration 6560 is 0.007952744925289568\n",
      "2021-02-24 07:29:55,163 Loss for iteration 8160 is 0.006717014969280423\n",
      "2021-02-24 07:30:28,324 Loss for iteration 9760 is 0.005850975347045015\n",
      "2021-02-24 07:30:36,864 Number of examples for the current task : 3184\n",
      "2021-02-24 07:30:46,883 Loss for iteration 160 is 0.03772198747504841\n",
      "2021-02-24 07:31:23,872 Loss for iteration 1760 is 0.007810354954746619\n",
      "2021-02-24 07:31:50,000 Number of examples for the current task : 13217\n",
      "2021-02-24 07:31:59,876 Loss for iteration 160 is 0.036526962564411486\n",
      "2021-02-24 07:32:33,156 Loss for iteration 1760 is 0.01565098582855048\n",
      "2021-02-24 07:33:06,404 Loss for iteration 3360 is 0.010490568806869707\n",
      "2021-02-24 07:33:39,950 Loss for iteration 4960 is 0.008025334868215135\n",
      "2021-02-24 07:34:14,444 Loss for iteration 6560 is 0.0066196871334740694\n",
      "2021-02-24 07:34:48,020 Loss for iteration 8160 is 0.005726847905952858\n",
      "2021-02-24 07:35:21,371 Loss for iteration 9760 is 0.005305049891911385\n",
      "2021-02-24 07:35:55,275 Loss for iteration 11360 is 0.004977419915516838\n",
      "2021-02-24 07:36:28,517 Loss for iteration 12960 is 0.004721536980956785\n",
      "2021-02-24 07:36:32,642 Number of examples for the current task : 3066\n",
      "2021-02-24 07:36:42,569 Loss for iteration 160 is 0.060082416092468935\n",
      "2021-02-24 07:37:17,795 Loss for iteration 1760 is 0.013299378433248011\n",
      "2021-02-24 07:37:41,420 Number of examples for the current task : 4825\n",
      "2021-02-24 07:37:51,054 Loss for iteration 160 is 0.03845213302834467\n",
      "2021-02-24 07:38:27,890 Loss for iteration 1760 is 0.01049619683390693\n",
      "2021-02-24 07:39:04,392 Loss for iteration 3360 is 0.0061923327942456615\n",
      "2021-02-24 07:39:30,922 Number of examples for the current task : 1343\n",
      "2021-02-24 07:39:40,666 Loss for iteration 160 is 0.04644950149072842\n",
      "2021-02-24 07:40:01,028 Number of examples for the current task : 1232\n",
      "2021-02-24 07:40:10,841 Loss for iteration 160 is 0.03514249991117553\n",
      "2021-02-24 07:40:29,779 Number of examples for the current task : 1963\n",
      "2021-02-24 07:40:39,660 Loss for iteration 160 is 0.014176161007278344\n",
      "2021-02-24 07:41:14,431 Loss for iteration 1760 is 0.003875832512766555\n",
      "2021-02-24 07:41:17,491 Number of examples for the current task : 10096\n",
      "2021-02-24 07:41:27,467 Loss for iteration 160 is 0.04762380968102\n",
      "2021-02-24 07:42:02,890 Loss for iteration 1760 is 0.013702282299254055\n",
      "2021-02-24 07:42:38,353 Loss for iteration 3360 is 0.009149274755968884\n",
      "2021-02-24 07:43:13,047 Loss for iteration 4960 is 0.006845953737637679\n",
      "2021-02-24 07:43:48,829 Loss for iteration 6560 is 0.005761094800640723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-24 07:44:23,523 Loss for iteration 8160 is 0.00494664468812735\n",
      "2021-02-24 07:44:58,917 Loss for iteration 9760 is 0.004648881752918316\n",
      "2021-02-24 07:45:04,561 Number of examples for the current task : 1672\n",
      "2021-02-24 07:45:14,527 Loss for iteration 160 is 0.030533375006846407\n",
      "2021-02-24 07:45:40,406 Number of examples for the current task : 7007\n",
      "2021-02-24 07:45:49,996 Loss for iteration 160 is 0.0510086864233017\n",
      "2021-02-24 07:46:24,024 Loss for iteration 1760 is 0.012711659674260026\n",
      "2021-02-24 07:46:57,849 Loss for iteration 3360 is 0.007864113615564251\n",
      "2021-02-24 07:47:32,437 Loss for iteration 4960 is 0.005998732485141945\n",
      "2021-02-24 07:48:06,785 Loss for iteration 6560 is 0.004763589451600093\n",
      "2021-02-24 07:48:14,056 Number of examples for the current task : 3156\n",
      "2021-02-24 07:48:24,118 Loss for iteration 160 is 0.05506738207556985\n",
      "2021-02-24 07:49:02,052 Loss for iteration 1760 is 0.011800922071120728\n",
      "2021-02-24 07:49:29,151 Number of examples for the current task : 2823\n",
      "2021-02-24 07:49:39,013 Loss for iteration 160 is 0.035995126998221334\n",
      "2021-02-24 07:50:14,718 Loss for iteration 1760 is 0.00674558841939444\n",
      "2021-02-24 07:50:33,898 Number of examples for the current task : 3886\n",
      "2021-02-24 07:50:44,293 Loss for iteration 160 is 0.0451145332983949\n",
      "2021-02-24 07:51:26,511 Loss for iteration 1760 is 0.011519499456610632\n",
      "2021-02-24 07:52:08,711 Loss for iteration 3360 is 0.0063513807198049286\n",
      "2021-02-24 07:52:19,921 Number of examples for the current task : 3327\n",
      "2021-02-24 07:52:30,302 Loss for iteration 160 is 0.052933592687953605\n",
      "2021-02-24 07:53:07,228 Loss for iteration 1760 is 0.010353130819632564\n",
      "2021-02-24 07:53:35,918 Number of examples for the current task : 5704\n",
      "2021-02-24 07:53:45,668 Loss for iteration 160 is 0.026765651323578575\n",
      "2021-02-24 07:54:22,302 Loss for iteration 1760 is 0.009148222235396403\n",
      "2021-02-24 07:54:59,491 Loss for iteration 3360 is 0.005671736675795163\n",
      "2021-02-24 07:55:36,432 Loss for iteration 4960 is 0.004573191929652403\n",
      "2021-02-24 07:55:50,190 Number of examples for the current task : 3362\n",
      "2021-02-24 07:56:00,048 Loss for iteration 160 is 0.030396570112894882\n",
      "2021-02-24 07:56:35,650 Loss for iteration 1760 is 0.008084187327046681\n",
      "2021-02-24 07:57:04,570 Number of examples for the current task : 2396\n",
      "2021-02-24 07:57:14,542 Loss for iteration 160 is 0.04461493190716614\n",
      "2021-02-24 07:57:51,182 Loss for iteration 1760 is 0.0073355478772092276\n",
      "2021-02-24 07:58:02,319 Number of examples for the current task : 1888\n",
      "2021-02-24 07:58:12,294 Loss for iteration 160 is 0.03966501003808596\n",
      "2021-02-24 07:58:49,034 Loss for iteration 1760 is 0.007043981956378851\n",
      "2021-02-24 07:58:51,124 Number of examples for the current task : 9341\n",
      "2021-02-24 07:59:01,227 Loss for iteration 160 is 0.05857560055499727\n",
      "2021-02-24 07:59:38,276 Loss for iteration 1760 is 0.0184197655190957\n",
      "2021-02-24 08:00:15,091 Loss for iteration 3360 is 0.012370468126951995\n",
      "2021-02-24 08:00:52,602 Loss for iteration 4960 is 0.009511043226935563\n",
      "2021-02-24 08:01:30,080 Loss for iteration 6560 is 0.007691108132131883\n",
      "2021-02-24 08:02:07,378 Loss for iteration 8160 is 0.0064400245212188215\n",
      "2021-02-24 08:02:29,305 Number of examples for the current task : 3131\n",
      "2021-02-24 08:02:39,611 Loss for iteration 160 is 0.048547806895591995\n",
      "2021-02-24 08:03:18,708 Loss for iteration 1760 is 0.01055947560680661\n",
      "2021-02-24 08:03:46,029 Number of examples for the current task : 3720\n",
      "2021-02-24 08:03:56,760 Loss for iteration 160 is 0.060495515099980614\n",
      "2021-02-24 08:04:36,067 Loss for iteration 1760 is 0.012580684707909068\n",
      "2021-02-24 08:05:15,708 Loss for iteration 3360 is 0.007281662905413572\n",
      "2021-02-24 08:05:22,463 Number of examples for the current task : 2210\n",
      "2021-02-24 08:05:33,073 Loss for iteration 160 is 0.05660120931200006\n",
      "2021-02-24 08:06:12,946 Loss for iteration 1760 is 0.009003066546954886\n",
      "2021-02-24 08:06:21,666 Number of examples for the current task : 4977\n",
      "2021-02-24 08:06:32,181 Loss for iteration 160 is 0.07753739709203894\n",
      "2021-02-24 08:07:09,536 Loss for iteration 1760 is 0.0193284277621707\n",
      "2021-02-24 08:07:46,695 Loss for iteration 3360 is 0.011120480318706575\n",
      "2021-02-24 08:08:24,230 Loss for iteration 4960 is 0.007826353292657703\n",
      "2021-02-24 08:08:24,237 Number of examples for the current task : 1337\n",
      "2021-02-24 08:08:34,560 Loss for iteration 160 is 0.03517087978649546\n",
      "2021-02-24 08:08:56,618 Number of examples for the current task : 4295\n",
      "2021-02-24 08:09:06,689 Loss for iteration 160 is 0.04364507272839546\n",
      "2021-02-24 08:09:41,655 Loss for iteration 1760 is 0.011311633200101505\n",
      "2021-02-24 08:10:16,650 Loss for iteration 3360 is 0.006362806726640181\n",
      "2021-02-24 08:10:32,467 Number of examples for the current task : 2327\n",
      "2021-02-24 08:10:42,764 Loss for iteration 160 is 0.04342053521593863\n",
      "2021-02-24 08:11:18,167 Loss for iteration 1760 is 0.00884066989500105\n",
      "2021-02-24 08:11:27,790 Number of examples for the current task : 1545\n",
      "2021-02-24 08:11:37,928 Loss for iteration 160 is 0.02970681763889099\n",
      "2021-02-24 08:12:03,914 Number of examples for the current task : 1457\n",
      "2021-02-24 08:12:14,587 Loss for iteration 160 is 0.04677287666973742\n",
      "2021-02-24 08:12:38,560 Number of examples for the current task : 3441\n",
      "2021-02-24 08:12:48,954 Loss for iteration 160 is 0.02532660783353177\n",
      "2021-02-24 08:13:25,361 Loss for iteration 1760 is 0.007182813391770102\n",
      "2021-02-24 08:14:01,794 Loss for iteration 3360 is 0.004357964114779254\n",
      "2021-02-24 08:14:02,922 Number of examples for the current task : 4489\n",
      "2021-02-24 08:14:13,257 Loss for iteration 160 is 0.05979492955587127\n",
      "2021-02-24 08:14:51,549 Loss for iteration 1760 is 0.013982989528763888\n",
      "2021-02-24 08:15:30,469 Loss for iteration 3360 is 0.008048455072441348\n",
      "2021-02-24 08:15:52,037 Number of examples for the current task : 8246\n",
      "2021-02-24 08:16:02,423 Loss for iteration 160 is 0.0529004189778458\n",
      "2021-02-24 08:16:38,131 Loss for iteration 1760 is 0.015635307767084456\n",
      "2021-02-24 08:17:14,216 Loss for iteration 3360 is 0.009775598220619863\n",
      "2021-02-24 08:17:50,301 Loss for iteration 4960 is 0.007147246427058993\n",
      "2021-02-24 08:18:27,038 Loss for iteration 6560 is 0.005943661377723247\n",
      "2021-02-24 08:19:03,662 Loss for iteration 8160 is 0.005363357149821981\n",
      "2021-02-24 08:19:04,833 Number of examples for the current task : 1646\n",
      "2021-02-24 08:19:15,030 Loss for iteration 160 is 0.04827826368537816\n",
      "2021-02-24 08:19:42,683 Number of examples for the current task : 6083\n",
      "2021-02-24 08:19:53,142 Loss for iteration 160 is 0.05117779627273029\n",
      "2021-02-24 08:20:28,389 Loss for iteration 1760 is 0.01166866501155603\n",
      "2021-02-24 08:21:04,151 Loss for iteration 3360 is 0.006879691655425658\n",
      "2021-02-24 08:21:39,802 Loss for iteration 4960 is 0.004958973848646963\n",
      "2021-02-24 08:21:59,516 Number of examples for the current task : 4232\n",
      "2021-02-24 08:22:09,626 Loss for iteration 160 is 0.08073753372512081\n",
      "2021-02-24 08:22:44,328 Loss for iteration 1760 is 0.0169897692462614\n",
      "2021-02-24 08:23:19,209 Loss for iteration 3360 is 0.009740819915820419\n",
      "2021-02-24 08:23:33,846 Number of examples for the current task : 4281\n",
      "2021-02-24 08:23:44,280 Loss for iteration 160 is 0.0452601138333028\n",
      "2021-02-24 08:24:22,476 Loss for iteration 1760 is 0.009638107939846688\n",
      "2021-02-24 08:25:00,557 Loss for iteration 3360 is 0.005631614884689077\n",
      "2021-02-24 08:25:18,062 Number of examples for the current task : 6112\n",
      "2021-02-24 08:25:28,277 Loss for iteration 160 is 0.026644451374357395\n",
      "2021-02-24 08:26:04,980 Loss for iteration 1760 is 0.008407255412847106\n",
      "2021-02-24 08:26:41,664 Loss for iteration 3360 is 0.005199051328336238\n",
      "2021-02-24 08:27:18,004 Loss for iteration 4960 is 0.003988963920343068\n",
      "2021-02-24 08:27:38,422 Number of examples for the current task : 6106\n",
      "2021-02-24 08:27:49,086 Loss for iteration 160 is 0.04585153677246787\n",
      "2021-02-24 08:28:28,334 Loss for iteration 1760 is 0.014636592475932327\n",
      "2021-02-24 08:29:07,717 Loss for iteration 3360 is 0.008722204522114418\n",
      "2021-02-24 08:29:47,310 Loss for iteration 4960 is 0.006207436656570274\n",
      "2021-02-24 08:30:10,287 Number of examples for the current task : 3550\n",
      "2021-02-24 08:30:20,415 Loss for iteration 160 is 0.06902028306980025\n",
      "2021-02-24 08:30:56,389 Loss for iteration 1760 is 0.014181375542607342\n",
      "2021-02-24 08:31:32,319 Loss for iteration 3360 is 0.00847108568696187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-24 08:31:35,378 Number of examples for the current task : 1174\n",
      "2021-02-24 08:31:45,601 Loss for iteration 160 is 0.06482798944820058\n",
      "2021-02-24 08:32:04,506 Number of examples for the current task : 1878\n",
      "2021-02-24 08:32:14,835 Loss for iteration 160 is 0.025001979729329996\n",
      "2021-02-24 08:32:51,905 Loss for iteration 1760 is 0.0049125564253224615\n",
      "2021-02-24 08:32:53,855 Number of examples for the current task : 2113\n",
      "2021-02-24 08:33:04,318 Loss for iteration 160 is 0.03856470211493698\n",
      "2021-02-24 08:33:42,157 Loss for iteration 1760 is 0.007273490950365568\n",
      "2021-02-24 08:33:48,749 Number of examples for the current task : 1831\n",
      "2021-02-24 08:33:59,132 Loss for iteration 160 is 0.036377087658779186\n",
      "2021-02-24 08:34:37,949 Loss for iteration 1760 is 0.0075522863596094655\n",
      "2021-02-24 08:34:39,051 Number of examples for the current task : 3311\n",
      "2021-02-24 08:34:49,538 Loss for iteration 160 is 0.04740526671098037\n",
      "2021-02-24 08:35:26,087 Loss for iteration 1760 is 0.00975596792901539\n",
      "2021-02-24 08:35:54,354 Number of examples for the current task : 2745\n",
      "2021-02-24 08:36:04,427 Loss for iteration 160 is 0.044501742009412155\n",
      "2021-02-24 08:36:40,409 Loss for iteration 1760 is 0.009604514670230003\n",
      "2021-02-24 08:36:57,729 Number of examples for the current task : 2803\n",
      "2021-02-24 08:37:08,301 Loss for iteration 160 is 0.04823265186595646\n",
      "2021-02-24 08:37:47,249 Loss for iteration 1760 is 0.008252478621953569\n",
      "2021-02-24 08:38:07,193 Number of examples for the current task : 2812\n",
      "2021-02-24 08:38:17,177 Loss for iteration 160 is 0.025834225562655112\n",
      "2021-02-24 08:38:51,971 Loss for iteration 1760 is 0.006187306021823682\n",
      "2021-02-24 08:39:09,420 Number of examples for the current task : 3974\n",
      "2021-02-24 08:39:19,636 Loss for iteration 160 is 0.03640345687215978\n",
      "2021-02-24 08:39:56,778 Loss for iteration 1760 is 0.00996467893154503\n",
      "2021-02-24 08:40:34,338 Loss for iteration 3360 is 0.005696595962177887\n",
      "2021-02-24 08:40:45,698 Number of examples for the current task : 3124\n",
      "2021-02-24 08:40:56,068 Loss for iteration 160 is 0.03160189684819092\n",
      "2021-02-24 08:41:32,143 Loss for iteration 1760 is 0.009509425689356175\n",
      "2021-02-24 08:41:55,819 Number of examples for the current task : 3603\n",
      "2021-02-24 08:42:05,872 Loss for iteration 160 is 0.028997424989938736\n",
      "2021-02-24 08:42:41,230 Loss for iteration 1760 is 0.00709140649904929\n",
      "2021-02-24 08:43:16,103 Loss for iteration 3360 is 0.00424655582706493\n",
      "2021-02-24 08:43:19,970 Number of examples for the current task : 6177\n",
      "2021-02-24 08:43:30,121 Loss for iteration 160 is 0.055735896934162484\n",
      "2021-02-24 08:44:05,989 Loss for iteration 1760 is 0.016532207944069628\n",
      "2021-02-24 08:44:42,268 Loss for iteration 3360 is 0.00990657039553311\n",
      "2021-02-24 08:45:18,656 Loss for iteration 4960 is 0.007002884096167854\n",
      "2021-02-24 08:45:40,546 Number of examples for the current task : 2666\n",
      "2021-02-24 08:45:51,134 Loss for iteration 160 is 0.04190127703953873\n",
      "2021-02-24 08:46:30,100 Loss for iteration 1760 is 0.009351859041720393\n",
      "2021-02-24 08:46:47,703 Number of examples for the current task : 3083\n",
      "2021-02-24 08:46:58,311 Loss for iteration 160 is 0.07725981360470707\n",
      "2021-02-24 08:47:36,847 Loss for iteration 1760 is 0.014209333240360793\n",
      "2021-02-24 08:48:02,094 Number of examples for the current task : 3499\n",
      "2021-02-24 08:48:12,759 Loss for iteration 160 is 0.05134001560509205\n",
      "2021-02-24 08:48:50,926 Loss for iteration 1760 is 0.011650057020020328\n",
      "2021-02-24 08:49:29,356 Loss for iteration 3360 is 0.006757921195449418\n",
      "2021-02-24 08:49:31,466 Number of examples for the current task : 1741\n",
      "2021-02-24 08:49:42,081 Loss for iteration 160 is 0.02350908709393645\n",
      "2021-02-24 08:50:12,008 Number of examples for the current task : 2713\n",
      "2021-02-24 08:50:22,543 Loss for iteration 160 is 0.0426659883973612\n",
      "2021-02-24 08:50:59,553 Loss for iteration 1760 is 0.008732448991436327\n",
      "2021-02-24 08:51:16,730 Number of examples for the current task : 685\n",
      "2021-02-24 08:51:26,729 Loss for iteration 160 is 0.022847063013945113\n",
      "2021-02-24 08:51:35,462 Number of examples for the current task : 5224\n",
      "2021-02-24 08:51:46,035 Loss for iteration 160 is 0.03702689724212343\n",
      "2021-02-24 08:52:22,723 Loss for iteration 1760 is 0.01070764424648505\n",
      "2021-02-24 08:52:59,413 Loss for iteration 3360 is 0.006409721756090523\n",
      "2021-02-24 08:53:35,843 Loss for iteration 4960 is 0.004982680404007396\n",
      "2021-02-24 08:53:40,195 Number of examples for the current task : 4673\n",
      "2021-02-24 08:53:50,615 Loss for iteration 160 is 0.029933629642156036\n",
      "2021-02-24 08:54:26,744 Loss for iteration 1760 is 0.010139088090802837\n",
      "2021-02-24 08:55:02,361 Loss for iteration 3360 is 0.006132872571547803\n",
      "2021-02-24 08:55:26,010 Number of examples for the current task : 1551\n",
      "2021-02-24 08:55:36,699 Loss for iteration 160 is 0.045286623083732346\n",
      "2021-02-24 08:56:04,463 Number of examples for the current task : 4115\n",
      "2021-02-24 08:56:14,856 Loss for iteration 160 is 0.047139895233241\n",
      "2021-02-24 08:56:51,414 Loss for iteration 1760 is 0.012454631344020905\n",
      "2021-02-24 08:57:27,724 Loss for iteration 3360 is 0.007026509933562075\n",
      "2021-02-24 08:57:41,199 Number of examples for the current task : 6033\n",
      "2021-02-24 08:57:51,574 Loss for iteration 160 is 0.05221310291777958\n",
      "2021-02-24 08:58:28,158 Loss for iteration 1760 is 0.015429026272089162\n",
      "2021-02-24 08:59:04,891 Loss for iteration 3360 is 0.00973213560726401\n",
      "2021-02-24 08:59:41,755 Loss for iteration 4960 is 0.007389456298881397\n",
      "2021-02-24 09:00:01,043 Number of examples for the current task : 2346\n",
      "2021-02-24 09:00:11,002 Loss for iteration 160 is 0.05124777166003531\n",
      "2021-02-24 09:00:46,439 Loss for iteration 1760 is 0.00893705721046809\n",
      "2021-02-24 09:00:56,375 Number of examples for the current task : 602\n",
      "2021-02-24 09:01:06,889 Loss for iteration 160 is 0.023095694053071467\n",
      "2021-02-24 09:01:14,914 Number of examples for the current task : 841\n",
      "2021-02-24 09:01:24,969 Loss for iteration 160 is 0.025960826060988686\n",
      "2021-02-24 09:01:36,083 Number of examples for the current task : 2398\n",
      "2021-02-24 09:01:46,528 Loss for iteration 160 is 0.03996751296587966\n",
      "2021-02-24 09:02:23,412 Loss for iteration 1760 is 0.00833933963020844\n",
      "2021-02-24 09:02:34,580 Number of examples for the current task : 3222\n",
      "2021-02-24 09:02:45,388 Loss for iteration 160 is 0.0471000201492147\n",
      "2021-02-24 09:03:24,448 Loss for iteration 1760 is 0.01042598845654268\n",
      "2021-02-24 09:03:53,193 Number of examples for the current task : 1934\n",
      "2021-02-24 09:04:04,280 Loss for iteration 160 is 0.029390935235741465\n",
      "2021-02-24 09:04:44,914 Loss for iteration 1760 is 0.004425907756363436\n",
      "2021-02-24 09:04:47,820 Number of examples for the current task : 4087\n",
      "2021-02-24 09:04:58,405 Loss for iteration 160 is 0.04298403300344944\n",
      "2021-02-24 09:05:36,980 Loss for iteration 1760 is 0.010710496137124227\n",
      "2021-02-24 09:06:15,134 Loss for iteration 3360 is 0.006371166997734552\n",
      "2021-02-24 09:06:28,682 Number of examples for the current task : 5615\n",
      "2021-02-24 09:06:39,230 Loss for iteration 160 is 0.04646519512276758\n",
      "2021-02-24 09:07:15,331 Loss for iteration 1760 is 0.011934192928533817\n",
      "2021-02-24 09:07:51,725 Loss for iteration 3360 is 0.007416102570102007\n",
      "2021-02-24 09:08:28,205 Loss for iteration 4960 is 0.0056593301344440745\n",
      "2021-02-24 09:08:39,682 Number of examples for the current task : 5374\n",
      "2021-02-24 09:08:50,005 Loss for iteration 160 is 0.044404926371168\n",
      "2021-02-24 09:09:24,812 Loss for iteration 1760 is 0.013160239561379107\n",
      "2021-02-24 09:09:59,147 Loss for iteration 3360 is 0.007709797477382467\n",
      "2021-02-24 09:10:33,619 Loss for iteration 4960 is 0.005890122933024646\n",
      "2021-02-24 09:10:39,974 Number of examples for the current task : 4155\n",
      "2021-02-24 09:10:50,588 Loss for iteration 160 is 0.055611117658289994\n",
      "2021-02-24 09:11:26,286 Loss for iteration 1760 is 0.012620160588732426\n",
      "2021-02-24 09:12:02,130 Loss for iteration 3360 is 0.007549422966540897\n",
      "2021-02-24 09:12:15,819 Number of examples for the current task : 2270\n",
      "2021-02-24 09:12:26,642 Loss for iteration 160 is 0.03839928106489507\n",
      "2021-02-24 09:13:05,646 Loss for iteration 1760 is 0.00733833246222521\n",
      "2021-02-24 09:13:15,218 Number of examples for the current task : 1977\n",
      "2021-02-24 09:13:25,744 Loss for iteration 160 is 0.0543936841528524\n",
      "2021-02-24 09:14:03,562 Loss for iteration 1760 is 0.009145522317749465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-24 09:14:07,326 Number of examples for the current task : 5189\n",
      "2021-02-24 09:14:18,040 Loss for iteration 160 is 0.05244341492652893\n",
      "2021-02-24 09:14:56,080 Loss for iteration 1760 is 0.013776709402435215\n",
      "2021-02-24 09:15:33,683 Loss for iteration 3360 is 0.00813365990056141\n",
      "2021-02-24 09:16:11,113 Loss for iteration 4960 is 0.006600535944262535\n",
      "2021-02-24 09:16:15,226 Number of examples for the current task : 1543\n",
      "2021-02-24 09:16:25,906 Loss for iteration 160 is 0.03438714866272428\n",
      "2021-02-24 09:16:51,019 Number of examples for the current task : 8142\n",
      "2021-02-24 09:17:01,785 Loss for iteration 160 is 0.0436824985187162\n",
      "2021-02-24 09:17:40,752 Loss for iteration 1760 is 0.014142251865590108\n",
      "2021-02-24 09:18:19,317 Loss for iteration 3360 is 0.009202268507173815\n",
      "2021-02-24 09:18:57,366 Loss for iteration 4960 is 0.007318448591352234\n",
      "2021-02-24 09:19:35,940 Loss for iteration 6560 is 0.006139341404353432\n",
      "2021-02-24 09:20:06,319 Number of examples for the current task : 4019\n",
      "2021-02-24 09:20:16,887 Loss for iteration 160 is 0.03049097430299629\n",
      "2021-02-24 09:20:52,725 Loss for iteration 1760 is 0.01030776287305278\n",
      "2021-02-24 09:21:28,606 Loss for iteration 3360 is 0.0065882037563337196\n",
      "2021-02-24 09:21:40,392 Number of examples for the current task : 1434\n",
      "2021-02-24 09:21:50,917 Loss for iteration 160 is 0.06032297607849945\n",
      "2021-02-24 09:22:13,387 Number of examples for the current task : 2315\n",
      "2021-02-24 09:22:24,017 Loss for iteration 160 is 0.02027166398792443\n",
      "2021-02-24 09:23:00,951 Loss for iteration 1760 is 0.004338972943154298\n",
      "2021-02-24 09:23:10,684 Number of examples for the current task : 1431\n",
      "2021-02-24 09:23:21,116 Loss for iteration 160 is 0.02818456334485249\n",
      "2021-02-24 09:23:42,335 Number of examples for the current task : 3281\n",
      "2021-02-24 09:23:52,902 Loss for iteration 160 is 0.041897872293537315\n",
      "2021-02-24 09:24:30,147 Loss for iteration 1760 is 0.010189397299613987\n",
      "2021-02-24 09:24:58,422 Number of examples for the current task : 2056\n",
      "2021-02-24 09:25:08,875 Loss for iteration 160 is 0.04913239265707406\n",
      "2021-02-24 09:25:45,026 Loss for iteration 1760 is 0.008813106207132994\n",
      "2021-02-24 09:25:50,064 Number of examples for the current task : 2185\n",
      "2021-02-24 09:26:00,576 Loss for iteration 160 is 0.0355988711711358\n",
      "2021-02-24 09:26:34,846 Loss for iteration 1760 is 0.007267038697325195\n",
      "2021-02-24 09:26:41,854 Number of examples for the current task : 4113\n",
      "2021-02-24 09:26:52,465 Loss for iteration 160 is 0.03788620479066263\n",
      "2021-02-24 09:27:29,795 Loss for iteration 1760 is 0.010029195858215963\n",
      "2021-02-24 09:28:07,317 Loss for iteration 3360 is 0.005805356660125029\n",
      "2021-02-24 09:28:20,790 Number of examples for the current task : 4082\n",
      "2021-02-24 09:28:31,743 Loss for iteration 160 is 0.04502852548929778\n",
      "2021-02-24 09:29:10,558 Loss for iteration 1760 is 0.012408545685183207\n",
      "2021-02-24 09:29:49,204 Loss for iteration 3360 is 0.00754267640811997\n",
      "2021-02-24 09:30:02,792 Number of examples for the current task : 2184\n",
      "2021-02-24 09:30:13,718 Loss for iteration 160 is 0.05223113111093302\n",
      "2021-02-24 09:30:52,168 Loss for iteration 1760 is 0.008954640908619317\n",
      "2021-02-24 09:30:59,658 Number of examples for the current task : 2858\n",
      "2021-02-24 09:31:10,396 Loss for iteration 160 is 0.031707548536360264\n",
      "2021-02-24 09:31:47,832 Loss for iteration 1760 is 0.0069042802623461045\n",
      "2021-02-24 09:32:08,135 Number of examples for the current task : 2097\n",
      "2021-02-24 09:32:19,033 Loss for iteration 160 is 0.045319459062408314\n",
      "2021-02-24 09:32:57,571 Loss for iteration 1760 is 0.009037154442594\n",
      "2021-02-24 09:33:03,828 Number of examples for the current task : 2528\n",
      "2021-02-24 09:33:14,841 Loss for iteration 160 is 0.03999338914978911\n",
      "2021-02-24 09:33:54,897 Loss for iteration 1760 is 0.008349361698361332\n",
      "2021-02-24 09:34:10,596 Number of examples for the current task : 1569\n",
      "2021-02-24 09:34:21,118 Loss for iteration 160 is 0.03067463877695528\n",
      "2021-02-24 09:34:45,813 Number of examples for the current task : 1477\n",
      "2021-02-24 09:34:56,463 Loss for iteration 160 is 0.027934669453481383\n",
      "2021-02-24 09:35:18,623 Number of examples for the current task : 1698\n",
      "2021-02-24 09:35:29,588 Loss for iteration 160 is 0.03596474264155735\n",
      "2021-02-24 09:35:57,985 Number of examples for the current task : 6244\n",
      "2021-02-24 09:36:08,973 Loss for iteration 160 is 0.025469458365643568\n",
      "2021-02-24 09:36:45,662 Loss for iteration 1760 is 0.00825705484021455\n",
      "2021-02-24 09:37:22,817 Loss for iteration 3360 is 0.005324700588575052\n",
      "2021-02-24 09:38:00,056 Loss for iteration 4960 is 0.004684049377560022\n",
      "2021-02-24 09:38:23,607 Number of examples for the current task : 3436\n",
      "2021-02-24 09:38:34,571 Loss for iteration 160 is 0.03544967198236422\n"
     ]
    }
   ],
   "source": [
    "my_experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-23 17:01:26,203 Initialise tokenizer\n",
      "2021-02-23 17:01:26,572 Loading pretrained model\n",
      "2021-02-23 17:01:30,747 Use default optimizer Adam\n",
      "2021-02-23 17:01:30,749 Initialise meta info\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from experiments import t5ranker\n",
    "options = t5ranker.default_options\n",
    "my_opt = struct(options)\n",
    "my_experiment = t5ranker.T5TaskRanker( my_opt, dataset)\n",
    "# load from a checkpoint (in this case each time a task loss is lower than a previous one)\n",
    "experiment_state_dict = torch.load(my_opt.save)\n",
    "my_experiment.state_dict = experiment_state_dict # state_dict being a property set state_dict will call the setter function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-23 17:01:37,008 Number of examples for the current task : 9555\n",
      "2021-02-23 17:01:48,059 Loss for iteration 160 is 0.016753503714095463\n",
      "2021-02-23 17:02:22,720 Loss for iteration 1760 is 0.02178568696720643\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1cc0c9be5203>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_experiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/CPD/repository/LifelongInformationRetrieval/examples/../experiments/t5ranker.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration_by_task\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mending_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 self.optimiser = optim.Adam(self.model.parameters(),\n",
      "\u001b[0;32m~/Documents/CPD/repository/LifelongInformationRetrieval/examples/../experiments/t5ranker.py\u001b[0m in \u001b[0;36m_train_task\u001b[0;34m(self, max_iteration, dataloader, shift)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0mloss_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mloss_n\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/CPD/repository/LifelongInformationRetrieval/examples/../experiments/t5ranker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, positive, negative)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_positive\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_negative\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         self.writer.add_scalar('batch_loss/train', loss.item(), \n\u001b[1;32m     73\u001b[0m                                 self.current_state[\"iteration\"])\n",
      "\u001b[0;32m/local/gerald/libraries/conda/envs/lire/lib/python3.9/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/gerald/libraries/conda/envs/lire/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "my_experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': 9,\n",
       " 'min_loss': {0: 0.04697433575693835,\n",
       "  1: 0.037123518672128396,\n",
       "  2: 0.0,\n",
       "  3: 0.0,\n",
       "  4: 0.0,\n",
       "  5: 0.0,\n",
       "  6: 0.0,\n",
       "  7: 0.0,\n",
       "  8: 0.0,\n",
       "  9: 0.0},\n",
       " 'iteration': 6544}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_experiment.current_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('how to oven dry peppers</s>To dry bell peppers in an oven, preheat the oven to 140 degrees Fahrenheit. Spread the sliced peppers in a single layer on a baking tray and put the trays in the oven.To ensure the heat is maintained between 145 and 150 degrees, place an oven thermometer on the tray.',\n",
       " 'how to oven dry peppers</s>Although it is an acid, the cream of tartar and the baking soda will not react when dry, so the entire reaction is saved for the mixing bowl and the oven. This is very similar to the reaction produced by baking powder in most recipes.')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_experiment.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_doc = my_experiment.tokenizer(my_experiment.dataset[0][1], return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  149,    12,  4836,  2192,  5270,     7,     1,  1875,    34,    19,\n",
       "            46,  3562,     6,     8,  3022,    13, 14981,   291,    11,     8,\n",
       "          6506, 12495,    56,    59,  8922,   116,  2192,     6,    78,     8,\n",
       "          1297,  6363,    19,  6024,    21,     8, 10623,  3047,    11,     8,\n",
       "          4836,     5,   100,    19,   182,  1126,    12,     8,  6363,  2546,\n",
       "            57,  6506,  4926,    16,   167,  5459,     5,     1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', 'false', '</s>']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = my_experiment.model.generate(input_ids=query_doc.cuda())\n",
    "my_experiment.tokenizer.convert_ids_to_tokens(out.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6136,    1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(my_experiment.index_token_negative[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out = my_experiment.model(input_ids=query_doc.cuda(), labels=my_experiment.index_token_negative[0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqLMOutput(loss=tensor(0., device='cuda:0', grad_fn=<NllLossBackward>), logits=tensor([[[-44.8086,  -2.0230, -23.2737,  ..., -46.8369, -46.8375, -46.4923],\n",
       "         [-54.6115,  45.3647, -34.9238,  ..., -83.7006, -83.7094, -83.5412]]],\n",
       "       device='cuda:0', grad_fn=<UnsafeViewBackward>), past_key_values=((tensor([[[[-7.8641e+00, -1.0998e+01, -5.3318e+00,  ..., -2.0629e+01,\n",
       "           -1.5163e+01, -7.9462e+00],\n",
       "          [ 1.2861e+01, -4.2666e+00, -3.3946e+00,  ...,  4.1916e-01,\n",
       "            1.8642e+01, -1.0817e+01]],\n",
       "\n",
       "         [[-1.2704e+01, -1.7671e+01,  4.6856e+00,  ...,  9.4959e+00,\n",
       "            1.7329e-01, -1.4422e+00],\n",
       "          [ 1.6155e+01,  1.7977e+01, -1.7975e+01,  ..., -1.2961e+01,\n",
       "            1.7898e+01,  1.0603e+01]],\n",
       "\n",
       "         [[ 2.5188e+00, -1.9332e+01, -1.7218e+00,  ..., -1.8845e+00,\n",
       "            4.0320e+00,  5.5735e+00],\n",
       "          [ 2.1294e+01,  1.3540e+01,  1.4564e+01,  ..., -2.1259e+01,\n",
       "           -1.2929e+01, -8.7379e-03]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.0676e+01,  1.1668e+01, -2.8853e+00,  ...,  5.1936e+00,\n",
       "           -2.3782e+01, -1.1400e+01],\n",
       "          [ 1.2764e+01,  1.1184e+01,  1.5494e+01,  ...,  1.6292e+01,\n",
       "            1.6188e+01,  2.4592e+01]],\n",
       "\n",
       "         [[-4.1042e+01, -1.1849e+01, -1.1360e+01,  ..., -1.1461e+01,\n",
       "            2.2502e+00,  6.1483e+00],\n",
       "          [ 1.4976e+01,  5.5557e+00,  1.0366e+01,  ...,  2.3172e+00,\n",
       "           -3.7872e+00,  6.5279e+00]],\n",
       "\n",
       "         [[-5.3139e+00, -2.2604e+00, -6.3537e+00,  ...,  2.6742e+00,\n",
       "            1.0401e+00, -2.7097e+00],\n",
       "          [ 1.6299e+01,  2.6412e+01,  2.0852e+01,  ...,  2.2654e+01,\n",
       "           -2.8721e+00, -8.0172e+00]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[ -0.7039,  12.2270,  -2.7879,  ...,   6.1690,  -4.0994,   6.0971],\n",
       "          [ -9.8048, -28.7822,  -4.5773,  ...,   7.5177,  28.3846,   2.2973]],\n",
       "\n",
       "         [[  3.5391,   1.8183, -10.1717,  ...,  -1.6173,   1.0842, -10.7800],\n",
       "          [-12.6415,  -6.1433,  -1.0528,  ...,  -8.9119, -11.7042,  23.2213]],\n",
       "\n",
       "         [[  1.7299,  -6.4139,   1.9510,  ...,  -0.6958, -17.6808,  -5.3814],\n",
       "          [-14.0963,  43.2239, -18.9895,  ...,  10.2218,  -2.8999,  38.9377]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 16.2392,   0.1935,  -1.9068,  ..., -10.8766,   9.1608,  -5.0832],\n",
       "          [-41.3203, -51.0655, -13.4660,  ...,   7.5653, -14.3757, -11.3679]],\n",
       "\n",
       "         [[ -8.4037,   2.9729,   8.9190,  ...,   5.9588,   1.6114,   6.0528],\n",
       "          [ 24.7340, -16.6911, -25.5255,  ..., -14.9329,   2.5493, -23.4293]],\n",
       "\n",
       "         [[  5.5039, -16.2509,  10.5538,  ...,   7.0779,   5.8163,  -6.9017],\n",
       "          [ 26.0591,   3.4127, -27.4719,  ...,  18.6634, -23.2173,  47.1187]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>), tensor([[[[  7.0843,   6.1294,  -2.3519,  ..., -14.5963,   5.3204,  -8.6919],\n",
       "          [  7.0541,   6.1371,  -2.2975,  ..., -14.5166,   5.2815,  -8.6194],\n",
       "          [  7.0566,   6.1295,  -2.4099,  ..., -14.5895,   5.3260,  -8.6909],\n",
       "          ...,\n",
       "          [  6.8809,   6.0589,  -2.4446,  ..., -14.2169,   5.2258,  -8.5438],\n",
       "          [  7.0245,   6.1624,  -2.3807,  ..., -14.5438,   5.2908,  -8.6559],\n",
       "          [  2.8614,   3.2886,  -1.7080,  ...,  -4.6827,   2.4633,  -3.4470]],\n",
       "\n",
       "         [[-10.5140,  -1.1634,  14.4820,  ...,  11.8928,  -8.1036,   6.7260],\n",
       "          [-10.4496,  -1.1537,  14.4200,  ...,  11.9051,  -8.0964,   6.7147],\n",
       "          [-10.4952,  -1.1403,  14.4683,  ...,  11.8698,  -8.0650,   6.7153],\n",
       "          ...,\n",
       "          [-10.2136,  -1.1410,  14.0844,  ...,  11.5762,  -7.8669,   6.6257],\n",
       "          [-10.4918,  -1.0976,  14.4809,  ...,  11.8816,  -8.0608,   6.6997],\n",
       "          [ -2.5758,  -1.4502,   3.6821,  ...,   4.0006,  -3.5987,   3.8446]],\n",
       "\n",
       "         [[  0.5591,   9.7075,   8.9998,  ...,  -2.5898,  -8.8401,  -3.8014],\n",
       "          [  0.7099,   9.6130,   8.9225,  ...,  -2.4677,  -8.8692,  -3.6859],\n",
       "          [  0.4634,   9.7256,   8.9921,  ...,  -2.6625,  -8.8808,  -3.8843],\n",
       "          ...,\n",
       "          [  0.6186,   9.2930,   8.6730,  ...,  -2.4761,  -8.7157,  -3.6941],\n",
       "          [  0.5239,   9.7331,   9.0104,  ...,  -2.6492,  -8.8876,  -3.8794],\n",
       "          [  4.9880,  -1.0532,   1.2196,  ...,   2.1596,  -3.6242,   1.4026]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-14.3892,   0.2188,   8.9240,  ...,  10.4060, -12.0959,  -1.4365],\n",
       "          [-14.3540,   0.1869,   8.8783,  ...,  10.3133, -12.0729,  -1.3252],\n",
       "          [-14.3708,   0.2313,   8.9677,  ...,  10.4202, -12.0985,  -1.3967],\n",
       "          ...,\n",
       "          [-14.1151,   0.2557,   8.8416,  ...,  10.1352, -11.8472,  -1.3518],\n",
       "          [-14.3469,   0.2174,   8.9737,  ...,  10.3941, -12.0599,  -1.3396],\n",
       "          [ -6.2445,  -0.3930,   3.0859,  ...,   2.3954,  -4.9541,   0.1827]],\n",
       "\n",
       "         [[ 10.5994,   3.7791,  -9.2084,  ...,   6.5860,  13.3041,   0.2643],\n",
       "          [ 10.5819,   3.7441,  -9.2336,  ...,   6.5979,  13.3151,   0.2058],\n",
       "          [ 10.5113,   3.7268,  -9.1957,  ...,   6.5265,  13.2729,   0.2320],\n",
       "          ...,\n",
       "          [ 10.2087,   3.5582,  -8.9788,  ...,   6.1538,  13.0839,   0.0944],\n",
       "          [ 10.5345,   3.6828,  -9.1873,  ...,   6.5484,  13.2866,   0.2239],\n",
       "          [  2.8221,   0.6079,  -3.3583,  ...,  -0.9710,   7.0787,  -2.3539]],\n",
       "\n",
       "         [[  2.4771,  -1.3236,   6.0618,  ...,   4.3443,   9.0178,   4.3354],\n",
       "          [  2.4764,  -1.2976,   6.0668,  ...,   4.2889,   8.9924,   4.3251],\n",
       "          [  2.4252,  -1.2908,   6.0742,  ...,   4.3218,   8.9987,   4.3239],\n",
       "          ...,\n",
       "          [  2.3526,  -1.2454,   5.9707,  ...,   4.1713,   8.7199,   4.1892],\n",
       "          [  2.4045,  -1.2984,   6.1113,  ...,   4.2850,   9.0438,   4.3311],\n",
       "          [  1.7332,   0.2034,   1.6585,  ...,   1.1010,   0.5073,   0.9121]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>), tensor([[[[ 15.6313,  17.6004,  14.0298,  ...,   7.5137, -11.5863,  -8.0384],\n",
       "          [ 15.6736,  17.5300,  14.0298,  ...,   7.4580, -11.4886,  -8.0277],\n",
       "          [ 15.6032,  17.4868,  14.0345,  ...,   7.4780, -11.5212,  -8.0044],\n",
       "          ...,\n",
       "          [ 15.4732,  16.9887,  13.9955,  ...,   7.2865, -11.3749,  -7.9012],\n",
       "          [ 15.6094,  17.4799,  14.0381,  ...,   7.4415, -11.4872,  -8.0007],\n",
       "          [ 11.4315,   5.9449,   9.2840,  ...,   2.7496,  -6.2953,  -4.8881]],\n",
       "\n",
       "         [[  7.1032,   4.4712,  -4.1296,  ...,  -4.8472,  -2.4964,  -1.7061],\n",
       "          [  7.0854,   4.5023,  -4.1344,  ...,  -4.8139,  -2.4605,  -1.6379],\n",
       "          [  7.0601,   4.4503,  -4.1011,  ...,  -4.8520,  -2.5359,  -1.6536],\n",
       "          ...,\n",
       "          [  6.8870,   4.7275,  -3.7723,  ...,  -4.6515,  -2.4009,  -1.4979],\n",
       "          [  7.0629,   4.3882,  -4.1399,  ...,  -4.8822,  -2.5250,  -1.6286],\n",
       "          [  2.8914,   8.8992,   2.3472,  ...,   0.6588,   0.9669,   0.6560]],\n",
       "\n",
       "         [[  2.0612,   3.7481,  26.4125,  ...,   6.0067,   0.7959, -22.3385],\n",
       "          [  1.9967,   3.8126,  26.3050,  ...,   6.0497,   0.8405, -22.3363],\n",
       "          [  2.0452,   3.7457,  26.3233,  ...,   5.9891,   0.8316, -22.2683],\n",
       "          ...,\n",
       "          [  1.8728,   3.7989,  26.1693,  ...,   6.0773,   1.0260, -22.0313],\n",
       "          [  2.0510,   3.7428,  26.2424,  ...,   5.9483,   0.8430, -22.1571],\n",
       "          [ -1.5461,   5.1071,  18.1720,  ...,   7.3304,   4.3651, -15.6140]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ -3.4402,  -6.0591,  -7.9918,  ...,  22.0245,  -7.5785,  -0.5209],\n",
       "          [ -3.4464,  -6.0410,  -7.9021,  ...,  22.0327,  -7.6191,  -0.5030],\n",
       "          [ -3.4008,  -6.0611,  -7.8682,  ...,  21.9593,  -7.5947,  -0.5702],\n",
       "          ...,\n",
       "          [ -3.2499,  -5.9878,  -7.6082,  ...,  21.7843,  -7.5907,  -0.5206],\n",
       "          [ -3.4108,  -6.0170,  -7.8287,  ...,  21.9099,  -7.6006,  -0.5964],\n",
       "          [ -0.6634,  -4.6757,  -3.2795,  ...,  15.6068,  -5.8965,   1.2247]],\n",
       "\n",
       "         [[  0.5451,   3.5937,  -5.6085,  ...,  -0.6866,  -3.2473,  -1.4844],\n",
       "          [  0.5586,   3.5298,  -5.6910,  ...,  -0.7199,  -3.2119,  -1.4158],\n",
       "          [  0.5274,   3.5597,  -5.5878,  ...,  -0.6314,  -3.2415,  -1.3928],\n",
       "          ...,\n",
       "          [  0.4209,   3.2816,  -5.5059,  ...,  -0.6592,  -3.0368,  -1.0655],\n",
       "          [  0.5098,   3.5405,  -5.6005,  ...,  -0.6020,  -3.2609,  -1.3774],\n",
       "          [ -1.4387,  -2.1887,  -4.0567,  ...,  -2.3851,   1.2784,   3.4283]],\n",
       "\n",
       "         [[ 18.9379,   3.1688,   7.7214,  ...,  -8.0636, -13.9954,  -2.6433],\n",
       "          [ 18.8587,   3.1585,   7.6347,  ...,  -8.1664, -14.0389,  -2.5553],\n",
       "          [ 18.8466,   3.1678,   7.6753,  ...,  -8.0190, -13.9696,  -2.6790],\n",
       "          ...,\n",
       "          [ 18.6385,   3.1457,   7.4125,  ...,  -8.2170, -14.0807,  -2.5694],\n",
       "          [ 18.7456,   3.1777,   7.6650,  ...,  -8.0046, -13.9225,  -2.6527],\n",
       "          [ 12.1568,   1.6073,   1.5264,  ..., -11.1709, -13.6244,   1.2037]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[  1.8248,   0.8955,  -3.5811,  ...,   1.4144, -11.2697,   9.3909],\n",
       "          [  3.0938,   2.7872,   4.7098,  ...,  -3.7126,   5.0634,  -2.2155]],\n",
       "\n",
       "         [[ -6.0527,  -4.0091,  -9.6927,  ...,   7.7472,   7.1965,  -0.8082],\n",
       "          [  5.7984,   3.9732,   2.8880,  ...,  -4.6115,  -5.8121,   3.5313]],\n",
       "\n",
       "         [[-15.7741,  -9.4475,   2.3884,  ..., -12.8003,  -0.5104,  12.2996],\n",
       "          [ 10.3729,   3.7824,  -2.2328,  ...,  12.1652,  -1.2691, -11.4019]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  3.9634,  -7.2067,  -1.2511,  ...,  -9.3145,  -2.9306,  -5.6538],\n",
       "          [  0.0236,   6.5080,   4.6056,  ...,   6.5775,   7.4456,  11.1263]],\n",
       "\n",
       "         [[ -4.1469,   5.1160,  -3.2839,  ...,   0.3867,  -1.6873,  -2.4273],\n",
       "          [  6.7106,  -5.5584,   5.7550,  ...,   2.0552,   1.9120,   5.7141]],\n",
       "\n",
       "         [[  1.1442,   5.2804,   4.6127,  ...,  -7.3843,  -2.3759,  -4.4649],\n",
       "          [  0.9199,  -5.0289,  -4.2495,  ...,   7.6495,   4.6661,   1.8939]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>), tensor([[[[ 10.8758,  -8.2809,  15.4391,  ...,   7.1270,  12.0598,   6.3720],\n",
       "          [-13.3590,   6.8823, -13.8673,  ...,  -2.4929,  -5.4064, -11.6366]],\n",
       "\n",
       "         [[-12.3871,   5.4315,  -8.1326,  ...,  -5.4339,  14.1180,  -2.7911],\n",
       "          [  2.0489,  -5.2388,   0.2611,  ...,   1.2152,  -3.2499, -11.3910]],\n",
       "\n",
       "         [[  9.7717,   2.1207,  10.1900,  ...,   5.8572,  12.2926,   2.3093],\n",
       "          [ -7.2527,   6.0162,  -1.8438,  ...,  -4.8355,  -7.5208,   5.5769]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  7.5662,   3.6381, -11.2532,  ...,  13.0095, -12.3132,  -4.8209],\n",
       "          [ -7.3894,   2.5158,  12.3363,  ..., -20.1271,   5.6729,   5.5904]],\n",
       "\n",
       "         [[ -6.6723, -12.4137,  13.3615,  ...,  -6.1311,  11.3926,  -9.0675],\n",
       "          [ 10.4609,   5.9621,  -9.4677,  ...,   3.8978,  -8.9433,   4.8242]],\n",
       "\n",
       "         [[ -4.9006,  12.2121,  15.0317,  ...,   7.1164,   5.1370, -28.2040],\n",
       "          [  6.2410, -10.2921, -18.1822,  ...,  -6.1613,  -1.0961,  16.4741]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>), tensor([[[[  4.3859,  -8.7726,   5.0332,  ...,  -3.6803,   3.6393, -16.5108],\n",
       "          [  4.4404,  -8.7328,   4.9870,  ...,  -3.6049,   3.5788, -16.5339],\n",
       "          [  4.3803,  -8.7435,   5.0515,  ...,  -3.6381,   3.5962, -16.4124],\n",
       "          ...,\n",
       "          [  4.2787,  -8.5902,   4.9288,  ...,  -3.5146,   3.4084, -15.9441],\n",
       "          [  4.3915,  -8.6930,   4.9996,  ...,  -3.6287,   3.5653, -16.4200],\n",
       "          [  1.9133,  -5.1070,   2.3987,  ...,  -0.9034,  -0.0386,  -6.1820]],\n",
       "\n",
       "         [[  1.0288,   1.4135,  12.9250,  ...,  16.3677,  -8.0591,  12.9428],\n",
       "          [  1.0326,   1.4357,  12.9415,  ...,  16.3822,  -8.0452,  12.9405],\n",
       "          [  1.0012,   1.3794,  12.8619,  ...,  16.3235,  -8.0469,  12.8863],\n",
       "          ...,\n",
       "          [  1.1210,   1.4043,  12.6679,  ...,  16.1157,  -8.0252,  12.6202],\n",
       "          [  0.9822,   1.3588,  12.8687,  ...,  16.3311,  -8.0586,  12.9155],\n",
       "          [  3.0625,   2.0559,   7.4567,  ...,   9.2176,  -5.6821,   5.4552]],\n",
       "\n",
       "         [[ 16.1769,   7.3909,   5.2702,  ...,   6.4706, -11.6138, -17.5950],\n",
       "          [ 16.1067,   7.4302,   5.2501,  ...,   6.4374, -11.6008, -17.5511],\n",
       "          [ 16.1660,   7.3732,   5.2701,  ...,   6.4777, -11.6405, -17.5918],\n",
       "          ...,\n",
       "          [ 15.8485,   7.1709,   5.2001,  ...,   6.2948, -11.4777, -17.2226],\n",
       "          [ 16.1249,   7.3953,   5.2662,  ...,   6.4652, -11.6527, -17.5993],\n",
       "          [  6.6163,   2.4717,   3.1564,  ...,   1.7863,  -5.4372,  -6.3614]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 12.1733,   2.9562,  18.7789,  ...,   6.2925,  16.0922,  11.9839],\n",
       "          [ 12.1715,   2.9659,  18.7634,  ...,   6.3033,  16.0851,  11.9436],\n",
       "          [ 12.1390,   2.9476,  18.7304,  ...,   6.2826,  16.0176,  11.9127],\n",
       "          ...,\n",
       "          [ 11.9930,   2.9350,  18.5076,  ...,   6.2595,  15.7856,  11.7375],\n",
       "          [ 12.1040,   2.9569,  18.7172,  ...,   6.2561,  15.9730,  11.8955],\n",
       "          [  6.8157,   2.5383,  11.3298,  ...,   5.0673,   9.7066,   6.5282]],\n",
       "\n",
       "         [[ 13.9547,  21.0769, -18.3463,  ...,  16.1248,  18.2030, -18.6316],\n",
       "          [ 13.8940,  21.0497, -18.2342,  ...,  16.0060,  18.1947, -18.5029],\n",
       "          [ 13.8910,  21.0406, -18.2814,  ...,  16.0604,  18.1212, -18.6006],\n",
       "          ...,\n",
       "          [ 13.5988,  20.6255, -17.6558,  ...,  15.5728,  17.7957, -18.0592],\n",
       "          [ 13.8507,  21.0482, -18.2984,  ...,  16.0398,  18.1118, -18.5662],\n",
       "          [  5.5607,   9.1740,  -3.2659,  ...,   4.0911,   9.2159,  -5.3088]],\n",
       "\n",
       "         [[  0.1871,  17.3621,   5.3048,  ...,  -5.6322,   5.2238,  10.4598],\n",
       "          [  0.0506,  17.3043,   5.2866,  ...,  -5.6070,   5.2768,  10.4680],\n",
       "          [  0.2591,  17.3035,   5.2327,  ...,  -5.5735,   5.2207,  10.4246],\n",
       "          ...,\n",
       "          [  0.3593,  16.9157,   4.6571,  ...,  -5.4803,   5.0988,  10.2930],\n",
       "          [  0.2028,  17.3061,   5.2713,  ...,  -5.5423,   5.2764,  10.4265],\n",
       "          [  0.9242,   7.7943,  -6.0687,  ...,  -4.3607,   1.6009,   6.7982]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>), tensor([[[[  0.4678,   4.0819,  -7.3744,  ...,  -2.6632,   0.4951, -14.2872],\n",
       "          [  0.5276,   4.1253,  -7.3012,  ...,  -2.6598,   0.4397, -14.2023],\n",
       "          [  0.4674,   4.0607,  -7.3159,  ...,  -2.6821,   0.4687, -14.2353],\n",
       "          ...,\n",
       "          [  0.4372,   4.0264,  -6.9884,  ...,  -2.3759,   0.1948, -13.9176],\n",
       "          [  0.5000,   4.0630,  -7.3063,  ...,  -2.7220,   0.4325, -14.1842],\n",
       "          [ -1.0749,   2.3479,   0.6450,  ...,   3.4805,  -2.2007,  -6.5186]],\n",
       "\n",
       "         [[ -9.0479, -12.9330,  -4.7760,  ..., -21.5801,   2.2179,  16.3443],\n",
       "          [ -9.0008, -12.9409,  -4.7373,  ..., -21.5566,   2.2790,  16.3567],\n",
       "          [ -9.0142, -12.9313,  -4.7498,  ..., -21.5042,   2.2348,  16.2290],\n",
       "          ...,\n",
       "          [ -8.8117, -12.9177,  -4.6852,  ..., -21.2667,   2.4467,  16.0118],\n",
       "          [ -8.9463, -12.9201,  -4.7574,  ..., -21.4527,   2.2136,  16.2045],\n",
       "          [ -3.9350, -10.7476,  -1.4060,  ..., -14.4165,   5.1634,  10.1682]],\n",
       "\n",
       "         [[  5.9718,  -0.5761,   0.3901,  ...,   5.2564,   5.4766,   1.5218],\n",
       "          [  5.9967,  -0.6111,   0.3785,  ...,   5.1998,   5.4172,   1.5243],\n",
       "          [  5.9476,  -0.6112,   0.3965,  ...,   5.2587,   5.4624,   1.4993],\n",
       "          ...,\n",
       "          [  5.8106,  -0.6539,   0.4503,  ...,   5.2150,   5.5183,   1.6127],\n",
       "          [  5.9604,  -0.6038,   0.4012,  ...,   5.2215,   5.4467,   1.5174],\n",
       "          [  2.8214,  -1.7134,   0.7249,  ...,   3.4254,   5.3342,   3.6943]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  6.9138,   7.3030,   5.8602,  ..., -15.4351,  -5.9644,  -4.0573],\n",
       "          [  6.8755,   7.3266,   5.9323,  ..., -15.3776,  -6.0137,  -4.0493],\n",
       "          [  6.8788,   7.3367,   5.8093,  ..., -15.3184,  -5.9756,  -4.0282],\n",
       "          ...,\n",
       "          [  6.8206,   7.3509,   5.7777,  ..., -15.1811,  -5.9248,  -3.8460],\n",
       "          [  6.8660,   7.3463,   5.8252,  ..., -15.2829,  -5.9903,  -4.0390],\n",
       "          [  4.3986,   5.2453,   4.3359,  ..., -11.2371,  -3.2351,  -1.1864]],\n",
       "\n",
       "         [[  0.6066,   8.7692,   2.0942,  ...,   1.6816,  17.5411,  -1.5345],\n",
       "          [  0.6407,   8.7135,   2.1374,  ...,   1.6723,  17.5651,  -1.4915],\n",
       "          [  0.6196,   8.7387,   2.0749,  ...,   1.6869,  17.5495,  -1.5379],\n",
       "          ...,\n",
       "          [  0.7205,   8.5462,   2.2257,  ...,   1.7311,  17.4684,  -1.4133],\n",
       "          [  0.6449,   8.7262,   2.0421,  ...,   1.6561,  17.4973,  -1.5104],\n",
       "          [  2.4734,   3.2179,   4.6133,  ...,   2.1477,  11.9230,   1.7615]],\n",
       "\n",
       "         [[-11.9332,  -3.3928,  -3.4424,  ...,  -3.0678,   0.8543,   3.9818],\n",
       "          [-11.9207,  -3.4758,  -3.3637,  ...,  -3.0215,   0.8587,   3.8947],\n",
       "          [-11.8701,  -3.3941,  -3.4372,  ...,  -3.0397,   0.8639,   3.9508],\n",
       "          ...,\n",
       "          [-11.6702,  -3.5279,  -3.3822,  ...,  -2.8705,   0.8436,   3.7636],\n",
       "          [-11.8318,  -3.4037,  -3.3756,  ...,  -3.0359,   0.8494,   3.9348],\n",
       "          [ -6.5053,  -4.8899,  -1.8120,  ...,  -0.4800,   0.7522,  -0.8866]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[  5.3155,  -3.3207,   6.4090,  ...,  -1.5650,   1.0135,   1.3587],\n",
       "          [ -3.1115,   4.7582,  -0.7355,  ...,   0.6726,   1.7419,   2.0567]],\n",
       "\n",
       "         [[ -3.0695,   2.0784,  -0.9104,  ...,   4.5375,   1.1401,  -4.0136],\n",
       "          [  3.1694,  -1.9565,   0.9893,  ...,   1.7547,  -1.6772,   1.6389]],\n",
       "\n",
       "         [[  0.8862,  -7.2099,  -0.1267,  ...,   9.9705,  -5.0135,  -2.6716],\n",
       "          [  1.9048,   7.3570,  -1.0343,  ...,  -6.4344,   9.5103,   1.1478]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0.5933,  -6.1822,   6.0230,  ...,   8.5668,  -7.0039,   0.1795],\n",
       "          [ -4.2483,   1.0522,  -2.4065,  ...,  -5.6831,   4.7776,  -5.1294]],\n",
       "\n",
       "         [[ -2.7603,   9.9048,  -8.5237,  ..., -10.1000,   6.2552,   2.3273],\n",
       "          [  0.2243,  -6.5716,  14.1550,  ...,  10.4990, -13.1765, -13.4677]],\n",
       "\n",
       "         [[ -3.1791,   0.3685,  -6.1811,  ...,  -0.7683,   2.3859,  -2.4419],\n",
       "          [  1.2207,   3.8959,   3.7195,  ...,   6.6597,   1.5518,   4.1128]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>), tensor([[[[ 10.8005,  -0.4563,   4.8413,  ...,  -5.8519,  -0.3900,   2.8616],\n",
       "          [-10.2543,  -3.6304,  -6.4504,  ...,  15.9184,  -0.6756,  -2.2594]],\n",
       "\n",
       "         [[-14.1551,  -0.8313,  -7.2420,  ...,  15.7982,   0.8341,   1.6472],\n",
       "          [  8.1959,  -8.1877,   0.2233,  ..., -17.8072,   8.6391,  -0.5417]],\n",
       "\n",
       "         [[ -3.6274,  -2.4838, -12.0967,  ...,   7.0470,  -2.0169,  -0.0572],\n",
       "          [  9.5975,   3.0018,   9.7981,  ...,   4.2137,   1.6936,   6.8036]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  1.2883,   5.4026,  -3.0476,  ...,  -3.2616,   2.5908,  -6.1591],\n",
       "          [  5.5385,  -2.1426,   4.9453,  ...,   1.9790,   1.4890,   3.4755]],\n",
       "\n",
       "         [[  7.4759,   2.0823,  14.9696,  ...,  -7.4841,  -4.6528,  -1.6080],\n",
       "          [ -4.2191,  -2.0201,  -5.9662,  ...,  -0.4347,  -0.9652,  -2.2058]],\n",
       "\n",
       "         [[  1.7664,  -0.1729,  -4.8394,  ..., -13.9620,  -6.0074,   4.6626],\n",
       "          [ 12.2833,  -7.3907,  -0.6903,  ...,   7.3539,  -4.5098,   0.3108]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>), tensor([[[[ 1.2126e+01, -9.1526e-01, -6.0494e+00,  ..., -1.0335e+01,\n",
       "           -1.2151e+01,  1.6134e+01],\n",
       "          [ 1.2052e+01, -8.8851e-01, -6.0385e+00,  ..., -1.0315e+01,\n",
       "           -1.2184e+01,  1.6113e+01],\n",
       "          [ 1.2088e+01, -9.1777e-01, -6.0699e+00,  ..., -1.0299e+01,\n",
       "           -1.2118e+01,  1.6096e+01],\n",
       "          ...,\n",
       "          [ 1.1710e+01, -9.1080e-01, -6.0574e+00,  ..., -1.0160e+01,\n",
       "           -1.1844e+01,  1.5726e+01],\n",
       "          [ 1.2056e+01, -8.8545e-01, -6.0436e+00,  ..., -1.0267e+01,\n",
       "           -1.2127e+01,  1.6083e+01],\n",
       "          [ 2.9975e+00, -5.1549e-01, -4.9250e+00,  ..., -6.3201e+00,\n",
       "           -4.9072e+00,  7.2094e+00]],\n",
       "\n",
       "         [[ 1.8933e+01, -3.3556e+00, -3.0824e+00,  ...,  9.8919e-01,\n",
       "            1.0091e+01,  1.2792e+01],\n",
       "          [ 1.8881e+01, -3.2860e+00, -3.1220e+00,  ...,  9.9940e-01,\n",
       "            1.0052e+01,  1.2804e+01],\n",
       "          [ 1.8902e+01, -3.3190e+00, -3.0459e+00,  ...,  9.8055e-01,\n",
       "            1.0010e+01,  1.2760e+01],\n",
       "          ...,\n",
       "          [ 1.8750e+01, -3.0835e+00, -3.0158e+00,  ...,  1.0058e+00,\n",
       "            9.7582e+00,  1.2490e+01],\n",
       "          [ 1.8824e+01, -3.2907e+00, -3.0791e+00,  ...,  9.5649e-01,\n",
       "            9.9898e+00,  1.2766e+01],\n",
       "          [ 1.2337e+01,  1.5038e+00, -2.3256e+00,  ...,  1.9471e+00,\n",
       "            4.2864e+00,  5.1950e+00]],\n",
       "\n",
       "         [[-1.4786e+00, -6.2700e+00,  2.1386e+01,  ..., -1.3581e+01,\n",
       "            1.3356e+01, -1.7552e+01],\n",
       "          [-1.4397e+00, -6.2295e+00,  2.1319e+01,  ..., -1.3511e+01,\n",
       "            1.3340e+01, -1.7533e+01],\n",
       "          [-1.4800e+00, -6.2811e+00,  2.1352e+01,  ..., -1.3555e+01,\n",
       "            1.3307e+01, -1.7478e+01],\n",
       "          ...,\n",
       "          [-1.5096e+00, -6.0713e+00,  2.1030e+01,  ..., -1.3445e+01,\n",
       "            1.3092e+01, -1.7267e+01],\n",
       "          [-1.4520e+00, -6.3051e+00,  2.1290e+01,  ..., -1.3499e+01,\n",
       "            1.3294e+01, -1.7437e+01],\n",
       "          [-1.7221e+00,  1.9617e-03,  1.1077e+01,  ..., -9.0881e+00,\n",
       "            6.9868e+00, -1.0356e+01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-4.6345e+00,  1.8154e+01, -1.6618e+01,  ..., -1.5822e+01,\n",
       "            1.6772e+01, -1.0670e+00],\n",
       "          [-4.6271e+00,  1.8043e+01, -1.6600e+01,  ..., -1.5801e+01,\n",
       "            1.6718e+01, -1.0684e+00],\n",
       "          [-4.5823e+00,  1.8054e+01, -1.6547e+01,  ..., -1.5767e+01,\n",
       "            1.6690e+01, -9.8841e-01],\n",
       "          ...,\n",
       "          [-4.6217e+00,  1.7567e+01, -1.6193e+01,  ..., -1.5451e+01,\n",
       "            1.6231e+01, -9.5905e-01],\n",
       "          [-4.5803e+00,  1.8012e+01, -1.6510e+01,  ..., -1.5757e+01,\n",
       "            1.6668e+01, -9.6801e-01],\n",
       "          [-4.6019e+00,  6.3230e+00, -7.8294e+00,  ..., -6.8890e+00,\n",
       "            5.7291e+00, -1.7064e+00]],\n",
       "\n",
       "         [[ 2.3516e+00,  1.5214e+00,  8.3187e+00,  ..., -5.8866e-01,\n",
       "            4.3988e+00, -9.4579e+00],\n",
       "          [ 2.3410e+00,  1.5258e+00,  8.3699e+00,  ..., -6.2927e-01,\n",
       "            4.3562e+00, -9.4429e+00],\n",
       "          [ 2.4342e+00,  1.4437e+00,  8.1627e+00,  ..., -5.2977e-01,\n",
       "            4.4778e+00, -9.4217e+00],\n",
       "          ...,\n",
       "          [ 2.3302e+00,  1.5247e+00,  8.0417e+00,  ..., -5.6935e-01,\n",
       "            4.4017e+00, -9.2530e+00],\n",
       "          [ 2.4545e+00,  1.4268e+00,  8.1714e+00,  ..., -5.2942e-01,\n",
       "            4.4500e+00, -9.4061e+00],\n",
       "          [-8.2274e-01,  3.0076e+00,  5.7806e+00,  ..., -1.6359e+00,\n",
       "            1.7098e+00, -4.8127e+00]],\n",
       "\n",
       "         [[ 9.9313e+00,  1.3698e+01, -9.5059e+00,  ..., -5.0355e+00,\n",
       "            4.9276e+00,  2.3695e+00],\n",
       "          [ 9.9318e+00,  1.3646e+01, -9.5125e+00,  ..., -5.0363e+00,\n",
       "            4.9331e+00,  2.3082e+00],\n",
       "          [ 9.9191e+00,  1.3668e+01, -9.4393e+00,  ..., -5.0080e+00,\n",
       "            4.9317e+00,  2.3577e+00],\n",
       "          ...,\n",
       "          [ 9.6959e+00,  1.3444e+01, -8.9209e+00,  ..., -4.6136e+00,\n",
       "            4.7934e+00,  2.2399e+00],\n",
       "          [ 9.9370e+00,  1.3641e+01, -9.4819e+00,  ..., -5.0397e+00,\n",
       "            4.9492e+00,  2.3185e+00],\n",
       "          [ 4.3320e+00,  7.0214e+00,  1.5268e+00,  ...,  2.5246e+00,\n",
       "            1.9929e+00, -1.5486e-01]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[ -6.2488,  -2.6563,   7.2995,  ...,  -4.9896,   2.1536,   4.3413],\n",
       "          [ -6.1940,  -2.4935,   7.2320,  ...,  -5.0733,   2.1166,   4.3650],\n",
       "          [ -6.2015,  -2.5969,   7.2249,  ...,  -4.9628,   2.1574,   4.3312],\n",
       "          ...,\n",
       "          [ -6.1561,  -2.1424,   6.9435,  ...,  -4.9569,   2.1335,   4.1398],\n",
       "          [ -6.1364,  -2.5321,   7.2322,  ...,  -4.9261,   2.1244,   4.3466],\n",
       "          [ -4.6744,   5.8057,   2.3588,  ...,  -4.5920,   2.4352,  -0.6596]],\n",
       "\n",
       "         [[  7.6853,   1.6750,   2.4637,  ..., -14.0160,  -7.6274,   5.0540],\n",
       "          [  7.7748,   1.6697,   2.5184,  ..., -14.0230,  -7.5794,   5.0667],\n",
       "          [  7.6802,   1.6221,   2.4639,  ..., -13.9653,  -7.6067,   5.0344],\n",
       "          ...,\n",
       "          [  7.6213,   1.3667,   2.4044,  ..., -13.6411,  -7.4300,   4.7075],\n",
       "          [  7.6998,   1.6161,   2.4333,  ..., -13.9716,  -7.5260,   5.0545],\n",
       "          [  5.3892,  -3.5736,   1.0792,  ...,  -5.0989,  -3.4681,  -1.3375]],\n",
       "\n",
       "         [[ -8.7598, -19.2434,  10.1207,  ...,  -2.5329,   3.4052, -18.1826],\n",
       "          [ -8.8254, -19.2368,  10.0448,  ...,  -2.5201,   3.4644, -18.0185],\n",
       "          [ -8.7626, -19.1938,  10.0231,  ...,  -2.4575,   3.3229, -18.1356],\n",
       "          ...,\n",
       "          [ -8.7732, -19.1659,   9.8081,  ...,  -2.5168,   3.5813, -17.8017],\n",
       "          [ -8.7590, -19.1455,   9.9853,  ...,  -2.4844,   3.2887, -18.0813],\n",
       "          [ -7.7751, -15.3353,   3.3852,  ...,  -3.9264,   8.5557,  -8.6204]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  3.8988,  20.3670,  14.4136,  ..., -10.3771, -19.2617,   0.3407],\n",
       "          [  3.8960,  20.3562,  14.4527,  ..., -10.3676, -19.2283,   0.4042],\n",
       "          [  3.9424,  20.3120,  14.3985,  ..., -10.4064, -19.2142,   0.3668],\n",
       "          ...,\n",
       "          [  3.8270,  19.9155,  14.4492,  ..., -10.2676, -18.9329,   0.5280],\n",
       "          [  3.8862,  20.2581,  14.3747,  ..., -10.4324, -19.1550,   0.3824],\n",
       "          [  1.0825,  10.3675,  12.7478,  ...,  -6.4002,  -9.6830,   4.0136]],\n",
       "\n",
       "         [[  9.2888,  14.6502,   3.1773,  ...,  -8.3984,  10.3307,   6.1426],\n",
       "          [  9.3449,  14.5856,   3.2259,  ...,  -8.3362,  10.2158,   6.1784],\n",
       "          [  9.2490,  14.6345,   3.2275,  ...,  -8.3270,  10.2564,   6.0930],\n",
       "          ...,\n",
       "          [  9.2019,  14.3992,   3.4454,  ...,  -8.2417,  10.0903,   6.0718],\n",
       "          [  9.2241,  14.6576,   3.2131,  ...,  -8.3035,  10.2369,   6.0963],\n",
       "          [  7.5135,   6.6228,   6.4664,  ...,  -5.1287,   6.1358,   5.1459]],\n",
       "\n",
       "         [[ -5.6388, -15.5777,  12.5581,  ...,   6.1799,   5.2180,  -4.8577],\n",
       "          [ -5.5700, -15.5828,  12.5516,  ...,   6.2552,   5.1592,  -4.7954],\n",
       "          [ -5.6187, -15.5526,  12.5179,  ...,   6.2317,   5.2361,  -4.8499],\n",
       "          ...,\n",
       "          [ -5.6069, -15.5726,  12.5043,  ...,   6.2867,   5.0986,  -4.5752],\n",
       "          [ -5.5759, -15.4999,  12.5078,  ...,   6.2274,   5.2131,  -4.8443],\n",
       "          [ -3.1865, -13.1855,  10.7229,  ...,   6.0922,   0.0414,   1.0634]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[ 1.2583e+01,  4.2947e+00,  4.5436e-01,  ...,  1.2211e+00,\n",
       "            1.2508e+00,  2.8642e+00],\n",
       "          [-8.5679e+00, -7.5047e+00, -1.0062e+01,  ...,  1.0309e+01,\n",
       "           -1.8111e+00, -1.0382e+01]],\n",
       "\n",
       "         [[-1.0167e+01, -5.0841e+00, -6.8121e+00,  ..., -4.9442e+00,\n",
       "            5.4298e-01,  9.8552e+00],\n",
       "          [ 4.2619e-01,  2.1160e+00,  2.9516e+00,  ...,  9.1711e-01,\n",
       "           -2.4109e+00, -7.4048e+00]],\n",
       "\n",
       "         [[-7.0455e+00,  1.5262e-01,  7.0214e-01,  ..., -5.0320e+00,\n",
       "            2.4105e+00, -4.3546e-01],\n",
       "          [ 6.0587e+00, -5.9189e+00, -3.8012e-01,  ...,  8.0844e-03,\n",
       "           -2.5992e+00, -6.6999e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.4352e-01,  3.3548e+00, -4.1412e+00,  ..., -4.2407e-01,\n",
       "            4.5163e-01, -6.2299e+00],\n",
       "          [ 5.2579e+00, -5.3151e+00,  3.8606e+00,  ...,  7.3532e-01,\n",
       "            2.9052e+00,  6.9655e+00]],\n",
       "\n",
       "         [[ 4.1045e+00, -6.9854e+00, -6.8521e+00,  ..., -2.5504e+00,\n",
       "            8.2278e-01, -5.9084e+00],\n",
       "          [-1.8004e+00,  1.0660e+01,  1.0079e+01,  ..., -1.7139e+00,\n",
       "           -1.1259e+01,  7.6101e+00]],\n",
       "\n",
       "         [[ 4.5707e+00, -5.0786e+00, -6.6896e-01,  ..., -2.3243e+00,\n",
       "           -4.8549e+00, -3.0870e+00],\n",
       "          [-5.4400e+00, -1.0502e+00,  3.9404e+00,  ...,  3.1224e+00,\n",
       "           -1.8345e+00,  1.0157e+00]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[  1.2635, -24.4296, -27.2698,  ...,  -8.5768,  -9.9506,   2.6497],\n",
       "          [  5.2164,   6.5282,  14.1319,  ...,   4.7196,   7.8940,   2.1983]],\n",
       "\n",
       "         [[ 25.9578, -14.0295, -14.3033,  ..., -25.6757,  -9.3775, -14.1039],\n",
       "          [ -9.0800,  10.9769,   0.0521,  ...,  14.0684,   9.9341,  10.9958]],\n",
       "\n",
       "         [[ 14.8461,  20.8632,  43.0737,  ...,  -8.8854,   8.1657, -11.0231],\n",
       "          [-18.2434, -11.9201, -27.8539,  ...,  -3.7045,   6.8390,  11.2649]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  3.1472,  -7.0381,  -1.2996,  ...,  -7.1073,  -2.0202,  11.7575],\n",
       "          [ -2.2230,   0.7871,  -9.1427,  ...,   5.9551,  -8.5044, -12.5112]],\n",
       "\n",
       "         [[  7.5843,   3.7220, -11.9592,  ..., -10.5697,   5.5299,  -1.9762],\n",
       "          [  2.5139,  -5.3647,   4.9710,  ...,   7.6271,   5.3222,   9.2210]],\n",
       "\n",
       "         [[  5.0391,   1.6226,  -3.8490,  ..., -17.2552,   3.0930,   3.7739],\n",
       "          [ 11.2815,  17.7976,   0.7255,  ...,   8.7643,  -3.8600,   0.1142]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>), tensor([[[[  3.0132,  -6.6592,  -1.6528,  ...,   6.5916,  -1.4342, -10.7273],\n",
       "          [  3.0091,  -6.7573,  -1.5527,  ...,   6.6119,  -1.4531, -10.7390],\n",
       "          [  2.9474,  -6.5676,  -1.7027,  ...,   6.4739,  -1.3856, -10.6117],\n",
       "          ...,\n",
       "          [  2.8861,  -6.4025,  -1.6757,  ...,   6.1846,  -1.3356, -10.3721],\n",
       "          [  2.9283,  -6.5699,  -1.6341,  ...,   6.4867,  -1.3756, -10.6184],\n",
       "          [  2.2034,  -3.2612,  -0.1886,  ...,   1.3543,  -0.4288,  -5.3119]],\n",
       "\n",
       "         [[ -6.3419,   5.1482, -12.2873,  ..., -14.7614,   7.1050,   6.5442],\n",
       "          [ -6.2958,   5.1382, -12.3074,  ..., -14.7504,   7.1142,   6.5519],\n",
       "          [ -6.2980,   5.1697, -12.2988,  ..., -14.6973,   7.0901,   6.5102],\n",
       "          ...,\n",
       "          [ -6.0630,   5.0365, -12.1427,  ..., -14.4669,   6.9701,   6.4726],\n",
       "          [ -6.2796,   5.1857, -12.3186,  ..., -14.6971,   7.0861,   6.5325],\n",
       "          [ -0.8496,   0.9771,  -6.8201,  ...,  -7.8794,   3.4821,   4.2573]],\n",
       "\n",
       "         [[  1.0015,   1.7966,  -0.6229,  ...,  -8.4391,   3.4377,   3.3559],\n",
       "          [  1.0216,   1.8214,  -0.6414,  ...,  -8.3640,   3.4303,   3.3478],\n",
       "          [  0.9946,   1.7951,  -0.6192,  ...,  -8.4509,   3.3715,   3.3420],\n",
       "          ...,\n",
       "          [  0.8020,   1.7355,  -0.8006,  ...,  -8.1857,   3.4249,   2.9107],\n",
       "          [  1.0331,   1.8598,  -0.5773,  ...,  -8.4471,   3.3447,   3.4423],\n",
       "          [ -3.8339,  -0.7496,  -4.8267,  ...,  -1.5499,   4.8495,  -5.7762]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-15.3453,   5.5716, -12.3493,  ...,  -1.7027,   4.8089,  -4.2813],\n",
       "          [-15.3375,   5.5110, -12.2887,  ...,  -1.7523,   4.8411,  -4.2383],\n",
       "          [-15.2366,   5.4800, -12.2861,  ...,  -1.6592,   4.7998,  -4.2109],\n",
       "          ...,\n",
       "          [-14.9017,   5.2226, -11.9702,  ...,  -1.5882,   4.6049,  -3.9777],\n",
       "          [-15.2357,   5.4688, -12.2704,  ...,  -1.6947,   4.8284,  -4.1925],\n",
       "          [ -6.6173,   0.2935,  -5.1376,  ...,  -0.1608,   0.0625,   0.4949]],\n",
       "\n",
       "         [[ -5.8504,   0.3043,   3.0985,  ..., -10.1234,  -1.0012,   4.8287],\n",
       "          [ -5.8859,   0.3651,   3.0472,  ..., -10.1264,  -1.0093,   4.8301],\n",
       "          [ -5.7688,   0.2392,   3.1087,  ..., -10.1216,  -1.0295,   4.7804],\n",
       "          ...,\n",
       "          [ -5.7687,   0.3732,   2.9272,  ...,  -9.9616,  -0.7784,   4.6377],\n",
       "          [ -5.7369,   0.2310,   3.1165,  ..., -10.1341,  -1.0878,   4.7537],\n",
       "          [ -6.0898,   3.6084,  -0.8644,  ...,  -4.8306,   4.6136,   1.5796]],\n",
       "\n",
       "         [[  0.5920,   8.7138,  10.4385,  ...,  -7.1285,   1.4562,  14.1091],\n",
       "          [  0.6063,   8.6741,  10.3728,  ...,  -7.1132,   1.4486,  14.1171],\n",
       "          [  0.5308,   8.7102,  10.4539,  ...,  -7.0362,   1.5184,  14.0700],\n",
       "          ...,\n",
       "          [  0.4027,   8.6368,  10.2729,  ...,  -6.9105,   1.6327,  13.8405],\n",
       "          [  0.5408,   8.7265,  10.4150,  ...,  -7.0286,   1.5293,  14.0775],\n",
       "          [ -0.7123,   5.3340,   4.4547,  ...,  -3.5140,   2.7810,   7.1792]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>), tensor([[[[-21.1296,  23.3768, -15.5930,  ..., -14.7883,  -2.0219,   9.7961],\n",
       "          [-21.1209,  23.3571, -15.6229,  ..., -14.7894,  -1.9515,   9.8056],\n",
       "          [-21.0534,  23.3548, -15.5839,  ..., -14.7159,  -2.0237,   9.8361],\n",
       "          ...,\n",
       "          [-20.6475,  23.1292, -15.4924,  ..., -14.4417,  -1.9273,   9.7101],\n",
       "          [-21.1044,  23.3299, -15.5338,  ..., -14.6931,  -2.0160,   9.7706],\n",
       "          [ -8.8225,  15.9706, -11.8369,  ...,  -8.1061,   0.8843,   5.0307]],\n",
       "\n",
       "         [[ -8.8322,   6.0641,  10.1755,  ..., -11.4705, -11.1614,  -4.8804],\n",
       "          [ -8.8089,   6.1035,  10.1487,  ..., -11.4263, -11.0669,  -4.8215],\n",
       "          [ -8.8031,   6.0507,  10.0709,  ..., -11.3752, -11.0865,  -4.8599],\n",
       "          ...,\n",
       "          [ -8.6867,   6.0642,   9.8029,  ..., -10.9892, -10.8087,  -4.8071],\n",
       "          [ -8.8189,   6.0405,  10.0858,  ..., -11.4013, -11.0454,  -4.8403],\n",
       "          [ -4.9091,   5.4225,   3.2573,  ...,  -3.6385,  -5.0159,  -4.7346]],\n",
       "\n",
       "         [[ -7.1277,  -3.0468,  -2.2342,  ...,   6.3334,   3.6496,   3.7228],\n",
       "          [ -7.0304,  -2.9298,  -2.1401,  ...,   6.4093,   3.6402,   3.8144],\n",
       "          [ -7.1110,  -2.9774,  -2.1521,  ...,   6.3965,   3.7149,   3.7432],\n",
       "          ...,\n",
       "          [ -6.8974,  -2.8278,  -1.9211,  ...,   6.4706,   3.8445,   3.8802],\n",
       "          [ -7.0652,  -2.9735,  -2.1847,  ...,   6.3714,   3.6295,   3.7232],\n",
       "          [  0.4355,   0.2672,   1.5570,  ...,   6.8361,   4.3946,   5.7887]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ -5.9240,   6.5970,  16.0566,  ...,  -9.0374,  13.1643, -27.6157],\n",
       "          [ -5.8138,   6.6218,  16.0401,  ...,  -9.0992,  13.1212, -27.6657],\n",
       "          [ -5.9127,   6.6856,  15.9365,  ...,  -9.0702,  13.1355, -27.5467],\n",
       "          ...,\n",
       "          [ -5.6568,   6.8295,  15.9356,  ...,  -9.0257,  12.8982, -27.4596],\n",
       "          [ -5.8952,   6.7211,  15.8148,  ...,  -9.0858,  13.1646, -27.4684],\n",
       "          [  0.1753,   8.3875,  13.8582,  ...,  -6.9397,   6.4019, -20.9551]],\n",
       "\n",
       "         [[ -1.4637,  -3.1165,   6.7097,  ...,   6.4140,   5.8503,  -5.0967],\n",
       "          [ -1.4782,  -3.1924,   6.6662,  ...,   6.3325,   5.8113,  -5.2231],\n",
       "          [ -1.3821,  -3.0625,   6.6769,  ...,   6.3652,   5.7637,  -5.2049],\n",
       "          ...,\n",
       "          [ -1.3235,  -3.1844,   6.4800,  ...,   5.9482,   5.5488,  -5.5171],\n",
       "          [ -1.3355,  -3.0373,   6.6251,  ...,   6.4052,   5.7730,  -5.1578],\n",
       "          [ -1.4198,  -5.3494,   1.9026,  ...,  -3.4247,   1.9859, -10.6547]],\n",
       "\n",
       "         [[  1.5457,  -9.2534,   6.3109,  ...,   2.4554, -13.5286,  13.7771],\n",
       "          [  1.5236,  -9.3409,   6.4265,  ...,   2.4372, -13.5461,  13.6921],\n",
       "          [  1.5950,  -9.2968,   6.3570,  ...,   2.5331, -13.4149,  13.7457],\n",
       "          ...,\n",
       "          [  1.3101,  -9.5810,   6.4363,  ...,   2.3742, -13.4207,  13.5525],\n",
       "          [  1.5835,  -9.3092,   6.3922,  ...,   2.5270, -13.3291,  13.7000],\n",
       "          [ -4.2834, -12.8164,   5.0840,  ...,   0.4706, -13.7314,   7.4275]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[  5.5839, -11.2110,  -0.5622,  ...,  -0.3487,  -6.9951,  10.8336],\n",
       "          [ -3.8793,  -8.3498,   6.5323,  ...,   8.8741,  -3.5127,  -7.9291]],\n",
       "\n",
       "         [[ 17.7621, -15.1454,   0.1579,  ...,   6.4852, -11.2752,  -6.5978],\n",
       "          [ -9.9459,  20.6650,  -0.4981,  ...,  -5.5991,  -5.3641,  -3.0941]],\n",
       "\n",
       "         [[-12.9616,   2.2691, -11.3516,  ...,  17.9359, -18.2803, -18.0020],\n",
       "          [ 11.9145, -12.0623,  14.3104,  ...,  -4.7560,  16.1723,  13.6617]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  8.0005, -12.8593,  -6.9537,  ...,   2.2262,  11.5560,   9.2974],\n",
       "          [  6.2863,  -2.0084,   1.1578,  ...,  -5.8923,   2.5847,   6.8087]],\n",
       "\n",
       "         [[-17.6860,  16.5289,  18.3884,  ...,   6.6482,  15.9135,  -9.7510],\n",
       "          [ -0.4182,   1.1429,  -4.6206,  ...,  14.7526,   1.2058,  -2.7271]],\n",
       "\n",
       "         [[-21.8228, -19.3376,   8.0004,  ...,   8.6801,  -1.3316,  19.5833],\n",
       "          [  1.8678,   5.2045,  -8.6378,  ...,  -9.8775, -10.1571,  -5.7480]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>), tensor([[[[ 3.0849e+01, -5.9210e+01, -4.1076e+01,  ...,  7.0417e+00,\n",
       "           -1.0015e+01,  3.9730e+01],\n",
       "          [-8.5601e+00,  3.9953e+01,  2.6056e+00,  ..., -9.3880e+00,\n",
       "           -1.1794e+01, -2.0027e+01]],\n",
       "\n",
       "         [[ 7.3245e+00, -3.7116e+00, -1.0771e+00,  ..., -3.1688e+01,\n",
       "           -1.1341e+01, -2.7095e+01],\n",
       "          [-4.9836e-02, -3.2577e-01,  8.0566e+00,  ...,  1.9602e+01,\n",
       "            1.3503e+01,  1.1240e+01]],\n",
       "\n",
       "         [[-1.4255e+01, -2.6281e+00,  2.8573e+00,  ...,  1.3435e+01,\n",
       "           -1.0458e+01,  2.8584e+01],\n",
       "          [-8.8888e+00, -1.8472e+01,  2.3087e+00,  ..., -1.3777e+00,\n",
       "           -1.6364e+01, -1.4297e+01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.1808e+01, -1.4378e+01, -1.1016e+02,  ..., -3.6005e+01,\n",
       "            2.2276e+01, -6.1076e+01],\n",
       "          [ 7.0736e+00, -3.9778e+00,  5.6584e+01,  ...,  1.2189e+01,\n",
       "            3.7144e+00,  3.0808e+01]],\n",
       "\n",
       "         [[ 1.9403e+01,  1.7956e+01, -7.1401e+00,  ...,  1.3528e+01,\n",
       "            8.9907e+01, -2.1817e+01],\n",
       "          [ 5.8189e+00, -6.8733e+00,  7.9859e-01,  ..., -4.5761e+00,\n",
       "           -4.7487e+01,  9.7683e-01]],\n",
       "\n",
       "         [[-5.8679e+00, -2.4907e+00,  1.7328e+01,  ..., -2.0143e+00,\n",
       "           -9.1765e+00, -4.8652e+00],\n",
       "          [ 1.2448e+01, -1.9574e+00, -9.9555e+00,  ..., -3.8046e+00,\n",
       "           -4.1149e+00,  1.6650e+00]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[  8.3403,  -3.1762,   9.1100,  ...,   6.7761, -11.0830,   3.7651],\n",
       "          [  8.2897,  -3.0606,   9.1150,  ...,   6.7301, -10.9122,   3.8691],\n",
       "          [  8.3496,  -3.1303,   9.1445,  ...,   6.8123, -11.1677,   3.6786],\n",
       "          ...,\n",
       "          [  8.6468,  -2.6936,   9.5194,  ...,   6.6189, -10.9796,   4.0095],\n",
       "          [  8.2173,  -3.1383,   9.0339,  ...,   6.8610, -11.1084,   3.6434],\n",
       "          [ 12.4601,   5.5257,  14.1627,  ...,   0.9482,  -4.3973,  11.2038]],\n",
       "\n",
       "         [[  1.6204,   0.9730,  -1.3331,  ...,  22.6931, -13.9950,  10.3552],\n",
       "          [  1.6811,   0.9090,  -1.3356,  ...,  22.6835, -13.9934,  10.3032],\n",
       "          [  1.6034,   0.9981,  -1.3321,  ...,  22.6512, -14.0064,  10.2684],\n",
       "          ...,\n",
       "          [  1.4047,   1.1492,  -1.4158,  ...,  22.2418, -13.8768,   9.9132],\n",
       "          [  1.6572,   0.9634,  -1.2626,  ...,  22.6261, -13.9919,  10.2349],\n",
       "          [ -2.3529,   2.9285,  -2.3310,  ...,  11.6827,  -8.5312,   3.4293]],\n",
       "\n",
       "         [[  9.8564,   3.6804,  -1.0942,  ...,  10.1316, -23.0843, -12.2000],\n",
       "          [  9.8887,   3.6769,  -1.0986,  ...,  10.1383, -23.0115, -12.1264],\n",
       "          [  9.8037,   3.6656,  -1.0856,  ...,  10.1209, -23.0320, -12.1452],\n",
       "          ...,\n",
       "          [  9.6850,   3.6661,  -1.0960,  ...,   9.9654, -22.8326, -11.8476],\n",
       "          [  9.8163,   3.6825,  -1.0738,  ...,  10.1039, -22.9637, -12.0936],\n",
       "          [  5.9737,   2.9319,  -1.7305,  ...,   5.7439, -14.7770,  -3.5573]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 22.7340,  24.0058, -10.4372,  ..., -13.6876,   3.2489,  -1.1075],\n",
       "          [ 22.6170,  23.9037, -10.4277,  ..., -13.5582,   3.2148,  -1.1094],\n",
       "          [ 22.6202,  23.9360, -10.4699,  ..., -13.7442,   3.2455,  -1.1199],\n",
       "          ...,\n",
       "          [ 21.9818,  23.3907, -10.6003,  ..., -13.7005,   3.0733,  -1.3006],\n",
       "          [ 22.6136,  23.8908, -10.3979,  ..., -13.6346,   3.2131,  -1.0588],\n",
       "          [  6.8305,   8.6417, -10.8318,  ..., -10.2989,  -0.9513,  -4.7326]],\n",
       "\n",
       "         [[ -8.3694,  11.2514,  13.1035,  ...,  -9.5558, -10.6232,  12.1827],\n",
       "          [ -8.3511,  11.2153,  13.0665,  ...,  -9.5975, -10.5397,  12.1470],\n",
       "          [ -8.3226,  11.2672,  13.0392,  ...,  -9.5414, -10.5726,  12.1714],\n",
       "          ...,\n",
       "          [ -8.0357,  11.0639,  12.6656,  ...,  -9.2469, -10.3517,  12.0277],\n",
       "          [ -8.3364,  11.2667,  13.0029,  ...,  -9.5695, -10.5383,  12.1390],\n",
       "          [ -0.8942,   4.2299,   4.9367,  ...,  -2.3869,  -4.9206,   6.3331]],\n",
       "\n",
       "         [[  7.7380,  14.7599,  13.8401,  ..., -12.8017,   3.0048, -13.5660],\n",
       "          [  7.6793,  14.6829,  13.7948,  ..., -12.7878,   2.9145, -13.4951],\n",
       "          [  7.6856,  14.7317,  13.8208,  ..., -12.7275,   3.0394, -13.5681],\n",
       "          ...,\n",
       "          [  7.3899,  14.2941,  13.4053,  ..., -12.4273,   2.8543, -13.1932],\n",
       "          [  7.6712,  14.7193,  13.8472,  ..., -12.6711,   3.0404, -13.5665],\n",
       "          [  1.6107,   4.6408,   3.8066,  ...,  -5.8774,  -1.8873,  -3.8463]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>), tensor([[[[-23.8294, -14.9730, -14.6747,  ...,  13.2016,  -9.0943, -12.6025],\n",
       "          [-23.8300, -14.8852, -14.6761,  ...,  13.2429,  -9.2177, -12.4951],\n",
       "          [-23.8618, -14.9222, -14.6577,  ...,  13.2176,  -8.9522, -12.4732],\n",
       "          ...,\n",
       "          [-23.6000, -14.8047, -14.4999,  ...,  13.4296,  -8.8339, -12.3641],\n",
       "          [-23.7049, -14.8325, -14.6466,  ...,  13.1212,  -9.0298, -12.3277],\n",
       "          [-14.9568, -10.1706,  -9.8628,  ...,  15.1075,  -8.1072, -10.7440]],\n",
       "\n",
       "         [[-20.0783,  -5.8494,  -5.6683,  ...,  -3.9991,  22.1661, -10.5401],\n",
       "          [-20.2092,  -5.9264,  -5.6312,  ...,  -4.0487,  22.2924, -10.3675],\n",
       "          [-19.9517,  -5.7354,  -5.5966,  ...,  -3.9510,  22.1002, -10.5493],\n",
       "          ...,\n",
       "          [-19.8187,  -5.8959,  -5.4302,  ...,  -3.9104,  22.1575, -10.0512],\n",
       "          [-19.8897,  -5.6132,  -5.6621,  ...,  -4.0272,  21.9757, -10.5129],\n",
       "          [-16.1601,  -9.9128,  -2.4924,  ...,  -2.3611,  19.8889,  -0.2619]],\n",
       "\n",
       "         [[-46.7365,  -5.0534,  -8.1308,  ..., -10.5371, -60.5605,  15.5112],\n",
       "          [-46.7682,  -5.0716,  -8.1737,  ..., -10.5018, -60.4894,  15.4148],\n",
       "          [-46.5500,  -5.1353,  -8.0646,  ..., -10.4570, -60.3707,  15.5413],\n",
       "          ...,\n",
       "          [-45.8540,  -5.0479,  -7.8380,  ..., -10.0158, -59.8559,  15.4661],\n",
       "          [-46.4371,  -5.1549,  -8.0278,  ..., -10.5482, -60.1341,  15.3894],\n",
       "          [-27.4450,  -0.3713,  -2.6332,  ...,  -1.9982, -40.4934,   9.7667]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-13.2097, -22.5435,  -3.4599,  ..., -20.1127,  24.3349,   4.5251],\n",
       "          [-13.1665, -22.5815,  -3.4646,  ..., -20.1605,  24.2912,   4.5891],\n",
       "          [-13.2446, -22.4264,  -3.3503,  ..., -19.9686,  24.3296,   4.6449],\n",
       "          ...,\n",
       "          [-13.0002, -22.2052,  -3.1632,  ..., -19.7805,  24.0205,   4.6858],\n",
       "          [-13.2124, -22.3437,  -3.2908,  ..., -19.9936,  24.3587,   4.6390],\n",
       "          [ -6.6056, -14.4037,   0.7326,  ..., -11.4239,  10.4640,   5.1360]],\n",
       "\n",
       "         [[ -2.3992, -13.0457, -20.7040,  ..., -10.2237, -28.9883, -14.3354],\n",
       "          [ -2.1739, -13.1608, -20.8253,  ..., -10.1946, -28.9663, -14.2975],\n",
       "          [ -2.3816, -13.0583, -20.7204,  ..., -10.2216, -28.9042, -14.2114],\n",
       "          ...,\n",
       "          [ -2.1560, -13.0081, -20.6351,  ..., -10.0841, -28.5069, -13.8854],\n",
       "          [ -2.3406, -13.0145, -20.6752,  ..., -10.2086, -28.7958, -14.1927],\n",
       "          [  3.3403, -10.5601, -17.2376,  ...,  -6.5329, -14.6703,  -8.4082]],\n",
       "\n",
       "         [[ -4.3001, -10.4865,   3.9866,  ...,  -8.6583,  -3.6617,  -2.8770],\n",
       "          [ -4.3117, -10.4919,   4.0439,  ...,  -8.5623,  -3.5853,  -2.9108],\n",
       "          [ -4.2772, -10.4971,   3.9338,  ...,  -8.5907,  -3.6700,  -2.9020],\n",
       "          ...,\n",
       "          [ -4.3535, -10.4248,   4.1923,  ...,  -8.4096,  -3.6446,  -2.7635],\n",
       "          [ -4.2446, -10.4584,   3.8909,  ...,  -8.5688,  -3.6344,  -2.9514],\n",
       "          [ -2.2301,  -7.1543,   9.2774,  ...,  -2.9852,  -2.9072,  -0.9325]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[  5.0975,  -9.9290,  -6.2995,  ...,  17.0197,  -6.8303,  -8.8393],\n",
       "          [ -9.6428,  22.5607,   7.1917,  ...,   2.5308,  24.6402,  30.7853]],\n",
       "\n",
       "         [[  8.4263, -16.5289,  -8.9241,  ...,  -3.6363, -21.2393, -21.4493],\n",
       "          [ -7.0547,   3.5921,   4.5172,  ...,  16.6436,   7.7298,  -5.6767]],\n",
       "\n",
       "         [[-13.5034,   6.3976,  13.4390,  ...,  23.0818,  11.6366,  -2.2301],\n",
       "          [ 11.2252,   8.6326, -19.7457,  ..., -15.6349,  -0.8121,  12.0514]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ -3.2494,   9.6718, -21.0405,  ...,  18.9053,   6.6001,   1.6218],\n",
       "          [-16.5981,   4.6288,  -0.7581,  ..., -10.8286,  -3.0473, -18.2631]],\n",
       "\n",
       "         [[ -9.9774,  -1.8568,  -4.4532,  ..., -11.8444, -11.5189, -13.8479],\n",
       "          [ 29.3307,  15.4661,   6.0387,  ...,  -0.2589, -13.2926,  -2.6614]],\n",
       "\n",
       "         [[-13.3743,  -0.6501, -13.6415,  ...,  23.0385, -19.3181,  17.4546],\n",
       "          [  7.0481,   3.3212,  -1.3530,  ...,  -5.8587,  18.1048, -21.0705]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>), tensor([[[[ 4.5939e+01, -3.3432e+01, -6.0866e-02,  ..., -2.9525e+01,\n",
       "           -4.9138e+01,  2.2901e+01],\n",
       "          [ 4.5089e+01, -1.9947e+01, -2.2952e+01,  ..., -5.9873e+00,\n",
       "           -1.8443e+01,  4.2118e+01]],\n",
       "\n",
       "         [[-9.0318e+01, -4.9060e+00,  2.5183e+01,  ...,  3.0813e+01,\n",
       "            1.2756e+00, -2.4621e+01],\n",
       "          [ 5.8637e+01,  3.7110e+01,  5.3133e+01,  ..., -2.1591e+01,\n",
       "            4.9056e+01,  1.2606e+01]],\n",
       "\n",
       "         [[-3.3532e+00, -6.5047e+01,  5.9225e+01,  ..., -1.7817e+01,\n",
       "           -2.9251e+01, -7.8564e+01],\n",
       "          [-1.5570e+01,  2.0951e+01, -1.0607e+01,  ..., -3.4545e+01,\n",
       "           -3.6396e+01,  9.1780e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.4659e+01,  7.7248e+01, -2.9356e+01,  ..., -3.8079e+01,\n",
       "            1.0132e+01,  8.0801e+01],\n",
       "          [ 3.5492e+01, -3.3789e-02,  1.9282e+00,  ..., -2.0006e+01,\n",
       "            3.4297e+01, -4.6363e+00]],\n",
       "\n",
       "         [[-2.3341e+01,  1.6935e+01,  6.8184e+01,  ...,  1.6298e+01,\n",
       "            2.4515e+01, -3.1758e+01],\n",
       "          [ 2.0634e+01, -4.0678e+01, -2.0841e+01,  ..., -6.3052e+01,\n",
       "            2.0096e+00, -2.8645e+00]],\n",
       "\n",
       "         [[-4.6808e+00,  4.3076e+01, -3.8332e+00,  ...,  1.6267e+01,\n",
       "           -1.0539e+01, -6.6573e+01],\n",
       "          [ 1.5532e+01, -3.0395e+01, -6.6411e+01,  ..., -1.7074e+01,\n",
       "            4.9517e+00,  2.1403e+01]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[ -4.4088,   3.9715,  12.9415,  ...,   0.0816,  -2.2267,   2.6489],\n",
       "          [ -4.4414,   4.0313,  12.9960,  ...,   0.0491,  -2.2428,   2.6320],\n",
       "          [ -4.4488,   3.9863,  12.8958,  ...,   0.0889,  -2.2430,   2.6308],\n",
       "          ...,\n",
       "          [ -4.2311,   3.8452,  12.6107,  ...,   0.0422,  -2.3631,   2.4875],\n",
       "          [ -4.5072,   4.0663,  12.9520,  ...,   0.0925,  -2.1982,   2.6438],\n",
       "          [  0.9354,   0.2229,   5.3064,  ...,  -0.9793,  -4.1980,  -0.6040]],\n",
       "\n",
       "         [[-16.1946,   7.5716,  10.2032,  ...,   9.7522, -11.9845,  -0.2294],\n",
       "          [-16.1608,   7.5128,  10.2473,  ...,   9.7412, -11.9288,  -0.2663],\n",
       "          [-16.1382,   7.5108,  10.1156,  ...,   9.6932, -11.9196,  -0.2347],\n",
       "          ...,\n",
       "          [-15.8488,   7.2630,   9.9222,  ...,   9.3512, -11.5518,  -0.3468],\n",
       "          [-16.1424,   7.4760,  10.1591,  ...,   9.6952, -11.9073,  -0.2264],\n",
       "          [ -7.9185,   1.6048,   4.3146,  ...,   2.0850,  -2.5832,  -2.8908]],\n",
       "\n",
       "         [[  6.6256,  -5.0774,  11.8017,  ...,  -2.1842,  10.4873,  -5.1185],\n",
       "          [  6.5855,  -4.9623,  11.6804,  ...,  -2.1839,  10.4153,  -5.0121],\n",
       "          [  6.6533,  -5.0595,  11.8188,  ...,  -2.2033,  10.5272,  -5.1360],\n",
       "          ...,\n",
       "          [  6.4902,  -5.0106,  11.6135,  ...,  -2.2310,  10.4083,  -4.8981],\n",
       "          [  6.6336,  -4.9809,  11.7572,  ...,  -2.1830,  10.4910,  -5.1103],\n",
       "          [  1.3901,  -2.3002,   5.5780,  ...,  -2.2258,   5.6432,   0.6093]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  5.7651, -11.4045,  17.3250,  ..., -27.8480, -16.5555, -12.9484],\n",
       "          [  5.7005, -11.3732,  17.3069,  ..., -27.7672, -16.4838, -12.9321],\n",
       "          [  5.7214, -11.3610,  17.2893,  ..., -27.8163, -16.4963, -12.8908],\n",
       "          ...,\n",
       "          [  5.4011, -10.9626,  17.0306,  ..., -27.7330, -16.0414, -12.6134],\n",
       "          [  5.6909, -11.3525,  17.2804,  ..., -27.6663, -16.4976, -12.9003],\n",
       "          [ -1.0980,  -1.3284,   8.9396,  ..., -20.6611,  -5.2530,  -5.1959]],\n",
       "\n",
       "         [[ -9.3228,  -8.8992,   8.8832,  ...,  -1.2023,   0.5447,  10.4095],\n",
       "          [ -9.3222,  -8.8893,   8.8371,  ...,  -1.2123,   0.5702,  10.3205],\n",
       "          [ -9.3089,  -8.8894,   8.8956,  ...,  -1.2470,   0.4914,  10.3906],\n",
       "          ...,\n",
       "          [ -8.9555,  -8.6913,   8.6546,  ...,  -1.2625,   0.6216,  10.0214],\n",
       "          [ -9.3406,  -8.8963,   8.8723,  ...,  -1.2797,   0.4578,  10.3570],\n",
       "          [ -1.0338,  -4.0867,   2.0077,  ...,  -0.4407,   3.7646,   2.1452]],\n",
       "\n",
       "         [[ -3.4696, -11.2452, -13.5681,  ..., -10.9584,  -4.5799, -12.5055],\n",
       "          [ -3.4498, -11.2285, -13.5568,  ..., -10.9591,  -4.6339, -12.5061],\n",
       "          [ -3.5034, -11.2628, -13.5456,  ..., -10.9143,  -4.5542, -12.4830],\n",
       "          ...,\n",
       "          [ -3.5053, -11.1156, -13.3819,  ..., -10.6923,  -4.4190, -12.2896],\n",
       "          [ -3.4825, -11.2601, -13.5242,  ..., -10.9008,  -4.6330, -12.4793],\n",
       "          [ -2.3029,  -6.0370,  -8.2568,  ...,  -5.6667,  -1.4834,  -6.8077]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>), tensor([[[[ -6.5489, -16.9090,   2.2857,  ...,  16.7639, -16.9368,   8.6268],\n",
       "          [ -6.5842, -16.7796,   2.1826,  ...,  16.7039, -16.9071,   8.4803],\n",
       "          [ -6.5827, -16.8442,   2.3318,  ...,  16.7927, -16.8206,   8.5952],\n",
       "          ...,\n",
       "          [ -6.5486, -16.3940,   1.8195,  ...,  16.1548, -16.4855,   8.1678],\n",
       "          [ -6.6195, -16.7992,   2.3160,  ...,  16.7137, -16.8116,   8.6094],\n",
       "          [ -2.1004,  -6.6478,  -9.4492,  ...,   1.1451,  -9.2268,  -0.9453]],\n",
       "\n",
       "         [[ -0.6491, -11.5151, -12.5560,  ..., -13.2078,  17.8394,  21.5944],\n",
       "          [ -0.3989, -11.6347, -12.6003,  ..., -13.1190,  17.9339,  21.8985],\n",
       "          [ -0.5100, -11.4020, -12.5473,  ..., -13.1418,  17.7986,  21.5896],\n",
       "          ...,\n",
       "          [ -0.1084, -11.5722, -12.5217,  ..., -12.9076,  17.7927,  21.8072],\n",
       "          [ -0.4451, -11.3868, -12.5401,  ..., -13.1820,  17.7438,  21.5681],\n",
       "          [  8.3631, -14.1718,  -8.3050,  ...,  -7.0859,  13.1953,  20.7267]],\n",
       "\n",
       "         [[ -0.9657,  31.5060,  11.2304,  ...,  19.2587, -10.3473,  13.6852],\n",
       "          [ -1.0222,  31.3325,  11.2804,  ...,  19.2413, -10.5354,  13.5792],\n",
       "          [ -1.0174,  31.3035,  11.1392,  ...,  19.1565, -10.2249,  13.5375],\n",
       "          ...,\n",
       "          [ -1.5873,  30.4581,  10.9160,  ...,  18.5776, -10.0695,  13.1808],\n",
       "          [ -0.9499,  31.2593,  11.1246,  ...,  19.0177, -10.1401,  13.5136],\n",
       "          [-10.5217,  12.6288,   4.7243,  ...,   7.2822,  -6.3913,   2.9402]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ -2.1662, -14.0656,  25.9139,  ...,   6.9708, -13.2778,  32.0144],\n",
       "          [ -2.3911, -14.0195,  26.0153,  ...,   6.9304, -13.0779,  32.0353],\n",
       "          [ -2.1234, -14.0474,  25.8384,  ...,   7.0007, -13.1435,  31.9246],\n",
       "          ...,\n",
       "          [ -2.3078, -14.0266,  25.6788,  ...,   6.7817, -12.6858,  31.2664],\n",
       "          [ -2.2034, -14.0177,  25.7950,  ...,   7.1514, -13.0341,  31.9255],\n",
       "          [ -6.3481, -11.0265,  18.6473,  ...,  -0.6729,  -3.2758,  17.5427]],\n",
       "\n",
       "         [[ -8.5404, -11.9275,   7.4489,  ...,   0.8123,  12.0822,   3.4450],\n",
       "          [ -8.3360, -11.8649,   7.5127,  ...,   1.0121,  12.0025,   3.2134],\n",
       "          [ -8.7337, -11.8291,   7.5818,  ...,   0.8488,  12.0130,   3.2505],\n",
       "          ...,\n",
       "          [ -8.5821, -11.4923,   7.6225,  ...,   1.2491,  11.6597,   2.4760],\n",
       "          [ -8.8218, -11.8148,   7.4562,  ...,   0.7592,  12.0472,   3.2223],\n",
       "          [  2.6285,  -6.8954,   6.5063,  ...,   8.4435,   2.2366, -10.8870]],\n",
       "\n",
       "         [[-11.2490, -24.9459,  16.3422,  ..., -31.2858,  31.9792,   3.6534],\n",
       "          [-10.9877, -24.9530,  16.3254,  ..., -31.3667,  32.1432,   3.7166],\n",
       "          [-11.0560, -25.0757,  16.2142,  ..., -31.0584,  31.8287,   3.6164],\n",
       "          ...,\n",
       "          [-10.5793, -25.0364,  15.9319,  ..., -30.8231,  31.1059,   3.8173],\n",
       "          [-10.9251, -24.9772,  16.2339,  ..., -30.9347,  31.7948,   3.7105],\n",
       "          [ -2.5667, -17.4498,   9.5180,  ..., -22.7545,  16.2837,   3.4014]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>))), decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[-0.0468, -0.0631,  0.3311,  ..., -0.0353,  0.0413, -0.0215],\n",
       "         [-0.0464, -0.0617,  0.3272,  ..., -0.0369,  0.0399, -0.0200],\n",
       "         [-0.0453, -0.0608,  0.3271,  ..., -0.0387,  0.0415, -0.0198],\n",
       "         ...,\n",
       "         [-0.0427, -0.0565,  0.3198,  ..., -0.0443,  0.0461, -0.0123],\n",
       "         [-0.0453, -0.0599,  0.3249,  ..., -0.0384,  0.0402, -0.0203],\n",
       "         [-0.0118, -0.0176,  0.1426,  ..., -0.0754,  0.1011,  0.1146]]],\n",
       "       device='cuda:0', grad_fn=<MulBackward0>), encoder_hidden_states=None, encoder_attentions=None)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[1176,    1]]), 'attention_mask': tensor([[1, 1]])}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " my_experiment.tokenizer(\"true\", return_tensors=\"pt\").input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-1b10e5481fa8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m data_utils.DataLoader(my_experiment,\n\u001b[0;32m----> 3\u001b[0;31m                       batch_size=self.options.batch_size, drop_last=True)\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data_utils\n",
    "data_utils.DataLoader(my_experiment,\n",
    "                      batch_size=self.options.batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-23 14:50:16,269 Number of examples for the current task : 13053\n",
      "2021-02-23 14:50:26,903 Loss for iteration 160 is 0.044410310346971855\n",
      "2021-02-23 14:50:58,447 Loss for iteration 1760 is 0.04668810661580112\n",
      "2021-02-23 14:51:30,201 Loss for iteration 3360 is 0.044428569922373755\n",
      "2021-02-23 14:52:08,039 Loss for iteration 4960 is 0.043729735220935186\n",
      "2021-02-23 14:52:47,018 Loss for iteration 6560 is 0.04334880319625647\n",
      "2021-02-23 14:53:25,092 Loss for iteration 8160 is 0.043204358510979236\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 7.92 GiB total capacity; 5.86 GiB already allocated; 78.25 MiB free; 6.37 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1cc0c9be5203>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_experiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/CPD/repository/LifelongInformationRetrieval/examples/../experiments/t5ranker.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'task'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration_by_task\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mending_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/CPD/repository/LifelongInformationRetrieval/examples/../experiments/t5ranker.py\u001b[0m in \u001b[0;36m_train_task\u001b[0;34m(self, max_iteration, dataloader)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                 \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m                 \u001b[0mloss_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mloss_n\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/CPD/repository/LifelongInformationRetrieval/examples/../experiments/t5ranker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, positive, negative)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_positive\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_negative\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/gerald/libraries/conda/envs/lire/lib/python3.9/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/gerald/libraries/conda/envs/lire/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 7.92 GiB total capacity; 5.86 GiB already allocated; 78.25 MiB free; 6.37 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "my_experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
