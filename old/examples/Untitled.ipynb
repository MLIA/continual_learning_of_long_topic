{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "enclosed-precipitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/gerald/Documents/CPD/repository/LifelongInformationRetrieval\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "waiting-organ",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lire.data_tools.dataset import MSMarco\n",
    "from lire.data_tools.data_reader import Qrels\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sticky-width",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of query: \" highmark address 0\"\n"
     ]
    }
   ],
   "source": [
    "data_folder    = \"/local/gerald/CPD/data\"\n",
    "embedding_path = \"/local/gerald/CPD/data/query_embeddings2.pkl\"\n",
    "cluster_path   = \"/local/gerald/CPD/data/cluster_completed2.pth\"\n",
    "cluster_center   = \"/local/gerald/CPD/data/cluster_center2.pth\"\n",
    "\n",
    "train_queries = MSMarco.MSMarcoPassageRankingDataset.load_queries(\"train\", data_folder)\n",
    "dev_queries   = MSMarco.MSMarcoPassageRankingDataset.load_queries(\"dev\", data_folder)\n",
    "eval_queries  = MSMarco.MSMarcoPassageRankingDataset.load_queries(\"eval\", data_folder)\n",
    "train_qrels   = MSMarco.MSMarcoPassageRankingDataset.load_qrels(\"train\", data_folder)\n",
    "dev_qrels     = MSMarco.MSMarcoPassageRankingDataset.load_qrels(\"dev\", data_folder)\n",
    "qrels_set     = Qrels.merge_qrels(train_qrels, dev_qrels)\n",
    "\n",
    "queries_set   = pd.concat([train_queries, dev_queries, eval_queries])\n",
    "query_example = queries_set.iloc[11][1]\n",
    "print('Example of query: \"', query_example, '0\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "incomplete-theater",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_sentences  = queries_set[1].tolist() \n",
    "queries_ids        = queries_set.index.values.tolist()\n",
    "queries_inv_ids    = {v:i for i, v in enumerate(queries_ids)}\n",
    "queries_ids_train  = len(train_queries)\n",
    "queries_ids_dev    = len(train_queries) + len(dev_queries)\n",
    "queries_ids_eval   = len(train_queries) + len(dev_queries) + len(eval_queries)\n",
    "\n",
    "\n",
    "clusters    = torch.load(cluster_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caroline-possibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters    = torch.load(cluster_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "identified-uganda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3551"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "auburn-sitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    train_set, dev_set, eval_set = [], [], [] \n",
    "    for cluster in clusters:\n",
    "        train_set.append([])\n",
    "        dev_set.append([])\n",
    "        eval_set.append([])\n",
    "\n",
    "        for query in cluster:\n",
    "            if(str(queries_ids[query]) in qrels_set):\n",
    "                if(query < queries_ids_train):\n",
    "                    train_set[-1].append(queries_ids[query])\n",
    "                elif(query < queries_ids_dev):\n",
    "                    dev_set[-1].append(queries_ids[query])\n",
    "                elif(query < queries_ids_eval):\n",
    "                    eval_set[-1].append(queries_ids[query])\n",
    "                else:\n",
    "                    raise Exception(\"Irrelevant query id\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "amateur-flush",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(min([len(i) for i in train_set]))\n",
    "print(min([len(i) for i in dev_set]))\n",
    "print(min([len(i) for i in eval_set]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "editorial-presentation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dev sets that contains less than 2 queries :  149\n",
      "Number of eval sets that contains less than 2 queries :  3551\n"
     ]
    }
   ],
   "source": [
    "too_small_dev_cluster = [ i for i, cluster in enumerate(dev_set) if(len(cluster) <= 2)]\n",
    "too_small_eval_cluster = [ i for i, cluster in enumerate(eval_set) if(len(cluster) <= 2)]\n",
    "\n",
    "print(\"Number of dev sets that contains less than 2 queries : \", len(too_small_dev_cluster))\n",
    "print(\"Number of eval sets that contains less than 2 queries : \", len(too_small_eval_cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "narrow-jungle",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(embedding_path, \"rb\") as fIn:\n",
    "    cache_data = pickle.load(fIn)\n",
    "corpus_sentences = cache_data['sentences']\n",
    "corpus_embeddings = cache_data['embeddings']\n",
    "\n",
    "clusters_to_merge = too_small_dev_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "every-kitty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[124, 81, 166, 34, 90, 136, 162, 114, 120, 85, 139, 52, 119, 155, 87, 130, 161, 118, 72, 39, 86, 47, 61, 117, 47, 62, 69, 94, 77, 89, 96, 52, 154, 57, 144, 54, 97, 137, 66, 52, 76, 72, 75, 124, 44, 21, 37, 47, 88, 94, 82, 140, 90, 83, 66, 61, 60, 105, 70, 44, 94, 90, 93, 110, 55, 66, 90, 104, 86, 40, 35, 90, 85, 114, 99, 56, 119, 65, 104, 72, 164, 115, 84, 57, 59, 42, 86, 41, 67, 58, 60, 91, 29, 65, 85, 84, 144, 79, 24, 110, 53, 36, 113, 61, 153, 46, 105, 47, 50, 125, 128, 50, 113, 126, 131, 126, 94, 67, 85, 67, 94, 73, 87, 60, 53, 61, 98, 35, 68, 89, 100, 110, 56, 63, 52, 36, 52, 43, 70, 90, 146, 36, 120, 23, 75, 121, 66, 78, 15]\n"
     ]
    }
   ],
   "source": [
    "print([len(clusters[cluster]) for cluster in clusters_to_merge])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ready-noise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cluster to merge :  149\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of cluster to merge : \", len(clusters_to_merge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "imposed-fifth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3551\n",
      "3551\n"
     ]
    }
   ],
   "source": [
    "cluster_center = torch.load(\"/local/gerald/CPD/data/cluster_center2.pth\")\n",
    "print(len(cluster_center))\n",
    "print(len(clusters))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "informational-wednesday",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cluster_center_merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-8724306249d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcluster_center_merged\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cluster_center_merged' is not defined"
     ]
    }
   ],
   "source": [
    "cluster_center_merged[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dominican-plaza",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3402\n",
      "3402\n",
      "3402\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ccc488fee64732bed13590998cf19b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clusters_center = torch.load(\"/local/gerald/CPD/data/cluster_center2.pth\")\n",
    "cluster_center_merged  = []\n",
    "cluster_center_reindex = []\n",
    "new_clusters = []\n",
    "\n",
    "# reindexing\n",
    "for i, cluster_center in enumerate(clusters_center):\n",
    "    if i not in clusters_to_merge:\n",
    "        cluster_center_merged.append(clusters_center[i])\n",
    "        cluster_center_reindex.append(i)\n",
    "        new_clusters.append(clusters[i])\n",
    "\n",
    "# much faster using pytorch\n",
    "cluster_center_merged  = torch.Tensor(cluster_center_merged)\n",
    "corpus_embeddings = torch.Tensor(corpus_embeddings)\n",
    "print(len(torch.Tensor(cluster_center_merged)))\n",
    "print(len(cluster_center_reindex))\n",
    "print(len(new_clusters))\n",
    "# merging\n",
    "for i, cluster_index in zip(tqdm.notebook.trange(len(clusters_to_merge)), clusters_to_merge):\n",
    "    cluster = clusters[cluster_index]\n",
    "    for query in cluster:\n",
    "\n",
    "        cos_sim = util.pytorch_cos_sim(cluster_center_merged, corpus_embeddings[query])\n",
    "        new_clusters[cos_sim.argmax()].append(query)\n",
    "        #sp = new_clusters[cos_sim.argmax()][0]\n",
    "        #print('query to move\"' , corpus_sentences[query], '\" | sample of new cluster \"', corpus_sentences[sp] , '\"\\n')        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "convertible-prize",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, dev_set, eval_set = [], [], [] \n",
    "for cluster in new_clusters:\n",
    "    train_set.append([])\n",
    "    dev_set.append([])\n",
    "    eval_set.append([])\n",
    "    \n",
    "    for query in cluster:\n",
    "        if(str(queries_ids[query]) in qrels_set):\n",
    "            if(query < queries_ids_train):\n",
    "                train_set[-1].append(queries_ids[query])\n",
    "            elif(query < queries_ids_dev):\n",
    "                dev_set[-1].append(queries_ids[query])\n",
    "            elif(query < queries_ids_eval):\n",
    "                eval_set[-1].append(queries_ids[query])\n",
    "            else:\n",
    "                raise Exception(\"Irrelevant query id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "mediterranean-aquatic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "3\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(min([len(i) for i in train_set]))\n",
    "print(min([len(i) for i in dev_set]))\n",
    "print(min([len(i) for i in eval_set]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dutch-roads",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/web/gerald/public_html/lire_data/msmarco_topics_cleaned_train_stsb-roberta-large.json', 'w') as outfile:\n",
    "    json.dump(train_set, outfile, indent = 4)\n",
    "with open('/web/gerald/public_html/lire_data/msmarco_topics_cleaned_dev_stsb-roberta-large.json', 'w') as outfile:\n",
    "    json.dump(dev_set, outfile, indent = 4)\n",
    "with open('/web/gerald/public_html/lire_data/msmarco_topics_cleaned_eval_stsb-roberta-large.json', 'w') as outfile:\n",
    "    json.dump(eval_set, outfile, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-warrant",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lire",
   "language": "python",
   "name": "lire"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
